diff --git a/README.md b/README.md
deleted file mode 100755
index 2957da5..0000000
--- a/README.md
+++ /dev/null
@@ -1,305 +0,0 @@
-# NeurIPS 2021: MineRL Competition Starter Kit
-
-[![Discord](https://img.shields.io/discord/565639094860775436.svg)](https://discord.gg/BT9uegr)
-
-
-This repository is the main MineRL 2021 Competition **submission template and starter kit**! Compete to solve obtaining diamond now!
-
-**This repository contains**:
-*  **Documentation** on how to submit your agent to the leaderboard
-*  **The procedure** for Round 1 and Round 2
-*  **Starter code** for you to base your submission!
-
-**Other Resources**:
-- [MineRL Competition Page](https://www.aicrowd.com/challenges/neurips-2021-minerl-competition) - Main registration page & leaderboard.
-- [MineRL Documentation](http://minerl.io/docs) - Documentation for the `minerl` package and dataset!
-- [Example Baselines](https://github.com/minerllabs/baselines) - A set of competition and non-competition baselines for `minerl`.
-
-
-![](https://i.imgur.com/XB1WORT.gif)
-
-#  Competition overview and tracks
-
-The competition is centered around one goal: **obtain diamond in Minecraft** from a random starting location without any items.
-There are two separate tracks with their separate rules and leaderboards. You may participate to the both or only one of them.
-You need to make different submissions for the two tracks. To choose the track for a submission, see description of the `aicrowd.json` file below. 
-
-* **Intro** track uses the `MineRLObtainDiamond-v0` environment, which provides original observation and action spaces of the environment. In this track
-you are free to use any means to reach the diamond, e.g. script the agent (see the baseline solutions), or train the agent or use both! *Intro* track
-only has one round (Round 1). **No training happens on the AICrowd evaluator side**, you only need to worry about the `test_submission_code.py` file.
-* **Research** track uses the `MineRLObtainDiamondVectorObf-v0` environment, in which both observation and action spaces are obfuscated to prevent
-manually coding actions (this is also prohibited by the rules). The amount of training is also restricted to 8M samples and four days (see rules). **Research** track has two rounds (Round 1 and 2).
-
-#  Competition Procedure - Intro track
-
-In the intro track you will train your agents locally and upload them to AICrowd (via git) to be evaluated by the organizers.
-
-1. **Sign up** to join the competition [on the AIcrowd website.](https://www.aicrowd.com/challenges/neurips-2021-minerl-competition)
-2. **Clone** this repo and start developing your submissions.
-3. **Update** `aicrowd.json` file (team information, track information, etc. See details below).
-4. **Train** your agents locally, place them under `./train` directory, update `test_submission_code.py` with your agent code and make sure the submission package works correctly with `utility/evaluation_locally.sh`.
-5. [**Submit**](https://github.com/minerllabs/competition_submission_starter_template#how-to-submit-a-model) your trained models to [AIcrowd Gitlab](https://gitlab.aicrowd.com) for evaluation [(full instructions below)](#how-to-submit-a-model).  The automated evaluation setup will evaluate the submissions against the validation environment, to compute and report the metrics on the leaderboard of the competition.
-
-After Round 1 ends, organizers will inspect the code repositories of the top participants to ensure compliance with the competition rules, after which intro track winners are announced.
-
-#  Competition Procedure - Research track
-
-In the Round 1 of research track you will train your agents locally with a limited number of samples and then upload them to AIcrowd (via git) to be evaluated by the organizers.
-
-![](http://minerl.io/assets/images/round1_procedure.png)
-
-1. **Sign up** to join the competition [on the AIcrowd website.](https://www.aicrowd.com/challenges/neurips-2021-minerl-competition)
-2. **Clone** this repo  and start developing your submissions.
-3. **Update** `aicrowd.json` file (team information, track information, etc. See details below).
-4. **Train** your models using the `utility/train_locally.sh` script (training code **must** be inside `train_submission_code.py` file), update `test_submission_code.py` code as well and make sure the submission package works correctly with `utility/evaluation_locally.sh`.
-5. [**Submit**](https://github.com/minerllabs/competition_submission_starter_template#how-to-submit-a-model) your trained models to [AIcrowd Gitlab](https://gitlab.aicrowd.com) for evaluation [(full instructions below)](#how-to-submit-a-model). The automated evaluation setup will evaluate the submissions against the validation environment, to compute and report the metrics on the leaderboard of the competition.
-
-Note that you **must** submit your training code during Round 1 as well! Organizers use this to verify that your training follows the competition rules.
-
-Once Round 1 is complete, the organizers will examine the code repositories of the top submissions on the leaderboard to ensure compliance with the competition rules. 
-
-In Round 2 (Research track only), top participants of Round 1 will be invited to submit their submissions, with the evaluator system this time training the agent on the organizer's server before evaluating this. No pre-trained agents are submitted!
-
-# How to Submit a Model!
-
-In brief: you define your Python environment using Anaconda environment files, and AICrowd system will build a Docker image and run your code using the docker scripts inside the `utility` directory.
-
-## Setup
-
-1.  **Clone the github repository** or press the "Use this Template" button on GitHub!
-
-    ```
-    git clone https://github.com/minerllabs/competition_submission_starter_template.git
-    ```
-
-2. **Install** competition specific dependencies! **Make sure you have the [JDK 8 installed first](http://minerl.io/docs/tutorials/getting_started.html)!**
-    ```
-    # 1. Make sure to install the JDK first
-    # -> Go to http://minerl.io/docs/tutorials/getting_started.html
-
-    # 2. Install the `minerl` package and its dependencies.
-    ```
-
-3. **Specify** your specific submission dependencies (PyTorch, Tensorflow, kittens, puppies, etc.)
-
-    * **Anaconda Environment**. To make a submission you need to specify the environment using Anaconda environment files. It is also recommended you recreate the environment on your local machine. Make sure at least version `4.5.11` is required to correctly populate `environment.yml` (By following instructions [here](https://www.anaconda.com/download)). Then:
-       * **Create your new conda environment**
-
-            ```sh
-            conda env create -f environment.yml 
-            conda activate minerl
-            ```
-      * **Your code specific dependencies**
-        Add your own dependencies to the `environment.yml` file. **Remember to add any additional channels**. PyTorch requires channel `pytorch`, for example.
-        You can also install them locally using
-        ```sh
-        conda install <your-package>
-        ```
-
-    * **Pip Packages** If you need pip packages (not on conda), you can add them to the `environment.yml` file (see the currently populated version):
-
-    * **Apt Packages** If your training procedure or agent depends on specific Debian (Ubuntu, etc.) packages, add them to `apt.txt`.
-
-
-## How do I specify my software runtime ?
-
-As mentioned above, **the software runtime is specified mainly in 2 places**: 
-* `environment.yml` -- The Anaconda environment specification. 
-    If you use a conda environment to run your submission code, you can expert the exact `environment.yml` file with
-    ```
-    conda env export --no-build > environment.yml
-    ```
-
-* `apt.txt` -- The Debian packages (via aptitude) used by your training procedure!
-
-These files are used to construct both the **local and AICrowd docker containers** in which your agent will train. 
-
-If above are too restrictive for defining your environment, see [this Discourse topic for more information](https://discourse.aicrowd.com/t/how-to-specify-runtime-environment-for-your-submission/2274).
-
-## What should my code structure be like ?
-
-Please follow the example structure shared in the starter kit for the code structure.
-The different files and directories have following meaning:
-
-```
-.
-├── aicrowd.json             # Submission meta information like your username
-├── apt.txt                  # Packages to be installed inside docker image
-├── data                     # The downloaded data, the path to directory is also available as `MINERL_DATA_ROOT` env variable
-├── test_submission_code.py  # IMPORTANT: Your testing/inference phase code. NOTE: This is NOT the the entry point for testing phase!
-├── train                    # Your trained model MUST be saved inside this directory
-├── train_submission_code.py # IMPORTANT: Your training phase code (only needed for the Research track)
-├── test_framework.py        # The entry point for the testing phase, which sets up the environment. Your code DOES NOT go here.
-└── utility                  # The utility scripts which provide a smoother experience to you.
-    ├── debug_build.sh
-    ├── docker_run.sh
-    ├── environ.sh
-    ├── evaluation_locally.sh
-    ├── parser.py
-    ├── train_locally.sh
-    └── verify_or_download_data.sh
-```
-
-Finally, **you must specify an AIcrowd submission JSON in `aicrowd.json` to be scored!** 
-
-The `aicrowd.json` of each submission should contain the following content:
-
-```json
-{
-  "challenge_id": "aicrowd-neurips-2021-minerl-diamond-challenge",
-  "authors": ["your-aicrowd-username"],
-  "tags": "change-me",
-  "description": "sample description about your awesome agent",
-  "license": "MIT",
-  "gpu": true
-}
-```
-
-This JSON is used to map your submission to the said challenge, so please remember to use the correct `challenge_id` as specified above.
-
-Please specify if your code will use a GPU or not for the evaluation of your model. If you specify `true` for the GPU, a **NVIDIA Tesla K80 GPU** will be provided and used for the evaluation.
-
-**Remember: You need to specify "tags" in aicrowd.json, which need to be either `"intro"` or `"research"`.** This defines the track for which you are submitting.
-
-### Dataset location
-
-You **don't** need to upload the MineRL dataset in submission and it will be provided in online submissions at `MINERL_DATA_ROOT` path. For local training and evaluations, you can download it once in your system via `python ./utility/verify_or_download_data.py` or place manually into `./data/` folder.
-
-
-## (Research track) IMPORTANT: Saving Models during Training!
-
-**Note: This only applies to the *Research* track**
-
-Before you submit to the Research track, make sure that your code does the following.
-
-* **During training** (`train_submission_code.py`) **save your models to the `train/` folder.**
-* **During testing** (`test_submission_code.py`) **load your model from the `train/` folder.**
-
-It is absolutely imperative **that you save your models during training** (`train_submission_code.py`) so that they can be used in the evaluation phase (`test_submission_code.py`) on AICrowd, and so the organizers can verify your training code in Round 1 and train agents during Round 2!
-
-## How to submit!
-
-To make a submission, you will have to create a private repository on [https://gitlab.aicrowd.com/](https://gitlab.aicrowd.com/).
-
-You will have to add your SSH Keys to your GitLab account by following the instructions [here](https://docs.gitlab.com/ee/gitlab-basics/create-your-ssh-keys.html).
-If you do not have SSH Keys, you will first need to [generate one](https://docs.gitlab.com/ee/ssh/README.html#generating-a-new-ssh-key-pair).
-
-Then you can create a submission by making a _tag push_ to your repository on [https://gitlab.aicrowd.com/](https://gitlab.aicrowd.com/).
-**Any tag push (where the tag name begins with "submission-") to your private repository is considered as a submission**  
-Then you can add the correct git remote, and finally submit by doing :
-
-```
-cd competition_submission_starter_template
-# Add AIcrowd git remote endpoint
-git remote add aicrowd git@gitlab.aicrowd.com:<YOUR_AICROWD_USER_NAME>/competition_submission_starter_template.git
-git push aicrowd master
-
-# Create a tag for your submission and push
-git tag -am "submission-v0.1" submission-v0.1
-git push aicrowd master
-git push aicrowd submission-v0.1
-
-# Note : If the contents of your repository (latest commit hash) does not change,
-# then pushing a new tag will **not** trigger a new evaluation.
-```
-
-You now should be able to see the details of your submission at: `https://gitlab.aicrowd.com/<YOUR_AICROWD_USER_NAME>/competition_submission_starter_template/issues/`
-
-**NOTE**: Remember to update your username in the link above :wink:
-
-In the link above, you should start seeing something like this take shape (each of the steps can take a bit of time, so please be patient too :wink: ) :
-![](https://i.imgur.com/FqScw4m.png)
-
-and if everything works out correctly, then you should be able to see the final scores like this :
-![](https://i.imgur.com/u00qcif.png)
-
-**Best of Luck** :tada: :tada:
-
-# Other Concepts
-
-## (Research track) Time constraints
-
-**Note: This only applies to the research track**.
-
-### Round 1
-
-You have to train your models locally **with under 8,000,000 samples** and with **worse or comprable hardware to that above** and upload the trained model in `train/` directory. But, to make sure, your training code is compatible with further round's interface, the training code will be executed in this round as well. The constraints will be a timeout of 5 minutes.
-
-### Round 2
-
-You are expected to train your model online using the training phase docker container and output the trained model in the `train/` directory. You need to ensure that your submission is trained in under 8,000,000 samples and within a 4 day period. Otherwise, the container will be killed
-
-## Local evaluation
-
-You can perform local training and evaluation using utility scripts shared in this directory. To mimic the online training phase you can run `./utility/train_locally.sh` from repository root, you can specify `--verbose` for complete logs.
-
-```
-aicrowd_minerl_starter_kit❯ ./utility/train_locally.sh --verbose
-2019-07-22 07:58:38 root[77310] INFO Training Start...
-2019-07-22 07:58:38 crowdai_api.events[77310] DEBUG Registering crowdAI API Event : CROWDAI_EVENT_INFO training_started {'event_type': 'minerl_challenge:training_started'} # with_oracle? : False
-2019-07-22 07:58:40 minerl.env.malmo.instance.17c149[77310] INFO Starting Minecraft process: ['/var/folders/82/wsds_18s5dq321scc1j531m40000gn/T/tmpnyzpjrsc/Minecraft/launchClient.sh', '-port', '9001', '-env', '-runDir', '/var/folders/82/wsds_18s5dq321scc1j531m40000gn/T/tmpnyzpjrsc/Minecraft/run']
-2019-07-22 07:58:40 minerl.env.malmo.instance.17c149[77310] INFO Starting process watcher for process 77322 @ localhost:9001
-2019-07-22 07:58:48 minerl.env.malmo.instance.17c149[77310] DEBUG This mapping 'snapshot_20161220' was designed for MC 1.11! Use at your own peril.
-2019-07-22 07:58:48 minerl.env.malmo.instance.17c149[77310] DEBUG #################################################
-2019-07-22 07:58:48 minerl.env.malmo.instance.17c149[77310] DEBUG          ForgeGradle 2.2-SNAPSHOT-3966cea
-2019-07-22 07:58:48 minerl.env.malmo.instance.17c149[77310] DEBUG   https://github.com/MinecraftForge/ForgeGradle
-2019-07-22 07:58:48 minerl.env.malmo.instance.17c149[77310] DEBUG #################################################
-2019-07-22 07:58:48 minerl.env.malmo.instance.17c149[77310] DEBUG                Powered by MCP unknown
-2019-07-22 07:58:48 minerl.env.malmo.instance.17c149[77310] DEBUG              http://modcoderpack.com
-2019-07-22 07:58:48 minerl.env.malmo.instance.17c149[77310] DEBUG          by: Searge, ProfMobius, Fesh0r,
-2019-07-22 07:58:48 minerl.env.malmo.instance.17c149[77310] DEBUG          R4wk, ZeuX, IngisKahn, bspkrs
-2019-07-22 07:58:48 minerl.env.malmo.instance.17c149[77310] DEBUG #################################################
-2019-07-22 07:58:48 minerl.env.malmo.instance.17c149[77310] DEBUG Found AccessTransformer: malmomod_at.cfg
-2019-07-22 07:58:49 minerl.env.malmo.instance.17c149[77310] DEBUG :deobfCompileDummyTask
-2019-07-22 07:58:49 minerl.env.malmo.instance.17c149[77310] DEBUG :deobfProvidedDummyTask
-...
-```
-
-For local evaluation of your code, you can use `./utility/evaluation_locally.sh`, add `--verbose` if you want to view complete logs.
-
-```
-aicrowd_minerl_starter_kit❯ ./utility/evaluation_locally.sh
-{'state': 'RUNNING', 'score': {'score': '0.0', 'score_secondary': 0.0}, 'instances': {'1': {'totalNumberSteps': 1001, 'totalNumberEpisodes': 0, 'currentEnvironment': 'MineRLObtainDiamondVectorObf-v0', 'state': 'IN_PROGRESS', 'episodes': [{'numTicks': 1001, 'environment': 'MineRLObtainDiamondVectorObf-v0', 'rewards': 0.0, 'state': 'IN_PROGRESS'}], 'score': {'score': '0.0', 'score_secondary': 0.0}}}}
-{'state': 'RUNNING', 'score': {'score': '0.0', 'score_secondary': 0.0}, 'instances': {'1': {'totalNumberSteps': 2001, 'totalNumberEpisodes': 0, 'currentEnvironment': 'MineRLObtainDiamondVectorObf-v0', 'state': 'IN_PROGRESS', 'episodes': [{'numTicks': 2001, 'environment': 'MineRLObtainDiamondVectorObf-v0', 'rewards': 0.0, 'state': 'IN_PROGRESS'}], 'score': {'score': '0.0', 'score_secondary': 0.0}}}}
-{'state': 'RUNNING', 'score': {'score': '0.0', 'score_secondary': 0.0}, 'instances': {'1': {'totalNumberSteps': 3001, 'totalNumberEpisodes': 0, 'currentEnvironment': 'MineRLObtainDiamondVectorObf-v0', 'state': 'IN_PROGRESS', 'episodes': [{'numTicks': 3001, 'environment': 'MineRLObtainDiamondVectorObf-v0', 'rewards': 0.0, 'state': 'IN_PROGRESS'}], 'score': {'score': '0.0', 'score_secondary': 0.0}}}}
-{'state': 'RUNNING', 'score': {'score': '0.0', 'score_secondary': 0.0}, 'instances': {'1': {'totalNumberSteps': 4001, 'totalNumberEpisodes': 0, 'currentEnvironment': 'MineRLObtainDiamondVectorObf-v0', 'state': 'IN_PROGRESS', 'episodes': [{'numTicks': 4001, 'environment': 'MineRLObtainDiamondVectorObf-v0', 'rewards': 0.0, 'state': 'IN_PROGRESS'}], 'score': {'score': '0.0', 'score_secondary': 0.0}}}}
-{'state': 'RUNNING', 'score': {'score': '0.0', 'score_secondary': 0.0}, 'instances': {'1': {'totalNumberSteps': 5001, 'totalNumberEpisodes': 0, 'currentEnvironment': 'MineRLObtainDiamondVectorObf-v0', 'state': 'IN_PROGRESS', 'episodes': [{'numTicks': 5001, 'environment': 'MineRLObtainDiamondVectorObf-v0', 'rewards': 0.0, 'state': 'IN_PROGRESS'}], 'score': {'score': '0.0', 'score_secondary': 0.0}}}}
-{'state': 'RUNNING', 'score': {'score': '0.0', 'score_secondary': 0.0}, 'instances': {'1': {'totalNumberSteps': 6001, 'totalNumberEpisodes': 0, 'currentEnvironment': 'MineRLObtainDiamondVectorObf-v0', 'state': 'IN_PROGRESS', 'episodes': [{'numTicks': 6001, 'environment': 'MineRLObtainDiamondVectorObf-v0', 'rewards': 0.0, 'state': 'IN_PROGRESS'}], 'score': {'score': '0.0', 'score_secondary': 0.0}}}}
-...
-```
-
-For running/testing your submission in a docker environment (identical to the online submission), you can use `./utility/docker_train_locally.sh` and `./utility/docker_evaluation_locally.sh`. You can also run docker image with bash entrypoint for debugging on the go with the help of `./utility/docker_run.sh`. These scripts respect following parameters:
-
-* `--no-build`: To skip docker image build and use the last build image
-* `--nvidia`: To use `nvidia-docker` instead of `docker` which include your nvidia related drivers inside docker image
-
-
-# Team
-
-The quick-start kit was authored by 
-[Anssi Kanervisto](https://github.com/Miffyli) and [Shivam Khandelwal](https://twitter.com/skbly7) with help from [William H. Guss](http://wguss.ml)
-
-The competition is organized by the following team:
-
-* [William H. Guss]((http://wguss.ml)) (OpenAI and Carnegie Mellon University)
-* Alara Dirik (Boğaziçi University)
-* Byron V. Galbraith (Talla)
-* Brandon Houghton (OpenAI and Carnegie Mellon University)
-* Anssi Kanervisto (University of Eastern Finland)
-* Noboru Sean Kuno (Microsoft Research)
-* Stephanie Milani (Carnegie Mellon University)
-* Sharada Mohanty (AIcrowd)
-* Karolis Ramanauskas
-* Ruslan Salakhutdinov (Carnegie Mellon University)
-* Rohin Shah (UC Berkeley)
-* Nicholay Topin (Carnegie Mellon University)
-* Steven H. Wang (UC Berkeley)
-* Cody Wild (UC Berkeley)
-
-
-
-<img src="https://d3000t1r8yrm6n.cloudfront.net/images/challenge_partners/image_file/35/CMU_wordmark_1500px-min.png" width="50%"> 
-
-<img src="https://d3000t1r8yrm6n.cloudfront.net/images/challenge_partners/image_file/34/MSFT_logo_rgb_C-Gray.png" width="20%" style="margin-top:10px">
-
-<img src="https://raw.githubusercontent.com/AIcrowd/AIcrowd/master/app/assets/images/misc/aicrowd-horizontal.png" width="20%"> 
diff --git a/aicrowd.json b/aicrowd.json
deleted file mode 100755
index 1446823..0000000
--- a/aicrowd.json
+++ /dev/null
@@ -1,7 +0,0 @@
-{
-  "challenge_id": "neurips-2021-minerl-diamond-competition",
-  "authors": ["aicrowd-bot"],
-  "tags": "change-me",
-  "description": "Test Model for MineRL Challenge",
-  "gpu": false
-}
diff --git a/aicrowd_helper.py b/aicrowd_helper.py
deleted file mode 100755
index f9a4c8f..0000000
--- a/aicrowd_helper.py
+++ /dev/null
@@ -1,153 +0,0 @@
-#!/usr/bin/env python
-import crowdai_api
-import os
-import logging
-
-########################################################################
-# Instatiate Event Notifier
-########################################################################
-crowdai_events = crowdai_api.events.CrowdAIEvents()
-current_phase = None
-training_progress = 0.0
-
-def inference_start():
-    ########################################################################
-    # Register Inference Start event
-    ########################################################################
-    logging.info("Inference Start...")
-    global current_phase
-    current_phase = "inference"
-    crowdai_events.register_event(
-                event_type=crowdai_events.CROWDAI_EVENT_INFO,
-                message="inference_started",
-                payload={ #Arbitrary Payload
-                    "event_type": "minerl_challenge:inference_started"
-                    }
-                )
-
-def inference_end():
-    ########################################################################
-    # Register Inference End event
-    ########################################################################
-    logging.info("Inference End...")
-    global current_phase
-    current_phase = None
-    crowdai_events.register_event(
-                event_type=crowdai_events.CROWDAI_EVENT_INFO,
-                message="inference_ended",
-                payload={ #Arbitrary Payload
-                    "event_type": "minerl_challenge:inference_ended"
-                    }
-                )
-
-def inference_error():
-    ########################################################################
-    # Register Inference Error event
-    ########################################################################
-    logging.error("Inference Failed...")
-    crowdai_events.register_event(
-                event_type=crowdai_events.CROWDAI_EVENT_INFO,
-                message="inference_error",
-                payload={ #Arbitrary Payload
-                    "event_type": "minerl_challenge:inference_error"
-                    }
-                )
-
-def training_start():
-    ########################################################################
-    # Register Training Start event
-    ########################################################################
-    logging.info("Training Start...")
-    global current_phase
-    current_phase = "training"
-    crowdai_events.register_event(
-                event_type=crowdai_events.CROWDAI_EVENT_INFO,
-                message="training_started",
-                payload={ #Arbitrary Payload
-                    "event_type": "minerl_challenge:training_started"
-                    }
-                )
-
-def training_end():
-    ########################################################################
-    # Register Training End event
-    ########################################################################
-    logging.info("Training End...")
-    register_progress(1.0)
-    global current_phase
-    current_phase = None
-    crowdai_events.register_event(
-                event_type=crowdai_events.CROWDAI_EVENT_INFO,
-                message="training_ended",
-                payload={ #Arbitrary Payload
-                    "event_type": "minerl_challenge:training_ended"
-                    }
-                )
-
-def training_error():
-    ########################################################################
-    # Register Training Error event
-    ########################################################################
-    logging.error("Training Failed...")
-    crowdai_events.register_event(
-                event_type=crowdai_events.CROWDAI_EVENT_INFO,
-                message="training_error",
-                payload={ #Arbitrary Payload
-                    "event_type": "minerl_challenge:training_error"
-                    }
-                )
-
-
-def register_progress(progress):
-    ########################################################################
-    # Register Evaluation Progress event
-    # progress : float [0, 1]
-    ########################################################################
-    logging.info("Progress : {}".format(progress))
-    global training_progress, current_phase
-    if current_phase is None:
-        raise Exception('Please register current phase by calling `training_start` \
-                         or `inference_start` before sending progress.')
-    if current_phase == "training":
-        if progress < training_progress:
-            logging.warn('Invalid progress update to %f while you are already \
-                          at %f. Skipping it...', progress, training_progress)
-            return
-        training_progress = progress
-
-    crowdai_events.register_event(
-                event_type=crowdai_events.CROWDAI_EVENT_INFO,
-                message="register_progress",
-                payload={ #Arbitrary Payload
-                    "event_type": "minerl_challenge:register_progress",
-                    "training_progress" : training_progress
-                    }
-                )
-
-def submit(payload={}):
-    ########################################################################
-    # Register Evaluation Complete event
-    ########################################################################
-    logging.info("AIcrowd Submit")
-    crowdai_events.register_event(
-                event_type=crowdai_events.CROWDAI_EVENT_SUCCESS,
-                message="submit",
-                payload={ #Arbitrary Payload
-                    "event_type": "minerl_challenge:submit",
-                    },
-                blocking=True
-                )
-
-def execution_error(error):
-    ########################################################################
-    # Register Evaluation Complete event
-    ########################################################################
-    crowdai_events.register_event(
-                event_type=crowdai_events.CROWDAI_EVENT_ERROR,
-                message="execution_error",
-                payload={ #Arbitrary Payload
-                    "event_type": "minerl_challenge:execution_error",
-                    "error" : error
-                    },
-                blocking=True
-                )
diff --git a/apt.txt b/apt.txt
deleted file mode 100755
index 936abd5..0000000
--- a/apt.txt
+++ /dev/null
@@ -1,17 +0,0 @@
-curl
-git
-vim
-ssh
-gcc
-python-dev
-libsm6
-libxext6
-libxrender-dev
-libglib2.0-0
-openjdk-8-jdk
-xvfb
-x11vnc
-freeglut3-dev
-libx11-6
-python-opengl
-x11-xserver-utils
diff --git a/environment.yml b/environment.yml
deleted file mode 100755
index 837d536..0000000
--- a/environment.yml
+++ /dev/null
@@ -1,21 +0,0 @@
-name: minerl
-channels:
-  - conda-forge
-  - defaults
-  - pytorch
-  - nvidia
-dependencies:
-  - python
-  - pip
-  - py-opencv
-  - cudatoolkit=11.1
-  - cudatoolkit-dev=11.1
-  - pytorch=1.9.0=py3.9_cuda11.1_cudnn8.0.5_0
-  - torchvision
-  - pip:
-    - crowdai_api
-    - minerl
-    - coloredlogs
-    - matplotlib
-    - pyro4
-    - deepspeed
diff --git a/job_outputs/eval_dyn_7901007.out b/job_outputs/eval_dyn_7901007.out
deleted file mode 100755
index 227837a..0000000
--- a/job_outputs/eval_dyn_7901007.out
+++ /dev/null
@@ -1,44 +0,0 @@
-/home/lieberum/.conda/envs/minerl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
-  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
-Encoder img_shapes: [(64, 64), (31, 31), (15, 15), (7, 7), (3, 3)]
-
-Image sizes for deconv: [(3, 3), (7, 7), (15, 15), (31, 31), (64, 64)]
-  0%|          | 0/3524 [00:00<?, ?it/s]100%|##########| 3524/3524 [00:00<00:00, 150291.59it/s]
-/home/lieberum/.conda/envs/minerl/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.)
-  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
-predict_recursively input shape:  torch.Size([1, 256])
-model_output.shape:  torch.Size([1, 21, 128])
-model_output.shape:  torch.Size([21, 3, 64, 64])
-model_output.shape:  torch.Size([21, 3, 64, 64])
-[[[  4   4   2 ... 169 169 169]
-  [  4   4   2 ... 169 169 169]
-  [  4   4   2 ... 170 169 167]
-  ...
-  [  0   0   0 ...   0   0   0]
-  [  0   0   0 ...   0   0   0]
-  [  0   0   0 ...   0   0   0]]
-
- [[  3   3   3 ... 181 181 181]
-  [  3   3   3 ... 181 181 181]
-  [  3   3   3 ... 180 181 179]
-  ...
-  [  0   0   0 ...   0   0   0]
-  [  0   0   0 ...   0   0   0]
-  [  0   0   0 ...   0   0   0]]
-
- [[  8   8   8 ... 243 243 243]
-  [  8   8   8 ... 243 243 243]
-  [  8   8   8 ... 243 243 241]
-  ...
-  [  0   0   0 ...   0   0   0]
-  [  0   0   0 ...   0   0   0]
-  [  0   0   0 ...   0   0   0]]]
-(3, 128, 64)
-21
-Traceback (most recent call last):
-  File "eval_dynamics.py", line 107, in <module>
-    load_model_and_eval(**args)
-  File "eval_dynamics.py", line 89, in load_model_and_eval
-    plt.imshow(images[i].transpose(0,2))
-ValueError: axes don't match array
-srun: error: r26n26: task 0: Exited with exit code 1
diff --git a/job_outputs/eval_dyn_7901009.out b/job_outputs/eval_dyn_7901009.out
deleted file mode 100755
index 5aa69d2..0000000
--- a/job_outputs/eval_dyn_7901009.out
+++ /dev/null
@@ -1,39 +0,0 @@
-/home/lieberum/.conda/envs/minerl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
-  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
-Encoder img_shapes: [(64, 64), (31, 31), (15, 15), (7, 7), (3, 3)]
-
-Image sizes for deconv: [(3, 3), (7, 7), (15, 15), (31, 31), (64, 64)]
-  0%|          | 0/6045 [00:00<?, ?it/s]100%|##########| 6045/6045 [00:00<00:00, 150006.61it/s]
-/home/lieberum/.conda/envs/minerl/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.)
-  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
-predict_recursively input shape:  torch.Size([1, 256])
-model_output.shape:  torch.Size([1, 21, 128])
-model_output.shape:  torch.Size([21, 3, 64, 64])
-model_output.shape:  torch.Size([21, 3, 64, 64])
-[[[ 0  0  0 ...  0  0  0]
-  [ 0  0  0 ...  1 24 24]
-  [ 0  0  0 ...  0 21 21]
-  ...
-  [ 0  0  0 ...  0  0  0]
-  [ 0  0  0 ...  0  0  0]
-  [ 0  0  0 ...  0  0  0]]
-
- [[ 0  0  0 ...  0  1  1]
-  [ 0  0  0 ...  1 27 27]
-  [ 0  0  0 ...  0 31 31]
-  ...
-  [ 0  0  0 ...  0  0  0]
-  [ 0  0  0 ...  0  0  0]
-  [ 0  0  0 ...  0  0  0]]
-
- [[ 0  0  2 ...  0  0  0]
-  [ 0  0  2 ...  1 17 17]
-  [ 0  0  0 ...  0  7  7]
-  ...
-  [ 0  0  0 ...  0  0  0]
-  [ 0  0  0 ...  0  0  0]
-  [ 0  0  0 ...  0  0  0]]]
-(3, 128, 64)
-21
-eval_dynamics.py:88: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
-  plt.figure()
diff --git a/job_outputs/eval_dyn_7901016.out b/job_outputs/eval_dyn_7901016.out
deleted file mode 100755
index df853dc..0000000
--- a/job_outputs/eval_dyn_7901016.out
+++ /dev/null
@@ -1,14 +0,0 @@
-/home/lieberum/.conda/envs/minerl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
-  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
-Encoder img_shapes: [(64, 64), (31, 31), (15, 15), (7, 7), (3, 3)]
-
-Image sizes for deconv: [(3, 3), (7, 7), (15, 15), (31, 31), (64, 64)]
-  0%|          | 0/7990 [00:00<?, ?it/s]100%|##########| 7990/7990 [00:00<00:00, 188779.36it/s]
-/home/lieberum/.conda/envs/minerl/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.)
-  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
-predict_recursively input shape:  torch.Size([1, 256])
-model_output.shape:  torch.Size([1, 21, 128])
-model_output.shape:  torch.Size([21, 3, 64, 64])
-model_output.shape:  torch.Size([21, 3, 64, 64])
-eval_dynamics.py:84: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
-  plt.figure()
diff --git a/research_code/BCModels.py b/research_code/BCModels.py
deleted file mode 100644
index 59a12ee..0000000
--- a/research_code/BCModels.py
+++ /dev/null
@@ -1,238 +0,0 @@
-import torch
-import torch.nn as nn
-import pytorch_lightning as pl
-import numpy as np
-import einops
-
-from action_vqvae import ActionVQVAE
-from vecobs_vqvae import VecObsVQVAE
-from vqvae import VQVAE
-from visual_models import ResnetVAE
-
-
-class ActionDiscretizer(nn.Module):
-    def __init__(self, centroid_path=None, vqvae_path=None, device=None):
-        super().__init__()
-        
-        # set device
-        if device is None:
-            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
-        else:
-            self.device = device
-            
-        # set model
-        if centroid_path is None and vqvae_path is None:
-            raise ValueError('No ActionDiscretizer detected!')
-        elif centroid_path is not None and vqvae_path is not None:
-            raise ValueError('Detected both centroids and vqvae, please only supply one!')
-        elif centroid_path is not None:
-            print('Using centroids for actions..')
-            self.use_centroids = True
-            self.centroids = torch.from_numpy(np.load(centroid_path)).to(self.device)
-            self.num_actions = self.centroids.shape[0]
-        elif vqvae_path is not None:
-            print('Using vqvae for actions..')
-            raise NotImplementedError
-            self.use_centroids = False
-            self.model = ActionVQVAE.load_from_checkpoint(vqvae_path)
-            self.num_actions = 0#self.model.quantizer.embedding_dim * self.model.quantizer.latent_size
-    
-    def forward(self, x):
-        if self.use_centroids:
-            action_idcs = torch.argmin((self.centroids[None,:,:] - x[:,None,:]).pow(2).sum(dim=-1), dim=1)
-            return action_idcs
-        else:
-            return self.model.encode_only(x)[1] # return only ind
-
-class VecObsDiscretizer(nn.Module):
-    def __init__(self, vqvae_path=None, device=None):
-        super().__init__()
-        
-        # set device
-        if device is None:
-            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
-        else:
-            self.device = device
-
-        # set model
-        if vqvae_path is not None:
-            print('Using vqvae for actions..')
-            self.use_vqvae = True
-            self.model = VecObsVQVAE.load_from_checkpoint(vqvae_path)
-        else:
-            print('Not using a vecobs discretizer..')
-            self.use_vqvae = False
-            
-    def forward(self, x):
-        if self.use_vqvae:
-            return self.model.encode_only(x)[0] # return only z_q
-        else:
-            return x
-
-class FeatureExtractor(nn.Module):
-    def __init__(self, model_path, model_class, device=None):
-        super().__init__()
-        
-        # set device
-        if device is None:
-            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
-        else:
-            self.device = device
-
-        # set up first model
-        if model_class is None:
-            self.first_model = nn.Sequential(
-                nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1, stride=2), # 64 -> 32
-                nn.GELU(),
-                nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1, stride=2), # 32 -> 16
-                nn.GELU(),
-            ).to(self.device)
-        else:
-            self.first_model = model_class.load_from_checkpoint(model_path).to(self.device)
-
-        if isinstance(self.first_model, VQVAE):
-            self.need_conv = True
-            self.second_model = nn.Sequential(
-                nn.Conv2d(in_channels=self.first_model.hparams.args.embedding_dim, out_channels=256, kernel_size=3, padding=1, stride=2), # 16 -> 8
-                nn.GELU(),
-                nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1, stride=2), # 8 -> 4
-                nn.GELU(),
-                nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=2), # 4 -> 2
-                nn.GELU(),
-                nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=2), # 2 -> 1
-                nn.GELU()
-            )
-        elif isinstance(self.first_model, ResnetVAE):
-            self.need_conv = False
-            self.second_model = nn.Sequential(
-                nn.Linear(self.model.hparams.encoder_kwargs['latent_dim'], 256),
-                nn.GELU(),
-                nn.Linear(256, 512),
-                nn.GELU(),
-                nn.Linear(512, 512),
-                nn.GELU(),
-                nn.Linear(512, 512),
-                nn.GELU()
-            )
-        elif isinstance(self.first_model, nn.Sequential):
-            self.need_conv = True
-            self.second_model = nn.Sequential(
-                nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, padding=1, stride=2), # 16 -> 8
-                nn.GELU(),
-                nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1, stride=2), # 8 -> 4
-                nn.GELU(),
-                nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=2), # 4 -> 2
-                nn.GELU(),
-                nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=2), # 2 -> 1
-                nn.GELU()
-            )
-        self.second_model = self.second_model.to(self.device)
-
-    def forward(self, x):
-        if self.need_conv:
-            if isinstance(self.first_model, nn.Sequential):
-                latent = self.first_model(x)
-            else:
-                latent = self.first_model.encode_only(x)[0]
-            latent = self.second_model(latent)
-            latent = einops.rearrange(latent, 'b c h w -> b (c h w)')
-        else:
-            latent = self.first_model.encode_only(x)[2]
-            print(f'{latent.shape = }')
-            latent = self.second_model(latent)
-            print(f'{latent.shape = }')
-        return latent
-    
-    @property
-    def trainable_params(self):
-        '''
-        Only the parameters of the second model should be trained.
-        '''
-        return list(self.second_model.parameters())
-    
-    
-class BCModel(pl.LightningModule):
-    def __init__(
-        self, 
-        feature_extractor_path, 
-        feature_extractor_class, 
-        lr,
-        action_centroids_path=None, 
-        action_vqvae_path=None, 
-        vecobs_vqvae_path=None,
-        hidden_dim=1024
-    ):
-        super().__init__()
-        self.save_hyperparameters()
-        
-        self.feature_extractor = FeatureExtractor(feature_extractor_path, feature_extractor_class)
-        self.action_discretizer = ActionDiscretizer(action_centroids_path, action_vqvae_path)
-        self.vecobs_discretizer = VecObsDiscretizer(vecobs_vqvae_path)
-                
-        # compute feature dim
-        dummy_povobs = torch.zeros(1,3,64,64).to(self.feature_extractor.device)
-        dummy_features = self.feature_extractor(dummy_povobs)
-        assert len(dummy_features.shape) == 2, f"Expected len(dummy_features.shape) = 2, but got {len(dummy_features.shape) = }"
-        self.feature_dim = dummy_features.shape[1]
-        
-        # compute vecobs dim
-        dummy_vecobs = torch.zeros(1,64).to(self.vecobs_discretizer.device)
-        discretized_dummy_vecobs = self.vecobs_discretizer(dummy_vecobs)
-        self.vecobs_dim = discretized_dummy_vecobs.shape[1]
-        
-        print(f'\n{self.feature_dim = }')
-        print(f'{self.vecobs_dim = }\n')
-        
-        # set up action predictor
-        self.input_dim = self.feature_dim + self.vecobs_dim
-        self.output_dim = self.action_discretizer.num_actions
-        self.action_predictor = nn.Sequential(
-            nn.Linear(self.input_dim, hidden_dim),
-            nn.GELU(),
-            nn.Linear(hidden_dim, hidden_dim),
-            nn.GELU(),
-            nn.Linear(hidden_dim, self.output_dim)
-        )
-        
-        # set up loss function
-        self.loss_fn = nn.CrossEntropyLoss()
-        
-    def forward(self, pov_obs, vec_obs):
-        
-        # extract features and discretize
-        features = self.feature_extractor(pov_obs)
-        discr_vecobs = self.vecobs_discretizer(vec_obs)
-        
-        # cat tensors before passing through action predictor
-        predictions = self.action_predictor(torch.cat([features, discr_vecobs], dim=1))
-        
-        return predictions
-        
-    
-    def training_step(self, batch, batch_idx):
-        # unpack batch and prepare for forward pass
-        obs, actions, *_ = batch
-        vec_obs = obs['vector'].float()[0]
-        pov_obs = obs['pov'].float()[0] / 255
-        pov_obs = einops.rearrange(pov_obs, 'b h w c -> b c h w')
-        actions = actions['vector'].float()[0]
-        assert [*pov_obs.shape[1:]] == [3, 64, 64], f"pov_obs shape should end with [3, 64, 64] but is {pov_obs.shape}"
-        
-        # predict actions
-        predicted_actions = self.forward(pov_obs, vec_obs)
-        
-        # discretize target actions
-        targets = self.action_discretizer(actions)
-
-        # compute loss
-        loss = self.loss_fn(predicted_actions, targets)
-        
-        # log
-        self.log('Training/loss', loss, on_step=True)
-        
-        return loss
-    
-    def configure_optimizers(self):
-        params = list(self.action_predictor.parameters()) + self.feature_extractor.trainable_params
-        self.optimizer = torch.optim.AdamW(params, lr=self.hparams.lr)
-        return self.optimizer
\ No newline at end of file
diff --git a/research_code/DQfD.py b/research_code/DQfD.py
deleted file mode 100644
index 39bb905..0000000
--- a/research_code/DQfD.py
+++ /dev/null
@@ -1,483 +0,0 @@
-import argparse
-import os
-import random
-from collections import deque, namedtuple
-from time import time
-
-import einops
-import gym
-import minerl
-import numpy as np
-import torch
-import torch.nn as nn
-from tqdm import tqdm
-
-from torch.utils.tensorboard import SummaryWriter
-
-from DQfD_pretrain import ConvFeatureExtractor, QNetwork
-
-Transition = namedtuple('Transition',
-                        ('state', 'action', 'next_state', 'reward', 'n_step_state', 'n_step_reward', 'td_error', 'expert'))
-
-
-class MemoryDataset(torch.utils.data.Dataset):
-    
-    def __init__(self, combined_memory):
-        '''
-        Wrapper class around combined memory to make it compatible with Dataset and be used by DataLoader
-        '''
-        self.combined_memory = combined_memory
-    
-    def __len__(self):
-        return len(self.combined_memory)
-    
-    def __getitem__(self, idx):
-        state, action, next_state, reward, n_step_state, n_step_reward, td_error, expert = self.combined_memory[idx]
-        
-        pov = einops.rearrange(state['pov'], 'h w c -> c h w').astype(np.float32) / 255
-        next_pov = einops.rearrange(next_state['pov'], 'h w c -> c h w').astype(np.float32) / 255
-        n_step_pov = einops.rearrange(n_step_state['pov'], 'h w c -> c h w').astype(np.float32) / 255
-
-        vec = state['vector'].astype(np.float32)
-        next_vec = next_state['vector'].astype(np.float32)
-        n_step_vec = n_step_state['vector'].astype(np.float32)
-
-        reward = reward.astype(np.float32)
-        n_step_reward = n_step_reward.astype(np.float32)
-        
-        weight = self.weights[idx]
-
-        return (pov, vec), (next_pov, next_vec), (n_step_pov, n_step_vec), action, reward, n_step_reward, idx, weight, expert
-    
-    def add_episode(self, obs, actions, rewards, td_errors, memory_id):
-        self.combined_memory.add_episode(obs, actions, rewards, td_errors, memory_id)
-    
-    @property
-    def weights(self):
-        return self.combined_memory.weights
-
-    def update_beta(self, new_beta):
-        self.combined_memory.update_beta(new_beta)
-    
-    def update_td_errors(self, batch_idcs, updated_td_errors):
-        self.combined_memory.update_td_errors(batch_idcs, updated_td_errors)
-        
-class CombinedMemory(object):
-    def __init__(self, agent_memory_capacity, n_step, discount_factor, p_offset, alpha, beta):
-        '''
-        Class to combine expert and agent memory
-        '''
-        self.n_step = n_step
-        self.discount_factor = discount_factor
-        self.beta = beta
-        self.alpha = alpha
-        self.memory_dict = {
-            'expert':ReplayMemory(None, n_step, discount_factor, p_offset['expert'], expert=True),
-            'agent':ReplayMemory(agent_memory_capacity, n_step, discount_factor, p_offset['agent'], expert=False)
-        }
-        self.concat_memo = np.concatenate([self.memory_dict['expert'].memory, self.memory_dict['agent'].memory])
-    
-    def __len__(self):
-        return len(self.memory_dict['expert']) + len(self.memory_dict['agent'])
-    
-    def add_episode(self, obs, actions, rewards, td_errors, memory_id):
-        #time1 = time()
-        self.memory_dict[memory_id].add_episode(obs, actions, rewards, td_errors)
-        #print(f'Time to add episode = {time() - time1:.2f}s')
-
-        # recompute weights
-        #time1 = time()
-        self._update_weights()
-        #print(f'Time to update weights = {time() - time1:.2f}s')
-        if memory_id == 'expert': # TODO do this in a less hacky way
-            self.concat_memo = self.memory_dict[memory_id].memory
-
-        elif memory_id == 'agent':
-            print(len(self.memory_dict['expert'].memory))
-            print(len(self.memory_dict['agent'].memory))
-            self.concat_memo = np.concatenate([self.memory_dict['expert'].memory, self.memory_dict['agent'].memory])
-   
-    def __getitem__(self, idx):
-        return self.concat_memo[idx]
-
-    def sample(self, batch_size):
-        idcs = np.random.choice(np.arange(len(self)), size=batch_size, replace=False, p=self.weights)
-        return self.concat_memo[idcs], idcs
-
-    def update_beta(self, new_beta):
-        for key in self.memory_dict:
-            self.memory_dict[key].update_beta(new_beta)
-    
-    def _update_weights(self):
-        weights = np.array([(sars.td_error + self.memory_dict[key].p_offset) ** self.alpha for key in ['expert', 'agent'] for sars in self.memory_dict[key].memory])
-        #print(weights.shape)
-        weights /= np.sum(weights) # = P(i)
-        weights = 1 / (len(self) * weights) ** self.beta
-        self.weights = weights / np.max(weights)
-    
-    def update_td_errors(self, idcs, td_errors):
-        #time1 = time()
-        for i, idx in enumerate(idcs):
-            if idx < len(self.memory_dict['expert']):
-                self.memory_dict['expert'].memory[idx]._replace(td_error=td_errors[i])
-            else:
-                self.memory_dict['agent'].memory[idx - len(self.memory_dict['expert'])]._replace(td_error=td_errors[i])
-        #print(f'Time to update td_errors = {time() - time1:.2f}s')
-        
-        #time1 = time()        
-        self._update_weights()
-        #print(f'Time to update weights = {time() - time1:.2f}s')
-
-class ReplayMemory(object):
-
-    def __init__(self, capacity, n_step, discount_factor, p_offset, expert=False):
-        self.n_step = n_step
-        self.discount_factor = discount_factor
-        self.p_offset = p_offset
-        self.memory = deque([],maxlen=capacity)
-        self.expert = int(expert)
-
-    def push(self, *args):
-        """Save a transition"""
-        self.memory.append(Transition(*args))
-
-    def __len__(self):
-        return len(self.memory)
-    
-    def add_episode(self, obs, actions, rewards, td_errors):
-        '''
-        Adds all transitions within an episode to the memory.
-        '''
-        assert len(obs) > self.n_step, f"Expected len(obs) > self.n_step, but are {len(obs)} and {self.n_step}!"
-        discount_array = np.array([self.discount_factor ** i for i in range(self.n_step)])
-
-        for t in range(len(obs)-self.n_step):
-            state = obs[t]
-            action = actions[t]
-            reward = rewards[t]
-            td_error = td_errors[t]
-            
-            if t + self.n_step < len(obs):
-                n_step_state = obs[t+self.n_step]
-                n_step_reward = np.sum(rewards[t:t+self.n_step] * discount_array)
-                next_state = obs[t+1]
-            else:
-                raise NotImplementedError(f't = {t}, len(obs) = {len(obs)}')
-            self.push(
-                state,
-                action,
-                next_state,
-                reward,
-                n_step_state,
-                n_step_reward,
-                td_error,
-                self.expert
-            )
-        
-        
-    def update_beta(self, new_beta):
-        self.beta = new_beta
-        
-def extract_pov_vec(state_list):
-    pov = np.array([state['pov'] for state in state_list])
-    vec = np.array([state['vector'] for state in state_list])
-    return pov, vec
-
-def load_expert_demo(env_name, data_dir, num_expert_episodes, centroids, combined_memory):
-    
-    # load data
-    print(f"Loading data of {env_name}...")
-    data = minerl.data.make(env_name,  data_dir=data_dir)
-    trajectory_names = data.get_trajectory_names()
-    random.shuffle(trajectory_names)
-    print(f'{len(trajectory_names) = }')
-
-    # Add trajectories to the data until we reach the required DATA_SAMPLES.
-    for i, trajectory_name in enumerate(trajectory_names):
-        if (i+1) > num_expert_episodes:
-            break
-
-        # load trajectory
-        print(f'Loading {i+1}th episode...')
-        trajectory = list(data.load_data(trajectory_name, skip_interval=0, include_metadata=False))
-
-        # extract lists
-        obs = [trajectory[i][0] for i in range(len(trajectory))]
-        actions = [np.argmin(np.sum((np.array(trajectory[i][1]['vector'])[None,:] - centroids)**2, axis=1)) for i in range(len(trajectory))]
-        rewards = np.array([trajectory[i][2] for i in range(len(trajectory))])
-        td_errors = np.ones_like(rewards)
-
-        # add episode to memory
-        combined_memory.add_episode(obs, actions, rewards, td_errors, memory_id='expert')
-        print(f'Reward: {np.sum(rewards)}\n')
-
-
-    print('\nLoaded ',len(combined_memory.memory_dict['expert']),' expert samples!')
-
-    return combined_memory
-
-def main(env_name, max_episode_len, model_path, max_env_steps, centroids_path, training_steps_per_iteration,
-         lr, n_step, capacity, discount_factor, action_repeat, epsilon, batch_size, num_expert_episodes, data_dir, save_dir,
-         alpha, beta_0, agent_p_offset, expert_p_offset, load_from_statedict):
-    
-    torch.manual_seed(1337)
-    np.random.seed(1337)
-    random.seed(1337)
-
-    # set save dir
-    save_dir = os.path.join(save_dir, 'DQfD', env_name, str(int(time())))
-    os.makedirs(save_dir, exist_ok=True)
-    save_path = os.path.join(save_dir, 'q_net.pt')
-    print(f'\nSaving model to {save_path}!')
-    writer = SummaryWriter(log_dir=save_dir)
-    
-
-    # set device
-    device = 'cuda' if torch.cuda.is_available() else 'cpu'
-
-    # log time
-    start = time()
-
-    # set up model
-    if load_from_statedict:
-        raise NotImplementedError
-        #q_net = torch.load(model_path).to(device)
-    else:
-        q_net = QNetwork.load_from_checkpoint(model_path).to(device)
-    
-    # set up optimization
-    optimizer = torch.optim.AdamW(q_net.parameters(), lr=lr)
-    loss_fn = nn.MSELoss(reduction='none')
-    
-    # load centroids
-    centroids_path = os.path.join(centroids_path, env_name + '_150_centroids.npy') #TODO make sure that it uses the same centroids as in pretraining
-    centroids = np.load(centroids_path)
-    
-    # init memory
-    beta = beta_0
-    combined_memory = CombinedMemory(capacity, n_step, discount_factor, {'agent':agent_p_offset, 'expert':expert_p_offset}, alpha, beta)
-    # init expert memory
-    combined_memory = load_expert_demo(env_name, data_dir, num_expert_episodes, centroids, combined_memory)
-    
-    
-    # init the dataset
-    dataset = MemoryDataset(combined_memory)
-    
-    # log total environment interactions
-    total_env_steps = 0
-    num_episodes = 0
-
-    # create env    
-    env = gym.make(env_name)
-
-    time1 = time()
-    while total_env_steps < max_env_steps:
-        obs_list = []
-        action_list = []
-        rew_list = []
-        td_error_list = []
-        
-        
-        num_episodes += 1
-        print(f'\nStarting episode {num_episodes}...')
-
-        # re-init env
-        done = False
-        time1 = time()
-        obs = env.reset()
-        print(f'Resetting the environment took {time()-time1}s')
-        
-        steps = 0
-        total_reward = 0
-        obs_list.append(obs)
-        # prepare input
-        obs_pov = torch.from_numpy(einops.rearrange(obs['pov'], 'h w c -> 1 c h w').astype(np.float32) / 255).to(q_net.device)
-        obs_vec = torch.from_numpy(einops.rearrange(obs['vector'], 'd -> 1 d').astype(np.float32)).to(q_net.device)
-
-        # go to eval mode
-        q_net.eval()
-        
-        with torch.no_grad():
-            # compute q values
-            q_values = q_net(obs_pov, obs_vec)[0].squeeze()
-            time0 = time()        
-            while not done:    
-                
-                # select new action
-                #time1 = time()
-                if steps % action_repeat == 0:
-                    if np.random.rand(1)[0] < epsilon:
-                        action_ind = np.random.randint(centroids.shape[0])
-                        highest_q = q_values[action_ind].cpu().item()
-                    else:
-                        action_ind = torch.argmax(q_values, dim=0).cpu().item()
-                        highest_q = q_values[action_ind].cpu().item()
-
-                    # remap action to centroid
-                    action = {'vector': centroids[action_ind]}
-                #print(f'Selecting an action took {time()-time1}s')
-                
-                # env step
-                #time1 = time()
-                obs, rew, done, _ = env.step(action)
-                
-                # store transition
-                obs_list.append(obs)
-                rew_list.append(rew)
-                action_list.append(action_ind)
-                
-                #print(f'Taking a step and storing transition took {time()-time1}s')
-                
-                # prepare input
-                #time1 = time()
-                obs_pov = torch.from_numpy(einops.rearrange(obs['pov'], 'h w c -> 1 c h w').astype(np.float32) / 255).to(q_net.device)
-                obs_vec = torch.from_numpy(einops.rearrange(obs['vector'], 'd -> 1 d').astype(np.float32)).to(q_net.device)
-                #print(f'Preparing input took {time()-time1}s')
-                
-                # compute q values
-                #time1 = time()
-                q_values = q_net(obs_pov, obs_vec)[0].squeeze()
-                #print(f'Computing q_values took {time()-time1}s')
-
-                # record td_error
-                #time1 = time()
-                td_error_list.append(np.abs(rew + discount_factor * q_net(obs_pov, obs_vec, target=True)[0].squeeze()[torch.argmax(q_values)].cpu().item() - highest_q))
-                #print(f'Computing td_error took {time()-time1}s')
-                        
-                # bookkeeping
-                total_reward += rew
-                steps += 1
-                #print(steps)
-                total_env_steps += 1
-                if steps >= max_episode_len or total_env_steps == max_env_steps:
-                    break
-
-        print(f'\nEpisode {num_episodes}: Total reward: {total_reward}, Duration: {time()-time0}s')
-        writer.add_scalar('Training/EpisodeReward', total_reward, global_step=num_episodes)
-
-        # store episode into replay memory
-        print('\nAdding episode to memory...')
-        dataset.add_episode(obs_list, action_list, np.array(rew_list), td_error_list, memory_id='agent')
-        
-        # init dataloader
-        sampler = torch.utils.data.WeightedRandomSampler(dataset.weights, training_steps_per_iteration*batch_size, replacement=True)
-        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=6, pin_memory=True)
-
-
-        # perform k updates
-        print(f'\nPerforming {training_steps_per_iteration} parameter updates...')
-        total_loss = 0
-        updated_td_errors = {}
-        
-        # go to train mode
-        q_net.train()
-
-        for i, batch in tqdm(enumerate(dataloader)):
-            # unpack batch
-            #time1 = time()
-            state, next_state, n_step_state, action, reward, n_step_reward, batch_idcs, weights, expert_mask = batch
-            #print(f'Unpacking batch took {time()-time1}s')
-
-            pov, vec = state
-            next_pov, next_vec = next_state
-            n_step_pov, n_step_vec = n_step_state
-
-            # prepare tensors
-            pov = pov.to(device)
-            vec = vec.to(device)
-            next_pov = next_pov.to(device)
-            next_vec = next_vec.to(device)
-            n_step_pov = n_step_pov.to(device)
-            n_step_vec = n_step_vec.to(device)
-            reward = reward.to(device)
-            n_step_reward = n_step_reward.to(device)
-            action = action.to(device)
-            weights = weights.to(device)
-            expert_mask = expert_mask.to(device)
-            
-            # compute q values and choose actions
-            q_values = q_net(pov, vec)[0]
-            next_target_q_values = q_net(next_pov, next_vec, target=True).detach()
-            next_q_values = q_net(next_pov, next_vec).detach()
-            next_action = torch.argmax(next_q_values, dim=1)
-            n_step_q_values = q_net(n_step_pov, n_step_vec, target=True).detach()
-            n_step_action = torch.argmax(n_step_q_values, dim=1)
-            
-            # compute losses
-            idcs = torch.arange(0, len(q_values), dtype=torch.long, requires_grad=False)
-            selected_q_values = q_values[idcs, action]
-            selected_next_q_values = next_target_q_values[idcs, next_action]
-            selected_n_step_q_values = n_step_q_values[idcs, n_step_action]
-
-            td_error = reward + gamma * next_q_values[idcs, next_action] - q_values[idcs, action]
-
-            J_DQ = (reward + gamma * selected_next_q_values - selected_q_values)**2
-            one_step_loss = (J_DQ * weights).mean() # importance sampling scaling
-            
-            n_step_td_errors = reward + (discount_factor ** n_step) * n_step_q_values - action_q_values
-            n_step_loss = ((n_step_td_errors ** 2) * weights).mean() # importance sampling scaling
-
-            J_E = (expert_mask * q_net._large_margin_classification_loss(q_values, action)).sum() / expert_mask.sum() # only average over actual expert demos
-            loss = one_step_loss + n_step_loss + J_E
-            total_loss += loss
-
-            writer.add_scalar('Training/one_step_loss', one_step_loss, global_step=(num_episodes-1)*training_steps_per_iteration + i)
-            writer.add_scalar('Training/n_step_loss', n_step_loss, global_step=(num_episodes-1)*training_steps_per_iteration + i)
-            writer.add_scalar('Training/classification_loss', J_E, global_step=(num_episodes-1)*training_steps_per_iteration + i)
-            writer.add_scalar('Training/ratio_expert_to_agent', expert_mask.detach().float().mean(), global_step=(num_episodes-1)*training_steps_per_iteration + i)
-            
-            # update td errors
-            # update towards n_step td error since that ought to be a more accurate estimate of the 'true' error
-            dataset.update_td_errors(batch_idcs, torch.abs(n_step_td_errors))
-            
-            # backward pass and update
-            optimizer.zero_grad()
-            loss.backward()
-            optimizer.step()
-
-        mean_loss = total_loss.item() / training_steps_per_iteration
-        print(f'\nMean loss = {mean_loss}')
-        writer.add_scalar('Training/Loss', mean_loss, global_step=(num_episodes-1)*training_steps_per_iteration)
-
-        cur_dur = time()-start
-        print(f'Time elapsed so far: {cur_dur // 60}m {cur_dur % 60:.1f}s')
-        print(f'Time per iteration: {cur_dur / num_episodes:.1f}s')
-        print('\nUpdating target...')
-        q_net._update_target()
-        print('\nSaving model')
-        torch.save(q_net.state_dict(), save_path)
-        print('\nUpdating beta...')
-        beta = min(beta + (1-beta_0)/max_env_steps, 1)
-        writer.add_scalar('Training/Beta', beta, global_step=num_episodes*training_steps_per_iteration)
-        dataset.update_beta(beta)
-        print('\nUpdating dataloader...')
-        sampler = torch.utils.data.WeightedRandomSampler(dataset.weights, training_steps_per_iteration*batch_size, replacement=True)
-        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=6, pin_memory=True)
-
-if __name__ == '__main__':
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--env_name', default='MineRLNavigateDenseVectorObf-v0')
-    parser.add_argument('--centroids_path', default='/home/lieberummaas/datadisk/minerl/data')
-    parser.add_argument('--save_dir', default='/home/lieberummaas/datadisk/minerl/run_logs')
-    parser.add_argument('--data_dir', default='/home/lieberummaas/datadisk/minerl/data')
-    parser.add_argument('--max_episode_len', type=int, default=5000)
-    parser.add_argument('--max_env_steps', type=int, default=1000000)
-    parser.add_argument('--num_expert_episodes', type=int, default=194)
-    parser.add_argument('--n_step', type=int, default=50)
-    parser.add_argument('--batch_size', type=int, default=100)
-    parser.add_argument('--action_repeat', type=int, default=1)
-    parser.add_argument('--lr', type=float, default=3e-4)
-    parser.add_argument('--epsilon', type=float, default=0.01)
-    parser.add_argument('--alpha', type=float, default=0.4, help='PER exponent')
-    parser.add_argument('--beta_0', type=float, default=0.6, help='Initial PER Importance Sampling exponent')
-    parser.add_argument('--agent_p_offset', type=float, default=0.001)
-    parser.add_argument('--expert_p_offset', type=float, default=1)
-    parser.add_argument('--discount_factor', type=float, default=0.99)
-    parser.add_argument('--capacity', type=int, default=50000)
-    parser.add_argument('--training_steps_per_iteration', type=int, default=200)
-    parser.add_argument('--model_path', help='Path to the (pretrained) DQN', required=True)
-    parser.add_argument('--load_from_statedict', action='store_true', help='loads model from state dict instead, used when continuing training')
-    
-    args = parser.parse_args()
-    
-    main(**vars(args))
diff --git a/research_code/DQfD_pretrain.py b/research_code/DQfD_pretrain.py
index d638a79..233d95a 100644
--- a/research_code/DQfD_pretrain.py
+++ b/research_code/DQfD_pretrain.py
@@ -1,230 +1,22 @@
-import torch
-import torch.nn as nn
-from torch.utils.data import DataLoader, random_split
-import pytorch_lightning as pl
-from pytorch_lightning.callbacks import ModelCheckpoint
-from einops.layers.torch import Rearrange
-
-import numpy as np
-import os
 import argparse
-import einops
 from copy import deepcopy
+import os
+import random 
 
-from vqvae import VQVAE
-from vae_model import VAE
+import einops
+import numpy as np
+import pytorch_lightning as pl
+from pytorch_lightning.callbacks import ModelCheckpoint
+from pytorch_lightning.loggers import WandbLogger
+import torch
+from torch.utils.data import DataLoader
+import wandb
 
 import datasets
-
-class ResBlock(nn.Module):
-    def __init__(self, input_channels, channel):
-        super().__init__()
-
-        self.conv = nn.Sequential(
-            nn.Conv2d(input_channels, channel, 3, padding=1),
-            nn.ReLU(inplace=True),
-            nn.Conv2d(channel, input_channels, 1),
-        )
-
-    def forward(self, x):
-        out = self.conv(x)
-        out += x
-        out = nn.functional.relu(out)
-        return out
-
-class ConvFeatureExtractor(nn.Module):
-    def __init__(self, n_hid=86, latent_dim=64):
-        super().__init__()
-        self.conv = nn.Sequential(
-            nn.Conv2d(3, n_hid, 4, stride=2, padding=1),
-            nn.ReLU(inplace=True),
-            nn.Conv2d(n_hid, 2*n_hid, 4, stride=2, padding=1),
-            nn.ReLU(inplace=True),
-            nn.Conv2d(2*n_hid, 2*n_hid, 3, padding=1),
-            nn.ReLU(inplace=True),
-            ResBlock(2*n_hid, 2*n_hid//4),
-            ResBlock(2*n_hid, 2*n_hid//4)
-        )
-        
-    def forward(self, x):
-        return self.conv(x)
-    
-    @torch.no_grad()
-    def encode_only(self, x):
-        return self(x), None, None
-    
-    def encode_with_grad(self, x):
-        return self(x), 0, None, None
-    
-    @property
-    def device(self):
-        return list(self.conv.parameters())[0].device
-
-class QNetwork(pl.LightningModule):
-    
-    def __init__(
-        self, 
-        n_actions, # number of distinct actions
-        optim_kwargs, 
-        target_update_rate, # how often to update the target network
-        margin, # margin in the classification loss
-        discount_factor, 
-        horizon, # time horizon for the N-step TD error
-        feature_extractor_cls=VQVAE,
-        feature_extractor_path=None,
-        feature_extractor_kwargs={},
-        freeze_feature_extractor=False
-    ):
-        super().__init__()
-        self.save_hyperparameters()
-        
-        # set up feature extractor
-        if feature_extractor_cls in [VQVAE, VAE]:
-            self.feature_extractor = feature_extractor_cls.load_from_checkpoint(feature_extractor_path)
-        elif feature_extractor_cls == ConvFeatureExtractor:
-            self.feature_extractor = feature_extractor_cls(**feature_extractor_kwargs)
-        else:
-            raise ValueError(f'Unrecognized feature extractor class {feature_extractor_cls}')
-
-        # conv feature extractor
-        dummy, dummy_idcs, _ = self.feature_extractor.encode_only(torch.ones(2,3,64,64).float().to(self.feature_extractor.device))
-        num_channels = dummy.shape[1]
-        self.conv_net = nn.Sequential(
-            nn.Conv2d(in_channels=num_channels, out_channels=128, kernel_size=3, padding=1, stride=1), # 16 -> 8
-            nn.GELU(),
-            nn.AdaptiveMaxPool2d((8,8)),
-            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1, stride=1), # 8 -> 4
-            nn.GELU(),
-            nn.AdaptiveMaxPool2d((4,4)),
-            #nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, padding=1, stride=2), # 4 -> 2
-            nn.AdaptiveMaxPool2d((2,2)),
-            Rearrange('b c h w -> b (c h w)')
-        )
-        dummy = self.conv_net(dummy)
-        pov_feature_dim = dummy.shape[1]
-
-        self.vecobs_featurizer = nn.Sequential(
-            nn.Linear(64, 100),
-            nn.GELU(),
-            nn.Linear(100, 100)
-        )
-
-        self.q_net = nn.Sequential(
-            nn.Linear(100 + pov_feature_dim, 150),
-            nn.GELU(),
-            nn.Linear(150, self.hparams.n_actions)
-        )
-        
-        # init target net
-        self._update_target()
-        
-        # loss function
-        self.loss_fn = nn.MSELoss()
-    
-    def _update_target(self):
-        self.target_net = nn.ModuleDict({
-            'feature_extractor':deepcopy(self.feature_extractor),
-            'conv_net':deepcopy(self.conv_net),
-            'vecobs_featurizer':deepcopy(self.vecobs_featurizer),
-            'q_net':deepcopy(self.q_net),
-        })
-        self.target_net.eval()
-        
-    def forward(self, pov, vec_obs, target=False):
-        if target:
-            # extract pov features
-            pov_out, _, _ = self.target_net['feature_extractor'].encode_only(pov)
-            
-            # apply conv net
-            pov_out = self.target_net['conv_net'](pov_out)
-            
-            # extract vec obs features
-            vec_out = self.target_net['vecobs_featurizer'](vec_obs)
-            
-            # compute q_values
-            q_values = self.target_net['q_net'](torch.cat([pov_out, vec_out], dim=1))
-            
-            return q_values
-        else:
-            # extract pov features
-            if self.hparams.freeze_feature_extractor:
-                pov_out, *_ = self.feature_extractor.encode_only(pov)
-                codebook_loss = 0
-            else:
-                pov_out, codebook_loss, _, _ = self.feature_extractor.encode_with_grad(pov)
-            
-            # apply conv net
-            pov_out = self.conv_net(pov_out)
-            
-            # extract vec obs features
-            vec_out = self.vecobs_featurizer(vec_obs)
-            
-            # compute q_values
-            q_values = self.q_net(torch.cat([pov_out, vec_out], dim=1))
-
-            return q_values, codebook_loss
-    
-    def _large_margin_classification_loss(self, q_values, expert_action):
-        '''
-        Computes the large margin classification loss J_E(Q) from the DQfD paper
-        '''
-        idcs = torch.arange(0,len(q_values),dtype=torch.long)
-        q_values = q_values + self.hparams.margin
-        q_values[idcs, expert_action] = q_values[idcs, expert_action] - self.hparams.margin
-        return torch.max(q_values, dim=1)[0] - q_values[idcs,expert_action]
-    
-    def training_step(self, batch, batch_idx):
-        pov, vec_obs, action, reward, next_pov, next_vec_obs, n_step_reward, n_step_pov, n_step_vec_obs = batch
-        
-        # predict q values
-        q_values, codebook_loss = self(pov, vec_obs)
-        action = action.detach()
-        target_next_q_values = self(next_pov, next_vec_obs, target=True).detach()
-        base_next_action = torch.argmax(self(next_pov, next_vec_obs)[0].detach(), dim=1)
-        target_n_step_q_values = self(n_step_pov, n_step_vec_obs, target=True).detach()
-        base_n_step_action = torch.argmax(self(n_step_pov, n_step_vec_obs)[0].detach(), dim=1)
-        
-        # compute the individual losses
-        idcs = torch.arange(0, len(q_values), dtype=torch.long, requires_grad=False)
-        expert_q_values = q_values[idcs, action].mean()
-        other_q_values =  deepcopy(q_values.detach())
-        other_q_values[idcs, action] = 0
-        other_q_values = other_q_values.mean()
-        classification_loss = self._large_margin_classification_loss(q_values, action).mean()
-        one_step_loss = self.loss_fn(q_values[idcs, action], reward + self.hparams.discount_factor * target_next_q_values[idcs, base_next_action])
-        n_step_loss = self.loss_fn(q_values[idcs, action], n_step_reward + (self.hparams.discount_factor ** self.hparams.horizon) * target_n_step_q_values[idcs, base_n_step_action])
-
-        # sum up losses
-        loss = classification_loss + one_step_loss + n_step_loss + codebook_loss
-
-        # compute perc where expert action has highest q_value for logging
-        expert_agent_agreement = (torch.argmax(q_values, dim=1) == action).sum() / q_values.shape[0]
-        
-        # logging
-        self.log('Training/1-step TD Error', one_step_loss, on_step=True)
-        self.log('Training/ClassificationLoss', classification_loss, on_step=True)
-        self.log('Training/n-step TD Error', n_step_loss, on_step=True)
-        self.log('Training/Loss', loss, on_step=True)
-        self.log('Training/ExpertAgentAgreement', expert_agent_agreement, on_step=True)
-        self.log('Training/ExpertQValues', expert_q_values, on_step=True)
-        self.log('Training/OtherQValues', other_q_values, on_step=True)
-        self.logger.experiment.add_histogram('Training/Actions', action, global_step=self.global_step)
-        
-        return loss
-    
-    def on_after_backward(self):
-        if (self.global_step + 1) % self.hparams.target_update_rate == 0:
-            print(f'\nGlobal step {self.global_step+1}: Updating Target Network\n')
-            self._update_target()
-        
-    def configure_optimizers(self):
-        # set up optimizer
-        params = list(self.conv_net.parameters()) + list(self.vecobs_featurizer.parameters()) + list(self.q_net.parameters())
-        if not self.hparams.freeze_feature_extractor:
-            params += list(self.feature_extractor.parameters())
-        optimizer = torch.optim.AdamW(params, **self.hparams.optim_kwargs)
-        return optimizer
-
+from DQfD_models import QNetwork, ConvFeatureExtractor
+from dynamics_models import MDN_RNN
+from vae_model import VAE
+from vqvae import VQVAE
 
 def main(
     env_name, 
@@ -232,94 +24,147 @@ def main(
     num_workers, 
     lr, 
     weight_decay, 
-    feature_extractor_path, 
     data_dir, 
     log_dir,
     epochs, 
-    feature_extractor_cls, 
-    freeze_feature_extractor,
+    num_expert_episodes,
+    visual_model_cls, 
+    visual_model_path, 
+    unfreeze_visual_model,
+    dynamics_model_cls, 
+    dynamics_model_path, 
+    unfreeze_dynamics_model,
     centroids_path, 
+    num_centroids,
     target_update_rate, 
     margin, 
     discount_factor, 
-    horizon
+    horizon,
+    use_one_hot
 ):
+    # random seed
     pl.seed_everything(1337)
+    random.seed(1337)
 
+    # some sanity checks
+    if batch_size > 1: 
+        raise NotImplementedError
+    
+    if visual_model_cls == 'conv' and ~unfreeze_visual_model:
+        raise ValueError("Mustn't freeze_visual_model when using conv!")
 
-    # load centroids
-    centroids_path = os.path.join(centroids_path, env_name + '_150_centroids.npy')
-    print(f'\nLoading centroids from {centroids_path}...')
+    # make sure that relevant dirs exist
+    os.makedirs(log_dir, exist_ok=True)
+    print(f'\nSaving logs and model to {log_dir}')
+    
+    # set up WandB logger
+    config = dict(
+        env_name=env_name,
+        visual_model_cls=visual_model_cls,
+        visual_model_path=visual_model_path,
+        freeze_visual_model=~unfreeze_visual_model,
+        dynamics_model_cls=dynamics_model_cls,
+        dynamics_model_path=dynamics_model_path,
+        freeze_dynamics_model=~unfreeze_dynamics_model,
+        use_one_hot=use_one_hot
+    )
+    if dynamics_model_cls is not None:
+        wandb_logger = WandbLogger(project='DQfD_pretraining', config=config, tags=[visual_model_cls, dynamics_model_cls, 'one_hot_'+str(use_one_hot)])
+    else:
+        wandb_logger = WandbLogger(project='DQfD_pretraining', config=config, tags=[visual_model_cls, 'one_hot_'+str(use_one_hot)])
+
+    # load centroids for action discretization
+    centroids_path = os.path.join(centroids_path, env_name + f'_{num_centroids}_centroids.npy')
     centroids = np.load(centroids_path)
-    print(f'Loaded centroids! Shape is {centroids.shape}.')
+    print(f'\nLoaded centroids from {centroids_path}! Shape is {centroids.shape}.')
 
-
-    if feature_extractor_cls == 'conv' and freeze_feature_extractor:
-        raise ValueError("Mustn't freeze_feature_extractor when using conv!")
-    
     ## some model kwargs
     optim_kwargs = {'lr':lr, 'weight_decay':weight_decay}
-    feature_extractor_cls = {
+    
+    visual_model_cls = {
         'vqvae':VQVAE,
         'vae':VAE,
         'conv':ConvFeatureExtractor
-    }[feature_extractor_cls]
+    }[visual_model_cls]
+    
+    dynamics_model_cls = {
+        'mdn':MDN_RNN,
+        None:None
+    }[dynamics_model_cls]
+    
     model_kwargs = {
-        'feature_extractor_path':feature_extractor_path,
+        'visual_model_path':visual_model_path,
         'optim_kwargs':optim_kwargs,
         'n_actions':centroids.shape[0],
         'target_update_rate':target_update_rate,
         'margin':margin,
         'discount_factor':discount_factor,
         'horizon':horizon,
-        'feature_extractor_cls':feature_extractor_cls,
-        'freeze_feature_extractor':freeze_feature_extractor
+        'visual_model_cls':visual_model_cls,
+        'freeze_visual_model':~unfreeze_visual_model,
+        'dynamics_model_cls':dynamics_model_cls, 
+        'dynamics_model_path':dynamics_model_path, 
+        'freeze_dynamics_model':~unfreeze_dynamics_model,
+        'use_one_hot':use_one_hot
     }
     
-    # make sure that relevant dirs exist
-    run_name = f'QNetwork/{env_name}/{feature_extractor_cls.__name__}'
-    log_dir = os.path.join(log_dir, run_name)
-    os.makedirs(log_dir, exist_ok=True)
-    print(f'\nSaving logs and model to {log_dir}')
-
+    # set up model
     model = QNetwork(**model_kwargs)
-        
     
     # load data
-    train_data = datasets.PretrainQNetIterableData(env_name, data_dir, centroids, horizon, discount_factor, num_workers)
+    train_data = datasets.TrajectoryIterData(env_name, data_dir, num_expert_episodes, centroids, num_workers=num_workers)
     train_loader = DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, pin_memory=True)
     
+    # set up trainer
     model_checkpoint = ModelCheckpoint(mode="min", monitor='Training/Loss', save_last=True, every_n_train_steps=500)
     trainer=pl.Trainer(
-                    progress_bar_refresh_rate=1, #every N batches update progress bar
-                    log_every_n_steps=10,
-                    callbacks=[model_checkpoint],
-                    gpus=torch.cuda.device_count(),
-                    #accelerator='dp', #anything else here seems to lead to crashes/errors
-                    default_root_dir=log_dir,
-                    max_epochs=epochs
-                )
+        logger=wandb_logger,
+        progress_bar_refresh_rate=1, #every N batches update progress bar
+        log_every_n_steps=10,
+        callbacks=[model_checkpoint],
+        gpus=torch.cuda.device_count(),
+        default_root_dir=log_dir,
+        max_epochs=epochs,
+        #track_grad_norm=2
+    )
+
+    # train
     trainer.fit(model, train_loader)
 
 
 if __name__ == '__main__':
     parser = argparse.ArgumentParser()
     parser.add_argument('--env_name', default='MineRLNavigateDenseVectorObf-v0')
-    parser.add_argument('--batch_size', default=100, type=int)
-    parser.add_argument('--num_workers', default=6, type=int)
-    parser.add_argument('--lr', default=3e-4, type=float)
-    parser.add_argument('--weight_decay', default=1e-5, type=float)
-    parser.add_argument('--discount_factor', default=0.99, type=float)
-    parser.add_argument('--margin', default=0.8, type=float)
-    parser.add_argument('--horizon', default=50, type=int, help='Horizon for n-step TD error')
-    parser.add_argument('--feature_extractor_cls', choices=['vqvae', 'vae', 'conv'], default='vqvae', help='Class of the feature_extractor model')
-    parser.add_argument('--feature_extractor_path', help='Path to feature_extractor model')
-    parser.add_argument('--freeze_feature_extractor', action='store_true', help='Whether to freeze or finetune the feature extractor')
     parser.add_argument('--data_dir', default='/home/lieberummaas/datadisk/minerl/data')
     parser.add_argument('--log_dir', default='/home/lieberummaas/datadisk/minerl/run_logs')
+    parser.add_argument('--centroids_path', type=str, default='/home/lieberummaas/datadisk/minerl/data/')
+    parser.add_argument('--num_centroids', type=int, default=150)
+    
+    # training args
     parser.add_argument('--epochs', default=10, type=int)
+    parser.add_argument('--num_expert_episodes', default=300, type=int)
     parser.add_argument('--target_update_rate', default=100, type=int, help='How often to update target network')
-    parser.add_argument('--centroids_path', type=str, default='/home/lieberummaas/datadisk/minerl/data/')
+    parser.add_argument('--batch_size', default=1, type=int)
+    parser.add_argument('--num_workers', default=0, type=int)
+    parser.add_argument('--lr', default=3e-4, type=float)
+    parser.add_argument('--weight_decay', default=1e-5, type=float)
+    
+    # Q-learning args
+    parser.add_argument('--discount_factor', default=0.99, type=float)
+    parser.add_argument('--margin', default=0.8, type=float)
+    parser.add_argument('--horizon', default=10, type=int, help='Horizon for n-step TD error')
+    parser.add_argument('--use_one_hot', action='store_true', help='whether to use one-hot representation')
+    
+    # feature extractor args
+    parser.add_argument('--visual_model_cls', choices=['vqvae', 'vae', 'conv'], default='vae', help='Class of the visual_model model')
+    parser.add_argument('--visual_model_path', help='Path to visual_model model')
+    parser.add_argument('--unfreeze_visual_model', action='store_true', help='Whether to freeze or finetune the feature extractor')
+    
+    # dynamics model args
+    parser.add_argument('--dynamics_model_cls', choices=['mdn', None], default=None, help='Class of the dynamics model')
+    parser.add_argument('--dynamics_model_path', default=None, help='Path to dynamics model')
+    parser.add_argument('--unfreeze_dynamics_model', action='store_true', help='Whether to freeze or finetune the dynamics model extractor')
+    
     
     args = parser.parse_args()
     
diff --git a/research_code/analyze_quantized_actions.py b/research_code/analyze_quantized_actions.py
deleted file mode 100644
index eba26c9..0000000
--- a/research_code/analyze_quantized_actions.py
+++ /dev/null
@@ -1,68 +0,0 @@
-from action_vqvae import ActionVQVAE
-import argparse
-import os
-import minerl
-import torch
-import numpy as np
-import pandas as pd
-from tqdm import tqdm
-import matplotlib.pyplot as plt
-import matplotlib.colors as colors
-
-def main(
-    env_name,
-    save_dir,
-    action_quantizer
-):
-
-    # set data path
-    data_path = os.path.join(save_dir, env_name, f'actions_version_{action_quantizer}.npz')
-
-    # load data
-    data = np.load(data_path)
-    deobf_data = data['unique_deobf']
-    obf_data = data['all_obf']
-
-    # 
-    unique_quantized = pd.DataFrame(obf_data).drop_duplicates().to_numpy()
-
-    # check if there is at least one collision
-    if len(deobf_data) == len(unique_quantized):
-        print("\n'deobf_data' and 'unqiue_quantized' have the same number of elements!")
-        print('That means there is no possible collision!')
-        print('Terminating early, without plotting...')
-        return
-    else:
-        print(f'Number of unique actions = {len(deobf_data)}')
-        print(f'Number of unique clusters = {len(unique_quantized)}')
-
-    # for every unique quantized action, find all deobf actions which get mapped to that quantized action
-    cluster_id_to_deobf = {}
-    for i in tqdm(range(len(unique_quantized))):
-        cluster_id_to_deobf[i] = deobf_data[(obf_data == unique_quantized[i]).all(axis=1)]
-        #print(np.std(cluster_id_to_deobf[i], axis=0))
-        #print(cluster_id_to_deobf[i])
-        #fig = plt.figure(figsize=(6,100))
-        #print(np.min(1 + np.abs(np.min(cluster_id_to_deobf[i])) + cluster_id_to_deobf[i]))
-        #plt.imshow(np.log(1 + np.abs(np.min(cluster_id_to_deobf[i])) + cluster_id_to_deobf[i]), cmap='viridis', interpolation='nearest')
-        #plt.savefig(os.path.join(save_dir, env_name, f'cluster_0_version_{action_quantizer}.png'))
-        #raise ValueError
-    #print(cluster_id_to_deobf)
-
-    # plot number of collisions
-    num_collisions = [v.shape[0] for v in cluster_id_to_deobf.values()]
-    fig = plt.figure()
-    plt.bar(np.arang e(len(num_collisions)), num_collisions)
-    plt.ylabel('Collisions')
-    plt.xlabel('Action ID')
-    plt.savefig(os.path.join(save_dir, env_name, f'collisions_version_{action_quantizer}.png'))
-
-if __name__ == '__main__':
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--env_name', type=str, default='MineRLTreechop-v0')
-    parser.add_argument('--save_dir', type=str, default='/home/lieberummaas/datadisk/minerl/action_analysis')
-    parser.add_argument('--action_quantizer', type=int, default=0)
-    
-    args = parser.parse_args()
-    
-    main(**vars(args))
\ No newline at end of file
diff --git a/research_code/analyze_quantized_vecobs.py b/research_code/analyze_quantized_vecobs.py
deleted file mode 100644
index 203ee54..0000000
--- a/research_code/analyze_quantized_vecobs.py
+++ /dev/null
@@ -1,66 +0,0 @@
-from vecobs_vqvae import VecObsVQVAE
-import argparse
-import os
-import minerl
-import torch
-import numpy as np
-import pandas as pd
-from tqdm import tqdm
-import matplotlib.pyplot as plt
-import matplotlib.colors as colors
-
-def main(
-    env_name,
-    save_dir,
-    vecobs_quantizer
-):
-
-    # set data path
-    data_path = os.path.join(save_dir, env_name, f'obs_version_{vecobs_quantizer}.npz')
-
-    # load data
-    data = np.load(data_path)
-    deobf_data = data['unique_deobf']
-    obf_data = data['all_obf']
-
-    # 
-    unique_quantized = pd.DataFrame(obf_data).drop_duplicates().to_numpy()
-
-    # check if there is at least one collision
-    print(f'Number of unique obs = {len(deobf_data)}')
-    print(f'Number of unique clusters = {len(unique_quantized)}')
-    if len(deobf_data) == len(unique_quantized):
-        print("\n'deobf_data' and 'unqiue_quantized' have the same number of elements!")
-        print('That means there is no possible collision!')
-        print('Terminating early, without plotting...')
-        return
-        
-    # for every unique quantized vecobs, find all deobf obs which get mapped to that quantized vecobs
-    cluster_to_deobf = [deobf_data[(obf_data == quant).all(axis=1)] for quant in tqdm(unique_quantized)]
-    num_collisions = np.array([v.shape[0] for v in cluster_to_deobf])
-    sorting_idcs = np.argsort(num_collisions)
-    cluster_to_deobf = [cluster_to_deobf[i] for i in np.argsort(num_collisions)]
-
-    # plot standard deviation within one cluster
-    fig = plt.figure(figsize=(6,100))
-    stds = [np.std(cluster, axis=0) for cluster in cluster_to_deobf]
-    plt.imshow(stds, cmap='viridis', interpolation='nearest')
-    plt.savefig(os.path.join(save_dir, env_name, f'std_version_{vecobs_quantizer}.png'))
-    
-    # plot number of collisions
-    fig = plt.figure()
-    plt.bar(np.arange(len(num_collisions)), num_collisions)
-    plt.ylim(0, 250)
-    plt.ylabel('Collisions')
-    plt.xlabel('Action ID')
-    plt.savefig(os.path.join(save_dir, env_name, f'collisions_version_{vecobs_quantizer}.png'))
-
-if __name__ == '__main__':
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--env_name', type=str, default='MineRLObtainIronPickaxe-v0')
-    parser.add_argument('--save_dir', type=str, default='/home/lieberummaas/datadisk/minerl/vecobs_analysis')
-    parser.add_argument('--vecobs_quantizer', type=int, default=0)
-    
-    args = parser.parse_args()
-    
-    main(**vars(args))
\ No newline at end of file
diff --git a/research_code/codebook_size_experiment.sh b/research_code/codebook_size_experiment.sh
deleted file mode 100644
index 9406065..0000000
--- a/research_code/codebook_size_experiment.sh
+++ /dev/null
@@ -1,8 +0,0 @@
-#!/bin/bash
-LOGDIR="/home/lieberummaas/datadisk/minerl/experiment_logs"
-for num_embeddings in 1 2 4 8 16 32 64 128 256
-    do
-        echo "Now training with ${num_embeddings} embeddings"
-        python vqvae.py --log_dir $LOGDIR --num_embeddings $num_embeddings --suffix $(printf $num_embeddings)
-    done
-~         
\ No newline at end of file
diff --git a/research_code/count_rew_frequencies.py b/research_code/count_rew_frequencies.py
deleted file mode 100644
index adfd1d1..0000000
--- a/research_code/count_rew_frequencies.py
+++ /dev/null
@@ -1,68 +0,0 @@
-import numpy as np
-import os
-import argparse
-from tqdm import tqdm 
-import torch
-import einops 
-
-import datasets
-from vecobs_vqvae import VecObsVQVAE
-
-class Counter():
-    def __init__(self, data, quantizer):
-        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
-        self.data = data
-        self.quantizer = quantizer.to(self.device)
-    
-        self.reward_hashmap = {2**i:i for i in range(10)}
-        self.obs_counter = {2**i:[] for i in range(10)}
-
-    def process_batch(self, batch):
-        obs, _, rew, *_ = batch
-        vec_obs = torch.from_numpy(obs['vector']).float().to(self.device)
-        #print(f'{vec_obs.shape = }')
-        z_q, ind = self.quantizer.encode_only(vec_obs)
-        ind = einops.rearrange(ind, '(b l) -> b l', b=self.data.batch_size)
-        
-        for i, r in enumerate(rew):
-            if r > 1:
-                self.obs_counter[r].append(ind[i].to('cpu').numpy())
-
-    def iter_through_data(self):
-        for i, batch in tqdm(enumerate(self.data)):
-            self.process_batch(batch)
-            print(self.obs_counter[1024])
-            #if (i+1) % 1 == 0:
-                
-    
-
-def main(
-    env_name, 
-    data_dir, 
-    log_dir, 
-    batch_size,
-    quantizer_version
-):
-
-    # instantiate quantizer
-    quantizer_path = os.path.join(log_dir, 'VecObsVQVAE', env_name, 'lightning_logs', 'version_'+str(quantizer_version), 'checkpoints', 'last.ckpt')
-    quantizer = VecObsVQVAE.load_from_checkpoint(quantizer_path)
-        
-    # load data
-    data = datasets.BufferedBatchDataset(env_name, data_dir, batch_size, num_epochs=1)
-
-    counter = Counter(data, quantizer)
-    counter.iter_through_data()
-
-if __name__=='__main__':
-    parser = argparse.ArgumentParser()
-
-    parser.add_argument('--data_dir', default='/home/lieberummaas/datadisk/minerl/data')
-    parser.add_argument('--log_dir', default='/home/lieberummaas/datadisk/minerl/run_logs')
-    parser.add_argument('--env_name', default='MineRLObtainIronPickaxeVectorObf-v0')
-    parser.add_argument('--batch_size', default=20000, type=int)
-    parser.add_argument('--quantizer_version', default=None, type=int)
-
-    args = vars(parser.parse_args())
-
-    main(**args)
diff --git a/research_code/datasets.py b/research_code/datasets.py
index 35ea8e8..8783679 100755
--- a/research_code/datasets.py
+++ b/research_code/datasets.py
@@ -244,6 +244,99 @@ class PretrainQNetIterableData(IterableDataset):
             worker_id = torch.utils.data.get_worker_info().id
             return self._get_stream_of_trajectories(self.names_per_worker[worker_id])
         
+class TrajectoryIterData(IterableDataset):
+    def __init__(self, env_name, data_dir, num_episodes=0, centroids=None, num_workers=0):
+        super().__init__()
+
+        self.centroids = centroids
+        self.num_workers = num_workers
+        self.pipeline = minerl.data.make(env_name, data_dir)
+        self.names = self.pipeline.get_trajectory_names()
+        self.names.sort()
+        if num_episodes > 0:
+            self.names = self.names[:num_episodes]
+
+        # split trajectories between workers
+        if self.num_workers > 0:
+            trajectories_per_worker = len(self.names) // self.num_workers
+            self.names_per_worker = {
+                worker_id: self.names[trajectories_per_worker * worker_id:trajectories_per_worker*(worker_id+1)] for worker_id in range(self.num_workers)
+            }
+        
+    def _load_trajectory(self, name):
+        print(f'Loading trajectory {self.names[idx]}..')
+        # load trajectory data
+        data = self.pipeline.load_data(name)
+        
+        # unpack data
+        obs, actions, rewards, *_ = zip(*data)
+        pov_obs, vec_obs = [item['pov'] for item in obs], [item['vector'] for item in obs]
+        pov_obs = einops.rearrange(np.array(pov_obs), 't h w c -> t c h w').astype(np.float32) / 255
+        vec_obs = np.array(vec_obs).astype(np.float32)
+        actions = np.array([ac['vector'] for ac in actions]).astype(np.float32)
+        rewards = np.array(rewards).astype(np.float32)
+        
+        if self.centroids is not None:
+            # compute action idcs
+            action_idx = np.argmin(((self.centroids[None,:,:] - actions[:,None,:]) ** 2).sum(axis=-1), axis=1).astype(np.int64)
+            return pov_obs, vec_obs, actions, action_idx, rewards
+        else:
+            return pov_obs, vec_obs, actions, rewards
+
+    def _get_stream_of_trajectories(self, names):
+        return chain.from_iterable(map(self._load_trajectory, names))
+        
+    def __iter__(self):
+        if self.num_workers == 0:
+            return self._get_stream_of_trajectories(self.names)
+        else:
+            worker_id = torch.utils.data.get_worker_info().id
+            return self._get_stream_of_trajectories(self.names_per_worker[worker_id])
+
+
+class TrajectoryData(Dataset):
+    def __init__(self, env_name, data_dir, num_episodes=0, centroids=None):
+        super().__init__()
+
+        self.centroids = centroids
+        self.num_workers = num_workers
+        self.pipeline = minerl.data.make(env_name, data_dir)
+        self.names = self.pipeline.get_trajectory_names()
+        self.names.sort()
+        if num_episodes > 0:
+            self.names = self.names[:num_episodes]
+
+        
+    def _load_trajectory(self, name):
+        # load trajectory data
+        data = self.pipeline.load_data(name)
+        
+        # unpack data
+        obs, actions, rewards, *_ = zip(*data)
+        pov_obs, vec_obs = [item['pov'] for item in obs], [item['vector'] for item in obs]
+        pov_obs = einops.rearrange(np.array(pov_obs), 't h w c -> t c h w').astype(np.float32) / 255
+        vec_obs = np.array(vec_obs).astype(np.float32)
+        actions = np.array([ac['vector'] for ac in actions]).astype(np.float32)
+        rewards = np.array(rewards).astype(np.float32)
+        
+        if self.centroids is not None:
+            # compute action idcs
+            action_idx = np.argmin(((self.centroids[None,:,:] - actions[:,None,:]) ** 2).sum(axis=-1), axis=1).astype(np.int64)
+            return pov_obs, vec_obs, actions, action_idx, rewards
+        else:
+            return pov_obs, vec_obs, actions, rewards
+
+
+    def __len__(self):
+        return len(self.names)
+    
+    def __getitem__(self, idx):
+        print(f'Loading trajectory {self.names[idx]}..')
+        return self._load_trajectory(self.names[idx])
+
+
+
+
 
 class StateVQVAEData(Dataset):
     def __init__(self, env_name, data_dir, num_workers, num_trajs):
@@ -267,7 +360,6 @@ class StateVQVAEData(Dataset):
         pov_obs = einops.rearrange(np.array(pov_obs), 't h w c -> t c h w').astype(np.float32) / 255
         vec_obs = np.array(vec_obs).astype(np.float32)
         actions = np.array([ac['vector'] for ac in actions]).astype(np.float32)
-        # TODO discretize actions?
 
         return pov_obs[:self.max_len], vec_obs[:self.max_len], actions[:self.max_len]
 
@@ -279,6 +371,7 @@ class StateVQVAEData(Dataset):
         return self._load_trajectory(self.names[idx])
 
 
+
 class BufferedBatchDataset(IterableDataset):
     '''
     For docs on BufferedBatchIter, see https://github.com/minerllabs/minerl/blob/dev/minerl/data/buffered_batch_iter.py
diff --git a/research_code/dynamics_models.py b/research_code/dynamics_models.py
index f5c35c4..092456e 100755
--- a/research_code/dynamics_models.py
+++ b/research_code/dynamics_models.py
@@ -7,9 +7,8 @@ import torchdiffeq as teq
 import einops
 
 from vae_model import VAE
-import util_models
 from vqvae import VQVAE
-from reward_model import RewardMLP
+#from reward_model import RewardMLP
 
 from time import time
 
@@ -18,12 +17,6 @@ visual_model_by_str = {
     'vqvae':VQVAE
 }
 
-EPS = 1e-10
-
-plt.switch_backend('agg')
-
-from x_transformers import XTransformer
-
 class MDNRNNReward(nn.Module):
     def __init__(self, mdn_path, reward_path):
         super().__init__()
@@ -40,15 +33,13 @@ class MDN_RNN(pl.LightningModule):
         self, 
         gru_kwargs, 
         optim_kwargs, 
-        scheduler_kwargs, 
-        seq_len, 
+        #scheduler_kwargs, 
         num_components=5, 
         visual_model_path='', 
         visual_model_cls='vae', 
-        temp=1, 
-        conditioning_len=0,
         curriculum_threshold=3.0, 
         curriculum_start=0, 
+        use_one_hot=False
     ):
         super().__init__()
         
@@ -61,29 +52,33 @@ class MDN_RNN(pl.LightningModule):
         
         # load VAE
         self.visual_model = visual_model_by_str[visual_model_cls].load_from_checkpoint(visual_model_path)
-        self.visual_model.eval() # TODO: maybe make this optional for finetuning
+        self.visual_model.eval() # TODO: maybe make this optional for finetuning --> need to save the new model too!
         
         if self.hparams.visual_model_cls == 'vqvae':
-            self.latent_dim = self.visual_model.hparams.args.embedding_dim
-            self.num_embeddings = self.visual_model.hparams.args.num_embeddings
-            print(f'\nlatent_dim = {self.latent_dim}')
-            print(f'\nnum_embeddings = {self.num_embeddings}')
+            if use_one_hot:
+                print('\nUsing one-hot representation')
+                self.latent_dim = self.visual_model.quantizer.num_variables * self.visual_model.quantizer.codebook_size
+            else:
+                print('\nUsing learned embedding representation')
+                self.latent_dim = self.visual_model.quantizer.num_variables * self.visual_model.quantizer.embedding_dim
 
         elif self.hparams.visual_model_cls == 'vae':
             self.latent_dim = self.visual_model.hparams.encoder_kwargs['latent_dim']
-            print(f'\nlatent_dim = {self.latent_dim}')
+        print(f'\nlatent_dim = {self.latent_dim}')
             
         
-        self.pre_gru_size = self.latent_dim
-        print(f'\npre_gru_size (H*W) = {self.pre_gru_size}')
-        
         # set up model
-        self.gru_input_dim = self.pre_gru_size + 64 + 64 # 64 for action dim, 64 again for vecobs
+        self.gru_input_dim = self.latent_dim + 64 + 64 # 64 for action dim, 64 again for vecobs
         self.gru = nn.GRU(**gru_kwargs, input_size=self.gru_input_dim, batch_first=True)
 
-        self.mdn_network = nn.Sequential(
-            nn.Linear(gru_kwargs['hidden_size'], num_components + num_components * 2 * self.latent_dim)
-        )
+        if self.hparams.visual_model_cls == 'vqvae':
+            self.mdn_network = nn.Sequential(
+                nn.Linear(gru_kwargs['hidden_size'], num_components + num_components * self.latent_dim + 64)
+            )
+        else:
+            self.mdn_network = nn.Sequential(
+                nn.Linear(gru_kwargs['hidden_size'], num_components + num_components * 2 * self.latent_dim + 64)
+            )
         
         self.ce_loss = nn.CrossEntropyLoss()
 
@@ -93,113 +88,121 @@ class MDN_RNN(pl.LightningModule):
 
         # make predictions
         if self.hparams.visual_model_cls == 'vqvae':
-            pov_logits, pov_sample, target = self(pov, vec, actions)
+            
+            logits, mixing_logits, vec_pred, target_probs, target_vec = self(pov, vec, actions)
+            target_probs = einops.rearrange(target_probs, 'b t num_vars cb_size -> (b t) (num_vars cb_size)')
+            logits = einops.rearrange(logits[:,:-1], 'b t K d -> (b t) K d')
+            mixing_logits = einops.rearrange(mixing_logits[:,:-1], 'b t K -> (b t) K')
+            vec_pred = vec_pred[:,:-1]
+            
+            sampled_mix = torch.nn.functional.gumbel_softmax(mixing_logits, tau=1, hard=True, dim=-1)
+            sampled_logits = torch.einsum('a b c, a b -> a c', logits, sampled_mix)
+            sampled_logits = torch.nn.functional.log_softmax(sampled_logits, dim=1)
+            
+            pov_loss = -(target_probs * sampled_logits).sum() / self.visual_model.quantizer.num_variables / target_probs.shape[0]
 
-            pred_frames = self.visual_model.decode_with_grad(einops.rearrange(pov_sample, 'b t c (h w) -> (b t) c h w', h=16, w=16))
-            target_frames = einops.rearrange(pov[:,1:], 'b t c h w -> (b t) c h w')
-            loss = (pred_frames - target_frames).pow(2).mean() / (2 * 0.06327039811675479)
+            vec_loss = (target_vec - vec_pred).pow(2).sum(-1).mean()
+            
+        elif self.hparams.visual_model_cls == 'vae':
+            means, log_stds, mixing_logits, vec_pred, target_mean, target_logstd, target_vec = self(pov, vec, actions)
 
+            target_mean = einops.rearrange(target_mean, 'b t d -> (b t) d')
+            target_logstd = einops.rearrange(target_logstd, 'b t d -> (b t) d')
+            means = einops.rearrange(means[:,:-1], 'b t K d -> (b t) K d')
+            log_stds = einops.rearrange(log_stds[:,:-1], 'b t K d -> (b t) K d')
+            mixing_logits = einops.rearrange(mixing_logits[:,:-1], 'b t K -> (b t) K')
+            vec_pred = vec_pred[:,:-1]
 
-        elif self.hparams.visual_model_cls == 'vae':
-            means, log_stds, mixing_coefficients, target = self(pov, vec, actions)
-            #print(f'{means.shape = }')
-            #print(f'{log_vars.shape = }')
-            #print(f'{mixing_coefficients.shape = }')
-            #print(f'{target.shape = }')
-            var = torch.exp(2 * log_stds)
-            
-            log_det = torch.sum(2 * log_stds, dim=-1)
-            max_det = torch.max(log_det, dim=-1)[0]
-            
-            log_det = log_det - max_det[...,None] / 2
+            sample = self.sample_from_gmm(mixing_logits, means, log_stds)
             
-            #print(f'{det.shape = }')
-            diff = (means - target[:,:,None]).pow(2)
-            #print(f'{diff.shape = }')
-            exp = torch.exp(-0.5 / (var * diff).sum(-1))
-            #print(f'{exp.shape = }')
-            # use logsumexp trick for numerical stability
-            loss = torch.log(torch.sum(mixing_coefficients / np.sqrt(2 * np.pi) * torch.exp(-0.5 * log_det) * exp, dim=-1))
-            loss = loss - max_det
-            loss = -loss.mean()
-            print(loss)
-            #loss = (pov_mean - target_mean).pow(2).sum(dim=[2,3,4]).mean()
+            # compute NLL under target dist
+            nll = 0.5 * ((target_mean - sample).pow(2) / torch.exp(2* target_logstd)).sum(-1)
+            pov_loss = nll.mean()
+            vec_loss = (target_vec - vec_pred).pow(2).sum(-1).mean()
         
-        return loss
+        return pov_loss, vec_loss
+    
+    def sample_from_gmm(self, mixing_logits, means, log_stds):
+
+        #sampled_mix = torch.argmax(torch.nn.functional.gumbel_softmax(mixing_logits, tau=1, hard=True, dim=-1),dim=-1)
+        # sampled_means = means[torch.arange(len(means)), sampled_mix]
+        # sampled_log_stds = log_stds[torch.arange(len(log_stds)), sampled_mix]
+        sampled_mix = torch.nn.functional.gumbel_softmax(mixing_logits, tau=1, hard=True, dim=-1)
+        sampled_means = torch.einsum('a b c, a b -> a c', means, sampled_mix)
+        sampled_log_stds = torch.einsum('a b c, a b -> a c', log_stds, sampled_mix)
+        sample = sampled_means + torch.exp(sampled_log_stds) * torch.normal(torch.zeros_like(sampled_means), torch.ones_like(sampled_log_stds))
     
+        return sample
+
+    def sample_from_categorical_mixture(self, mixing_logits, logits):
+
+        sampled_mix = torch.nn.functional.gumbel_softmax(mixing_logits, tau=1, hard=True, dim=-1)
+        sampled_logits = torch.einsum('a b c, a b -> a c', logits, sampled_mix)
+        sampled_logits = einops.rearrange(sampled_logits, 'b (n d) -> b n d', n=32, d=32)
+        sampled_one_hot = torch.nn.functional.gumbel_softmax(sampled_logits, hard=True, dim=-1)
+
+        sample = []
+        for i in range(sampled_one_hot.shape[1]):
+            sample.append(sampled_one_hot[:,i] @ self.visual_model.quantizer.embeds[i].weight)
+        sample = torch.stack(sample, dim=1)
+        
+        return einops.rearrange(sample, 'b n d -> b (n d)')
+
+
+
     def forward(self, pov, vec, actions, last_hidden=None):
         '''
         Given a sequence of pov, vec and actions, computes priors over next latent
         state.
         Inputs:
-            pov - ([B], T, 3, 64, 64)
-            vec - ([B], T, 64)
-            actions - ([B], T, 64)
-            last_hidden - ([B], gru_kwargs['hidden_size'],), potential last hidden state of the recurrent network
+            pov - (B, T, 3, 64, 64)
+            vec - (B, T, 64)
+            actions - (B, T, 64)
+            last_hidden - (B, gru_kwargs['hidden_size'],), potential last hidden state of the recurrent network
         Output:
-            pov_logits_list or pov_mean_list - List of logits or means of the state at time t
-            pov_sample_list - List of samples at time timesteps t
-            target - ([B], T-1, latent_dim + vec_dim) sample of ground truth encoding
+            vqvae:
+                logits, mixing_logits, vec_pred, target_probs, target_vec
+            vae:
+                means, log_stds, mixing_logits, vec_pred, target_mean, target_logstd, target_vec
         '''
         # save shape params
         B, T = pov.shape[:2]
-        print(f'{pov.shape = }')
 
         # merge frames with batch for batch processing
         pov = einops.rearrange(pov, 'b t c h w -> (b t) c h w')
+        target_vec = vec[:,1:]
         
         # encode pov to latent
         if self.hparams.visual_model_cls == 'vqvae':
-            sample, ind, log_priors = self.visual_model.encode_only(pov)
-            print(f'{pov_sample.shape = }')
-            ind = einops.rearrange(ind, '(b t) h w -> b t h w', b=B, t=T)
-            sample = einops.rearrange(sample, '(b t) c h w -> b t c h w', b=B, t=T)
-            log_priors = einops.rearrange(einops.rearrange(log_priors, '(b t) c h w -> b t c h w', b=B, t=T)[:,:-1], 'b t c h w -> (b t h w) c')
-            target = {
-                'pov': ind[:,1:]
-            }
+            if self.hparams.use_one_hot:
+                z_q, probs = self.visual_model.encode_only_one_hot(pov)
+                probs = einops.rearrange(probs, '(b t) num_vars cb_size -> b t num_vars cb_size', b=B, t=T)
+            else:
+                z_q, _, probs = self.visual_model.encode_only(pov)
+                probs = einops.rearrange(torch.softmax(probs,dim=2), '(b t) num_vars cb_size -> b t num_vars cb_size', b=B, t=T)
+
+            sample = einops.rearrange(z_q, '(b t) num_vars cb_size -> b t (num_vars cb_size)', b=B, t=T)
+            target_probs = probs[:,1:]
             
         elif self.hparams.visual_model_cls == 'vae':
             sample, mean, logstd = self.visual_model.encode_only(pov) 
+            sample = einops.rearrange(sample, '(b t) d -> b t d', b=B)
             logstd = einops.rearrange(logstd, '(b t) d -> b t d', b=B)
             mean = einops.rearrange(mean, '(b t) d -> b t d', b=B)
-            sample = einops.rearrange(sample, '(b t) d -> b t d', b=B)
 
             target_mean = mean[:,1:]
             target_logstd = logstd[:,1:]
 
-            # sample from distribution to get an estimate of KL later
-            target = target_mean + torch.exp(target_logstd) * torch.normal(torch.zeros_like(target_logstd), torch.ones_like(target_mean))
-            
-        # condition on previous sequence to prime the RNN
-        if self.hparams.conditioning_len > 0:
-            raise NotImplementedError
-            '''
-            if self.hparams.predict_idcs_directly:
-                raise NotImplementedError
-            if self.conv_net is None:
-                raise NotImplementedError
-            else:
-                condition_states = pov_sample[:,:self.hparams.conditioning_len]
-                condition_states = self.conv_net(einops.rearrange(condition_states, 'b t c h w -> (b t) c h w'))
-                condition_states = einops.rearrange(condition_states, '(b t) c h w -> b t (c h w)', t=self.hparams.conditioning_len)
-                condition_actions = actions[:,:self.hparams.conditioning_len]
-                gru_input = torch.cat([condition_states, condition_actions], dim=2)
-                if last_hidden is None:
-                    hidden_seq, last_hidden = self.gru(gru_input)
-                else:
-                    hidden_seq, last_hidden = self.gru(gru_input, last_hidden)
-            '''
-
         # compute one-step predictions
-        input_states = torch.cat([sample, vec], dim=2)[:,:-1]
+        input_states = torch.cat([sample, vec], dim=2)
         if self.hparams.visual_model_cls == 'vqvae':
-            pov_logits, pov_sample, hidden_seq = self.one_step_prediction(input_states, actions[:,:-1], last_hidden, log_priors)
-            return pov_logits, pov_sample, target
+            logits, mixing_logits, vec_pred = self.one_step_prediction(input_states, actions, last_hidden)
+            return logits, mixing_logits, vec_pred, target_probs, target_vec
         else:
-            means, log_stds, mixing_coefficients = self.one_step_prediction(input_states, actions[:,:-1], last_hidden)
-            return means, log_stds, mixing_coefficients, target
-            
+            means, log_stds, mixing_logits, vec_pred = self.one_step_prediction(input_states, actions, last_hidden)
+            return means, log_stds, mixing_logits, vec_pred, target_mean, target_logstd, target_vec
+    
+    
     def one_step_prediction(self, states, actions, h0=None, log_prior=None):
         '''
         Helper function which takes a sequence of states and the action taken in each state.
@@ -217,7 +220,7 @@ class MDN_RNN(pl.LightningModule):
         '''
         # save T for later
         B, T, D = states.shape
-        
+
         # compute hidden states of gru
         if h0 is None:
             hidden_states_seq, _ = self.gru(torch.cat([states, actions], dim=2))
@@ -228,79 +231,103 @@ class MDN_RNN(pl.LightningModule):
         mdn_out = self.mdn_network(einops.rearrange(hidden_states_seq, 'b t d -> (b t) d'))
 
         if self.hparams.visual_model_cls == 'vqvae':
-            raise NotImplementedError
-            pov_logits = einops.rearrange(pov_pred, 'bt embedding_dim h w -> (bt h w) embedding_dim') # embedding_dim or num_embeddings
-            # skip connection / update priors over discrete embedding vectors
-            if log_prior is not None:
-                pov_logits += log_prior
-            
-            # sample next state
-            one_hot_ind = nn.functional.gumbel_softmax(pov_logits, dim=-1, tau=self.hparams.temp, hard=True)
-            state = self.visual_model.quantizer.embed_one_hot(one_hot_ind)
-            state = einops.rearrange(state, '(b t latent_size) embed_dim -> b t embed_dim latent_size', latent_size=self.latent_size, t=T)
-            pov_logits = einops.rearrange(pov_logits, '(b t latent_size) num_embeds -> b t num_embeds latent_size', t=T, latent_size=self.latent_size)
-            print(f'{pov_logits.shape = }')
-            return pov_logits, state, hidden_states_seq
+            mixing_logits = mdn_out[:,:self.hparams.num_components]
+            mixing_logits = einops.rearrange(mixing_logits, '(b t) K -> b t K',t=T, b=B)
+            logits = torch.chunk(mdn_out[:,self.hparams.num_components:self.hparams.num_components*(1+self.latent_dim)], chunks=self.hparams.num_components, dim=1)
+            logits = einops.rearrange(torch.stack(logits, dim=1), '(b t) K d -> b t K d',t=T, b=B)
+            vec_pred = mdn_out[:,-64:]
+            vec_pred = einops.rearrange(vec_pred, '(b t) d -> b t d', b=B, t=T)
+            return logits, mixing_logits, vec_pred
         
         elif self.hparams.visual_model_cls == 'vae':
             # mean == pov_pred
-            mixing_coefficients = torch.softmax(mdn_out[:,:self.hparams.num_components], dim=-1)
-            mixing_coefficients = einops.rearrange(mixing_coefficients, '(b t) K -> b t K',t=T, b=B)
+            mixing_logits = mdn_out[:,:self.hparams.num_components]
+            mixing_logits = einops.rearrange(mixing_logits, '(b t) K -> b t K',t=T, b=B)
             means = torch.chunk(mdn_out[:,self.hparams.num_components:self.hparams.num_components*(1+self.latent_dim)], chunks=self.hparams.num_components, dim=1)
             means = einops.rearrange(torch.stack(means, dim=1), '(b t) K d -> b t K d',t=T, b=B)
             log_stds = torch.chunk(mdn_out[:,self.hparams.num_components*(1+self.latent_dim):self.hparams.num_components*(1+2*self.latent_dim)], chunks=self.hparams.num_components, dim=1)
             log_stds = einops.rearrange(torch.stack(log_stds, dim=1), '(b t) K d -> b t K d',t=T, b=B)
+            vec_pred = mdn_out[:,-64:]
+            vec_pred = einops.rearrange(vec_pred, '(b t) d -> b t d', b=B, t=T)
 
-            # skip connection for mean
-            #mean = mean + states
-
-            return means, log_stds, mixing_coefficients
+            return means, log_stds, mixing_logits, vec_pred
         
     @torch.no_grad()
-    def predict_recursively(self, states, actions, horizon, log_priors):
-        '''
-        Auto-regressively applies dynamics model. Actions for imagination are supplied, so only states are being predicted
-        Input:
-            states - (T, D), where D is latent_dim + obf_vector_dim
-            actions - (T + H, D_a), where D_a is obf_action_dim and H is the horizon
-            horizon - int, number of time steps to extrapolate
-        Output:
-            predicted_states - (H, D)
-        '''
-        assert horizon > 0, f"horizon must be greater 0, but is {horizon}!"
-        
-        
-        print('\nSetting curriculum to max!\n')
-        seq_len = states.shape[0]
-        self._init_curriculum(seq_len)
-        self.curriculum_step = len(self.curriculum) - 1
-        h = states.shape[2]
-        if self.hparams.embed:
-            raise NotImplementedError
+    def imaginate(self, starting_pov, starting_vec, action_sequence):
+        assert starting_pov.shape == (3, 64, 64), f"{starting_pov.shape = }"
+        assert starting_vec.shape == (64,), f"{starting_vec.shape = }"
+        assert action_sequence.shape[1:] == (64,), f"{action_sequence.shape[1:] = }"
+
+        # apply frame encoding
+        if self.hparams.visual_model_cls == 'vae':
+            pov_sample, *_ = self.visual_model.encode_only(starting_pov[None])
+        else:
+            if self.hparams.use_one_hot:
+                pov_sample, *_ = self.visual_model.encode_only_one_hot(starting_pov[None])
+                pov_sample = einops.rearrange(pov_sample, 'b num_vars cb_size -> b (num_vars cb_size)')
+            else:
+                pov_sample, *_ = self.visual_model.encode_only(starting_pov[None])
+                pov_sample = einops.rearrange(pov_sample, 'b num_vars emb_dim -> b (num_vars emb_dim)')
+
+        starting_state = torch.cat([pov_sample, starting_vec[None]], dim=1)[None] # 1 1 1088
+        #starting_state = pov_sample[None]
         
-        one_step_priors = einops.rearrange(log_priors[:-1], 't D h w -> (t h w) D')
-        one_step_priors = None
-        extrapolating_prior = einops.rearrange(log_priors[-1], 'D h w -> (h w) D')
-        states = einops.rearrange(states, 't embed_dim h w-> 1 t embed_dim (h w)')
-        actions = einops.rearrange(actions, 't act_dim -> 1 t act_dim')
-
-        _, states, h_n = self.one_step_prediction(states[:, :-1], actions[:,:-horizon-1], h0=None, log_priors=one_step_priors)
-        h_n = h_n[:,-1]
-
-        state = states[:,-1]
-        # extrapolate
-        predicted_states, _ = self.extrapolate_latent(state, actions[-horizon-1:], h0=h_n, log_prior=extrapolating_prior)
-        predicted_states = torch.cat([state[:,None], predicted_states], dim=1)
-        predicted_states = einops.rearrange(predicted_states, 'b t embed_dim (h w) -> b t embed_dim h w', h=h, w=h)[0]
-        print(f'predicted_states.shape = {predicted_states.shape}')
-        return predicted_states
+        # first prediction
+        _, h0 = self.gru(torch.cat([starting_state, action_sequence[0][None, None]], dim=-1))
+
+        # predict next state
+        mdn_out = self.mdn_network(einops.rearrange(h0, 'b t d -> (b t) d'))
+        mixing_logits = mdn_out[:,:self.hparams.num_components]
+        vec_pred = mdn_out[:,-64:]
+
+        if self.hparams.visual_model_cls == 'vae':
+            means = torch.chunk(mdn_out[:,self.hparams.num_components:self.hparams.num_components*(1+self.latent_dim)], chunks=self.hparams.num_components, dim=1)
+            means = torch.stack(means, dim=1)
+            log_stds = torch.chunk(mdn_out[:,self.hparams.num_components*(1+self.latent_dim):self.hparams.num_components*(1+2*self.latent_dim)], chunks=self.hparams.num_components, dim=1)
+            log_stds = torch.stack(log_stds, dim=1)
+            sample = self.sample_from_gmm(mixing_logits, means, log_stds)
+        else:
+            logits = torch.chunk(mdn_out[:,self.hparams.num_components:self.hparams.num_components*(1+self.latent_dim)], chunks=self.hparams.num_components, dim=1)
+            logits = torch.stack(logits, dim=1)
+            sample = self.sample_from_categorical_mixture(mixing_logits, logits)
+
+        
+        sample = torch.cat([sample, vec_pred], dim=-1)
+        sample_list = [sample]
+
+        for t in range(len(action_sequence)-1):
+            action = action_sequence[t+1]
+            gru_input = torch.cat([sample[None], action[None,None]], dim=-1)
+            _, h0 = self.gru(gru_input, h0)
+
+            # predict next state
+            mdn_out = self.mdn_network(einops.rearrange(h0, 'b t d -> (b t) d'))
+            mixing_logits = mdn_out[:,:self.hparams.num_components]
+            vec_pred = mdn_out[:,-64:]
+            if self.hparams.visual_model_cls == 'vae':
+                means = torch.chunk(mdn_out[:,self.hparams.num_components:self.hparams.num_components*(1+self.latent_dim)], chunks=self.hparams.num_components, dim=1)
+                means = torch.stack(means, dim=1)
+                log_stds = torch.chunk(mdn_out[:,self.hparams.num_components*(1+self.latent_dim):self.hparams.num_components*(1+2*self.latent_dim)], chunks=self.hparams.num_components, dim=1)
+                log_stds = torch.stack(log_stds, dim=1)
+                sample = self.sample_from_gmm(mixing_logits, means, log_stds)
+            else:
+                logits = torch.chunk(mdn_out[:,self.hparams.num_components:self.hparams.num_components*(1+self.latent_dim)], chunks=self.hparams.num_components, dim=1)
+                logits = torch.stack(logits, dim=1)
+                sample = self.sample_from_categorical_mixture(mixing_logits, logits)
+            sample = torch.cat([sample, vec_pred], dim=-1)
+            sample_list.append(sample)
+        return torch.stack(sample_list, dim=1)[0]
 
     def training_step(self, batch, batch_idx):
         # perform predictions and compute loss
-        loss = self._step(batch)
-        # score and log predictions
-        self.log('Training/loss', loss, on_step=True)
+        pov_loss, vec_loss = self._step(batch)
+        loss = pov_loss + vec_loss
         
+        # score and log predictions
+        self.log('Training/loss', loss,)
+        self.log('Training/pov_loss',pov_loss)
+        self.log('Training/vec_loss',vec_loss)
+
         return loss
         
     def validation_epoch_end(self, batch_losses):
@@ -315,17 +342,10 @@ class MDN_RNN(pl.LightningModule):
     
     def configure_optimizers(self):
         # set up optimizer
-        params = list(self.gru.parameters()) + list(self.mdn_network.parameters()) # + list(self.linear.parameters())
+        params = list(self.gru.parameters()) + list(self.mdn_network.parameters())
         optimizer = torch.optim.AdamW(params, **self.hparams.optim_kwargs, weight_decay=0)
-        # set up scheduler
-        lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, self.hparams.scheduler_kwargs['lr_gamma'])
-        lr_dict = {
-            'scheduler': lr_scheduler,
-            'interval': self.hparams.scheduler_kwargs['lr_step_mode'],
-            'frequency': self.hparams.scheduler_kwargs['lr_decrease_freq'],
-        }
-        return {'optimizer':optimizer, 'lr_scheduler':lr_dict}
-    
+        return optimizer
+
     def _init_curriculum(self, seq_len=None, curriculum_start=0):
         self.curriculum_step = 0
         self.curriculum = [0]
@@ -343,550 +363,3 @@ class MDN_RNN(pl.LightningModule):
                 print(f'\nCurriculum updated! New forecast horizon is {self.curriculum[self.curriculum_step]}\n')
         
 
-class RSSM(pl.LightningModule):
-    def __init__(self, lstm_kwargs, optim_kwargs, scheduler_kwargs, seq_len, use_pretrained=True, VAE_path=None, VAE_class='Conv'):
-        '''
-        Adapted from https://arxiv.org/pdf/1811.04551.pdf
-        '''
-        
-        super().__init__()
-        
-        # save params
-        self.save_hyperparameters()
-
-        if use_pretrained:
-            # load VAE
-            if VAE_path == None:
-                raise ValueError('Need to specify VAE path ')
-            self.VAE = vae_model_by_str[VAE_class].load_from_checkpoint(VAE_path)
-            self.VAE.eval()
-            self.latent_dim = self.VAE.hparams.encoder_kwargs['latent_dim']
-        else:
-            raise NotImplementedError()
-            '''
-            # init new VAE
-            if VAE_kwargs == None:
-                raise ValueError('Need to specify VAE kwargs ')
-            self.VAE = vae_model_by_str[VAE_class](**VAE_kwargs)
-            self.latent_dim = VAE_kwargs['latent_dim']
-            '''
-        # save some vars
-        self.scheduler_kwargs = scheduler_kwargs
-        self.optim_kwargs = optim_kwargs
-        self.seq_len = seq_len
-
-        # set up model
-        self.mse_loss = nn.MSELoss(reduction='none')
-        self.merge = util_models.MergeFramesWithBatch()
-        self.split = util_models.SplitFramesFromBatch(self.seq_len)
-        self.split_cut = util_models.SplitFramesFromBatch(self.seq_len-1)
-        lstm_input_dim = self.latent_dim + 128 # s_t-1, a_t-1,  where s_t = [z_t, v_t]
-        self.lstm = nn.LSTM(**lstm_kwargs, input_size=lstm_input_dim, batch_first=True)
-        self.mdn_network = nn.Sequential(nn.Linear(lstm_kwargs['hidden_size'], 200), nn.ReLU(), nn.Linear(200, (2 * self.latent_dim + 64)))
-        self.elu = nn.ELU()
-        self.relu = nn.ReLU()
-        self.reward_network = nn.Sequential(nn.Linear(2 * (self.latent_dim + 64) + self.latent_dim, 1024), nn.ReLU(), nn.Linear(1024, 1), nn.Sigmoid())
-    
-    def forward_latent(self, states, actions, h0=None, c0=None, batched=False):
-        '''
-        Helper function which takes (a sample of the current belief over the) current state or a sequence thereof
-        as well as the action taken in that state or states, as well as the current lstm state and computes a belief
-        over the next state as well as a prediction of the reward
-        Input:
-            states - ([B], T, 64 + latent_dim)
-            actions - ([B], T, 64)
-            h0 - ([B], lstm_kwargs['hidden_size'],)
-            c0 - ([B], lstm_kwargs['hidden_size'],)
-            batched - Bool, whether pov, vec, actions have a batch dimension before the time dimension
-        Output:
-            (s_mean, s_std) - belief over state, shape ([B], T, latent_dim + action_dim)
-            s_t -  sample from the above factorized normal distribution
-            r_t - predicted reward
-            (h_n, c_n) - last hidden and cell state of the lstm
-        '''
-        # concat states and action
-        if batched:
-            lstm_input = torch.cat([states, actions], dim=2)
-        else:
-            lstm_input = torch.cat([states, actions], dim=1)[None,...]
-        
-        # compute hidden states of lstm
-        if h0 is None or c0 is None:
-            h_t, (h_n, c_n) = self.lstm(lstm_input)
-        else:
-            h_t, (h_n, c_n) = self.lstm(lstm_input, (h0,c0))
-        
-        # merge h_t
-        h_t = self.merge(h_t) 
-
-        # compute next deterministic state
-        s_dist = self.mdn_network(h_t) 
-        z_mean, z_logstd = torch.chunk(s_dist[...,:2*self.latent_dim], chunks=2, dim=-1)
-        v_mean = s_dist[...,-64:] 
-        s_mean = torch.cat([z_mean, v_mean], dim=-1)
-
-        # skip connection for the mean to bias it towards no change
-        if batched and len(states.shape) == 3:
-            s_mean = s_mean + self.merge(states)
-        elif not batched and len(states.shape) == 2:
-            s_mean = s_mean + states
-        else:
-            raise ValueError(f'Unexpected error: batched = {batched} but len(states.shape) = {len(states.shape)} ({states.shape}) ')
-        
-        #print(f'mean z_logstd = {self.split(s_logstd)[:,:-1,:self.latent_dim].mean()}')
-        z_std = torch.exp(z_logstd) # make sure std is non-negative #TODO: could add minimum std here
-
-        # sample from the multi-dim gaussian parameterized by h_t
-        s_t = s_mean
-        s_t[...,:-64] = s_t[...,:-64] + z_std * torch.normal(torch.zeros_like(z_std), torch.ones_like(z_std))
-        
-        # predict reward given h_t and s_t
-        rew_input = torch.cat([s_mean, z_std, s_t], dim=1)
-        r_t = self.reward_network(rew_input)
-
-        return (s_mean, z_std), s_t, r_t, (h_n, c_n)
-
-    def forward(self, pov, vec, actions, h0=None, c0=None, batched=False):
-        '''
-        Given the last state, latest obs and taken action, this function computes 
-        the belief over the next state, as well as predicts the reward.
-        Inputs:
-            pov - ([B], T, 3, 64, 64)
-            vec - ([B], T, 64)
-            actions - ([B], T, 64)
-            h0 - ([B], lstm_kwargs['hidden_size'],)
-            c0 - ([B], lstm_kwargs['hidden_size'],)
-            batched - Bool, whether pov, vec, actions have a batch dimension before the time dimension
-        Output:
-            (s_mean, s_std) - belief over state, shape ([B], T, latent_dim + action_dim)
-            s_t -  sample from the above factorized normal distribution
-            r_t - predicted reward
-            (h_n, c_n) - last hidden and cell state of the lstm
-            pov_mean - ([B], T, latent_dim) ground truth state mean
-            pov_std - ([B], T, latent_dim) ground truth state std
-        '''
-        if batched:
-            # merge frames with batch
-            pov = self.merge(pov)
-
-        # encode pov to latent
-        pov_mean, pov_std, pov_sample = self.VAE.encode_only(pov) 
-        
-        if batched:
-            # split frames from batch again
-            pov_mean, pov_std, pov_sample = self.split(pov_mean), self.split(pov_std), self.split(pov_sample)
-        
-        # construct state sample
-        states = torch.cat([pov_sample, vec], dim=2 if batched else 1)
-
-        (s_mean, z_std), s_t, r_t, (h_n, c_n) = self.forward_latent(states, actions, h0, c0, batched)        
-        
-        return (s_mean, z_std), s_t, r_t, (h_n, c_n), pov_mean, pov_std
-        
-
-    def _get_log_p(self, x, mean, std):
-        '''
-        Computes log prob of a x under a diagonal multivariate gaussian
-        Shapes:
-        x - (B*T, D)
-        mu - (B*T, D)
-        std - (B*T, D)
-        '''
-        D = x.shape[1]
-        return -0.5 * D * np.log(2*np.pi) - torch.sum(torch.log(std) + (x - mean).abs().pow(2) / (2 * std.abs().pow(2)), dim=1)
-
-    def _step(self, batch):
-        '''
-        Helper function which encodes the pov obs, cats them with vec obs and action to pass through self.forward
-        returns prediction and target
-        '''
-        # get data
-        pov, vec, actions, rew = batch
-
-        # merge frames with batch for batch processing
-        merged_vec = self.merge(vec[:,1:,:])
-        merged_rew = self.merge(rew[:,1:])
-
-        (s_mean, z_std), s_t, r_t, (h_n, c_n), pov_mean, pov_std = self(pov, vec, actions, batched=True)
-
-        # extract distributions from the tensors
-        predicted_z_mean = s_mean[:,:self.latent_dim]
-        predicted_z_std = z_std
-        #print(f'predicted_z_mean.shape = {predicted_z_mean.shape}')
-
-        predicted_v_mean = s_mean[:,self.latent_dim:]
-        #print(f'predicted_v_mean.shape = {predicted_v_mean.shape}')
-
-        # compute log_prob of v_t under its dist
-        # cut off last prediction since it can't be scored
-        # also cut off first target since it was not predicted
-        predicted_v_mean = self.merge(self.split(predicted_v_mean)[:,:-1,:])
-        v_loss = self.mse_loss(merged_vec, predicted_v_mean)
-
-        # compute mse of reward (is same as logp under scalar gaussian with unit variance --> see their paper)
-        mse_r = self.mse_loss(self.merge(self.split(r_t)[:,:-1,:]).squeeze(), merged_rew)
-        
-        # compute KL divergence between h_t = (m1, s1) and (pov_mean, pov_std)
-        pov_mean, pov_std = self.merge(pov_mean[:,1:,:]), self.merge(pov_std[:,1:,:])
-        predicted_z_mean, predicted_z_std = self.merge(self.split(predicted_z_mean)[:,:-1,:]), self.merge(self.split(predicted_z_std)[:,:-1,:])
-
-        # compute KL(enc(o) || pred(z)) in paper, but that seems to lead to bad behavior for us.
-        # so for now we comput KL(pred(z) || enc(o))
-        # specifically, the predicted std is ~1 oom too large in the KL(enc|pred) case, resulting in
-        # very wild extrapolations
-        #kld = self._compute_kl((predicted_z_mean, predicted_z_std), (pov_mean, pov_std))
-        # Since we are currently training the modules seperately, the pov_mean is not trainable
-        # so that the gradient of the KL is the same as the gradient of the following negative log-likelihood:
-        # TODO use pov_sample instead of pov_mean
-        z_loss = 0.5 * ((predicted_z_mean - pov_mean) / predicted_z_std) ** 2 + torch.log(predicted_z_std)
-        z_loss = z_loss.sum(dim=1) 
-        #print(f'mean true z std = {pov_std.mean()}')
-        #print(f'mean predicted z std = {predicted_z_std.mean()}')
-        #print(f'mse std = {self.split_cut((pov_std-predicted_z_std)**2).sum(dim=1).mean()}')
-        
-        # sum up all losses, split them into frames, sum over frames and average over batch
-        v_loss = self.split_cut(v_loss).sum(dim=2).mean() #sum over 2 in deterministic case, since we didn't reduce over the feature dim
-        z_loss = self.split_cut(z_loss).mean()
-        #print(f'kld = {z_loss}')
-        r_loss = self.split_cut(mse_r).mean()
-        #print(f'z_loss = {z_loss}')
-        #print(f'v_loss = {v_loss}')
-        #print(f'r_loss = {r_loss}')
-        
-        #print(f'pov_std = {pov_std}')
-        #print(f'predicted_z_std = {predicted_z_std}')
-        #print(f'predicted_v_std = {predicted_v_std}')
-        
-        return v_loss, z_loss, r_loss
-    
-    def _compute_kl(self, p, q):
-        '''
-        Computes KL divergence KL(p || q) between two gaussians p and q with diagonal covariance matrix
-        Args:
-            p - (mean1, std1), where mean1 and std1 are of shape (B*T, D) with batch dimension B and num frames T
-            q - (mean2, std2)
-        Returns:
-            kld - KL divergence, shape (B*T,)
-        '''
-        mean1, std1 = p
-        mean2, std2 = q
-        #print(f'Mean 1 = {mean1.mean()}')
-        #print(f'Mean 2 = {mean2.mean()}')
-        #print(f'Std 1 = {std1.mean()}')
-        #print(f'Std 2 = {std2.mean()}')
-        kld = torch.log(std2 / std1) + 0.5 * (std1 ** 2 + (mean2 - mean1) ** 2) / (std2 ** 2) - 0.5#, constant summands don't matter for gradients.
-        kld = kld.sum(dim=1)
-        #print(f'kld ={kld.mean()}')
-        return kld
-        
-    def training_step(self, batch, batch_idx):
-        # perform predictions and compute loss
-        v_loss, z_loss, r_loss = self._step(batch)
-
-        # average losses
-        loss = (v_loss + z_loss + r_loss) / 3
-
-        # score and log predictions
-        self.log('Training/loss', loss, on_step=True)
-        self.log('Training/v_loss', v_loss, on_step=True)
-        self.log('Training/r_loss', r_loss, on_step=True)
-        self.log('Training/z_loss', z_loss, on_step=True)
-        return loss
-        
-    def validation_step(self, batch, batch_idx):
-        # perform predictions and compute loss
-        v_loss, z_loss, r_loss = self._step(batch)
-        
-        # average losses
-        loss = (v_loss + z_loss + r_loss) / 3
-        
-        # score and log predictions
-        self.log('Validation/loss', loss, on_epoch=True)
-        self.log('Validation/v_loss', v_loss, on_epoch=True)
-        self.log('Validation/r_loss', r_loss, on_epoch=True)
-        self.log('Validation/z_loss', z_loss, on_epoch=True)
-        return loss
-    
-    def configure_optimizers(self):
-        # set up optimizer
-        optimizer = torch.optim.Adam(self.parameters(), **self.optim_kwargs)
-        # set up scheduler
-        lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, self.scheduler_kwargs['lr_gamma'])
-        lr_dict = {
-            'scheduler': lr_scheduler,
-            'interval': self.scheduler_kwargs['lr_step_mode'],
-            'frequency': self.scheduler_kwargs['lr_decrease_freq'],
-        }
-        return {'optimizer':optimizer, 'lr_scheduler':lr_dict}
-
-    @torch.no_grad()
-    def predict_recursively(self, states, actions, horizon):
-        '''
-        Auto-regressively applies dynamics model. Actions for imagination are supplied, so only states are being predicted
-        Input:
-            states - (T, D), where D is latent_dim + obf_vector_dim
-            actions - (T + H, D_a), where D_a is obf_action_dim and H is the horizon
-            horizon - int, number of time steps to extrapolate
-        Output:
-            predicted_states - (H, D)
-        '''
-        assert horizon > 0, f"horizon must be greater 0, but is {horizon}!"
-
-        (s_mean, z_std), s_t, _, (h_n, c_n) = self.forward_latent(states, actions[:-horizon], h0=None, c0=None, batched=False)
-
-        state_list = []
-        for t in range(horizon):
-            # get last state and action
-            s_t = s_t[-1][None,:]
-            action = actions[-horizon+t][None,:]
-            
-            # save state
-            state_list.append(s_t)        
-
-            # sample next state
-            (s_mean, z_std), s_t, _, (h_n, c_n) = self.forward_latent(s_t, action, h0=h_n, c0=h_n, batched=False)
-
-        # concat states
-        predicted_states = torch.cat(state_list, dim=0)
-
-        return predicted_states
-
-
-
-
-class NODEDynamicsModel(pl.LightningModule):
-    def __init__(self, base_model_class, base_model_kwargs, VAE_path, optim_kwargs, scheduler_kwargs, seq_len):
-        super().__init__()
-        
-        # save params
-        self.save_hyperparameters()
-    
-        # load VAE
-        self.VAE = visual_models.ConvVAE.load_from_checkpoint(VAE_path)
-        self.VAE.eval()
-
-        # save some vars
-        self.scheduler_kwargs = scheduler_kwargs
-        self.optim_kwargs = optim_kwargs
-        self.seq_len = seq_len
-        self.base_model = base_model_class(**base_model_kwargs)
-        self.criterion = nn.MSELoss()
-        self.timesteps = None
-        self.merge = util_models.MergeFramesWithBatch()
-        self.split = util_models.SplitFramesFromBatch(self.seq_len)
-        
-    
-    def forward(self, model_input):
-        if self.timesteps is None:
-            self.timesteps = torch.linspace(0,self.seq_len,self.seq_len, device=self.device)
-        # pass through ode solver
-        pred_y = teq.odeint_adjoint(self.base_model, model_input, self.timesteps, adjoint_options={"norm": "seminorm"})
-        return pred_y
-
-    def _step(self, batch):
-        '''
-        Helper function
-        '''
-        # get data
-        pov, vec, actions = batch
-        pov = self.merge(pov) # merge frames with batch for batch processing
-        pov = self.VAE.encode_only(pov)
-        pov = self.split(pov) # split frames from batch again
-        obs = torch.cat([pov, vec], dim=2)
-        input_obs, target_obs = obs[:,0,:], obs[:,1:,:] # split into input and target
-        model_input = torch.cat([input_obs, actions[:,0,:]], dim=1)
-        # create predictions
-        pred_obs = self(model_input)[:,:,:obs.shape[2]] # throw away the predicted trajectories of actions
-        pred_obs = pred_obs[1:,:,:].transpose(0,1) # flip to batch first, and throw away initial value, since it didn't change
-        return pred_obs, target_obs
-    
-    def training_step(self, batch, batch_idx):
-        pred_obs, target_obs = self._step(batch)
-        # score and log predictions
-        loss = self.criterion(pred_obs, target_obs)
-        self.log('Training/loss', loss, on_step=True)
-        return loss
-        
-    def validation_step(self, batch, batch_idx):
-        pred_obs, target_obs = self._step(batch)
-        # score and log predictions
-        loss = self.criterion(pred_obs, target_obs)
-        self.log('Validation/loss', loss, on_epoch=True, on_step=False)
-        return loss
-    
-    def configure_optimizers(self):
-        # set up optimizer
-        optimizer = torch.optim.Adam(self.parameters(), **self.optim_kwargs)
-        # set up scheduler
-        lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, self.scheduler_kwargs['lr_gamma'])
-        lr_dict = {
-            'scheduler': lr_scheduler,
-            'interval': self.scheduler_kwargs['lr_step_mode'],
-            'frequency': self.scheduler_kwargs['lr_decrease_freq'],
-        }
-        return {'optimizer':optimizer, 'lr_scheduler':lr_dict}
-
-class DynamicsBaseModel(nn.Module):
-    '''
-    Base model for NODEDynamicsModel
-    '''
-    def __init__(self, input_dim, hidden_dims):
-        super().__init__()
-        hidden_dims = [input_dim] + hidden_dims
-        layers = []
-        for i in range(len(hidden_dims)-1):
-            layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))
-            layers.append(nn.ReLU())
-        layers.append(nn.Linear(hidden_dims[-1], input_dim))
-
-        self.net = nn.Sequential(*layers)
-        
-    def forward(self, t, model_input):
-        '''
-        t - time, needed for odeint, but not used in model
-        input should be of shape (B, latent_dim + vec_obs_dim + action_dim), e.g. (B, 256)
-        '''
-        return self.net(model_input)
-
-
-class DynamicsModel(pl.LightningModule):
-
-    def __init__(self, input_size, num_layers, num_hidden, optim_kwargs, scheduler_kwargs):
-        self.save_hyperparameters()
-
-        self.optim_kwargs = optim_kwargs
-        self.scheduler_kwargs = scheduler_kwargs
-        
-
-        self.lstm = nn.LSTM(input_size=input_size, hidden_size=num_hidden, num_layers=num_layers, batch_first=True)
-        self.linear = nn.Linear(self.input_size, self.input_size-64) # want to predict latent + vec_obs, not action
-        self.criterion = nn.MSELoss()
-
-    def forward(self, input):
-        '''
-        input should be of shape (B, T, D), where D = L + 64 + 64 and L is the latent dimension of the encoding.
-        '''
-        print('LSTM input shape', input.shape)
-        lstm_out = self.lstm(input)[0] # return last hidden state at every step
-        pred = self.linear(lstm_out)
-        return pred
-
-    
-    def configure_optimizers(self):
-        # set up optimizer
-        optimizer = torch.optim.Adam(self.parameters(), **self.optim_kwargs)
-        # set up 
-        lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, self.scheduler_kwargs['lr_gamma'])
-        lr_dict = {
-            'scheduler': lr_scheduler,
-            'interval': self.scheduler_kwargs['lr_step_mode'],
-            'frequency': self.scheduler_kwargs['lr_decrease_freq'],
-        }
-        return {'optimizer':optimizer, 'lr_scheduler':lr_dict}
-
-    def training_step(self, batch, batch_idx):
-        pred = self(batch)
-        print('pred shape', pred.shape)
-        # pred should be of same shape as input, i.e. (B, L + 128)
-        # pred is scored against original sequence
-        loss = self.criterion(pred[:,:-1], batch[:,1:,:-64])
-        self.log('Training/loss', loss.mean().item(), on_step=True)
-    
-    def validation_step(self, batch, batch_idx):
-        pred = self(batch)
-        loss = self.criterion(pred[:,:-1], batch[:,1:,:-64])
-        self.log('Validation/loss', loss.mean().item(), on_step=True)
-
-
-
-
-
-
-
-class BCLinear(pl.LightningModule):
-
-    def __init__(self, input_dim, hidden_dims, output_dim, learning_rate, scheduler_kwargs, centroids_path, VAE_path):
-        super().__init__()
-        self.save_hyperparameters()
-        
-        self.VAE = visual_models.ConvVAE.load_from_checkpoint(VAE_path)
-        self.VAE.eval()
-        self.centroids = torch.from_numpy(np.load(centroids_path))
-        self.learning_rate = learning_rate
-        self.scheduler_kwargs = scheduler_kwargs
-        self.loss_fct = nn.CrossEntropyLoss()
-
-        hidden_dims = [input_dim] + hidden_dims
-        layers = []
-        for i in range(len(hidden_dims)-1):
-            layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))
-            layers.append(nn.ReLU())
-        layers.append(nn.Linear(hidden_dims[-1], output_dim))
-
-        self.net = nn.Sequential(*layers)
-        
-    def forward(self, model_input):
-        '''
-        input should be of shape (B, latent_dim + vec_obs_dim), e.g. (B, 192)
-        '''
-        return self.net(model_input)
-
-    def configure_optimizers(self):
-        # set up optimizer
-        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)
-        
-        # set up 
-        lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, self.scheduler_kwargs['lr_gamma'])
-        lr_dict = {
-            'scheduler': lr_scheduler,
-            'interval': self.scheduler_kwargs['lr_step_mode'],
-            'frequency': self.scheduler_kwargs['lr_decrease_freq'],
-        }
-        return {'optimizer':optimizer, 'lr_scheduler':lr_dict}
-    
-    def training_step(self, batch, batch_idx):
-        # get model input and target actions
-        pov, vec, actions = batch
-        model_input = torch.cat([self.VAE.encode_only(pov), vec], dim=1)
-        
-        # generate predictions
-        pred = self(model_input)
-        
-        # map action to centroids
-        actions = self.remap_actions(actions)
-        
-        # compute loss and log
-        loss = self.loss_fct(pred, actions) 
-        self.log('Training/loss', loss.mean().item(), on_step=True)
-        return loss
-    
-    def validation_step(self, batch, batch_idx):
-        # get model input and target actions
-        pov, vec, actions = batch
-        model_input = torch.cat([self.VAE.encode_only(pov), vec], dim=1)
-        
-        # generate predictions
-        pred = self(model_input)
-        
-        # map action to centroids
-        actions = self.remap_actions(actions)
-        
-        # compute loss and log
-        loss = self.loss_fct(pred, actions) 
-        self.log('Validation/loss', loss.mean().item())
-        return loss
-
-    @torch.no_grad()
-    def remap_actions(self, actions):
-        if self.device != self.centroids.device:
-            self.centroids = self.centroids.to(self.device)
-        # compute distances between action vectors and centroids
-        distances = torch.sum((actions - self.centroids[:, None]) ** 2, dim=2)
-        # Get the index of the closest centroid to each action.
-        # This is an array of (batch_size,)
-        actions = torch.argmin(distances, dim=0)
-        return actions
-    
-
-
-
diff --git a/research_code/episodic_offline_q_learning.py b/research_code/episodic_offline_q_learning.py
deleted file mode 100644
index 31a6c07..0000000
--- a/research_code/episodic_offline_q_learning.py
+++ /dev/null
@@ -1,382 +0,0 @@
-import argparse
-import os
-import einops
-from vqvae import VQVAE
-from action_vqvae import ActionVQVAE
-from vecobs_vqvae import VecObsVQVAE
-
-from torch.utils.data import DataLoader
-import torch
-import torch.nn as nn
-import pytorch_lightning as pl
-from pytorch_lightning.callbacks import ModelCheckpoint
-import matplotlib.pyplot as plt
-import numpy as np 
-from copy import deepcopy
-
-from datasets import TrajectoryData
-
-class ConvFeatureExtractor(nn.Module):
-    def __init__(self):
-        super().__init__()
-
-        self.conv = nn.Sequential(
-            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1, stride=2), # 64 -> 32
-            nn.GELU(),
-            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=2), # 32 -> 16
-            nn.GELU(),
-            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1, stride=2), # 16 -> 8
-            nn.GELU(),
-            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=2), # 8 -> 4
-            nn.GELU(),
-            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=2), # 4 -> 2
-            nn.GELU(),
-            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1, stride=2), # 2 -> 1
-            nn.GELU()
-        )
-    
-    def forward(self, x):
-        conv_out = self.conv(x)
-        return einops.rearrange(conv_out, 'b c h w -> b (c h w)')
-    
-    @property
-    def trainable_parameters(self):
-        return self.parameters()
-
-class VQVAEFeatureExtractor(nn.Module):
-    def __init__(self, model_path, finetune_vqvae=False):
-        super().__init__()
-        self.vqvae = VQVAE.load_from_checkpoint(model_path)
-        
-        self.finetune_vqvae = finetune_vqvae
-
-        self.conv = nn.Sequential(
-            nn.Conv2d(in_channels=self.vqvae.hparams.args.embedding_dim, out_channels=64, kernel_size=3, padding=1, stride=2), # 16 -> 8
-            nn.GELU(),
-            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=2), # 8 -> 4
-            nn.GELU(),
-            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=2), # 4 -> 2
-            nn.GELU(),
-            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1, stride=2), # 2 -> 1
-            nn.GELU()
-        )
-
-        self.trainable_params = list(self.conv.parameters())
-        if finetune_vqvae:
-            self.trainable_params += list(self.vqvae.parameters())
-        else:
-            self.vqvae.eval()
-            
-    def forward(self, x):
-        # center image
-        x = self.vqvae.recon_loss.inmap(x)
-
-        # quantize
-        if self.finetune_vqvae:
-            vqvae_latent = self.vqvae.encode_with_grad(x)[0]
-        else:
-            vqvae_latent, ind, *_ = self.vqvae.encode_only(x)
-        
-        # distill
-        conv_out = self.conv(vqvae_latent)
-        
-        return einops.rearrange(conv_out, 'b c h w -> b (c h w)')
-
-    @property
-    def trainable_parameters(self):
-        return self.trainable_params
-
-class VectorQuantizer(nn.Module):
-    def __init__(self, model_class, model_path=None, centroids=True):
-        super().__init__()
-        self.centroids = centroids
-        device = 'cuda' if torch.cuda.is_available() else 'cpu'
-        if centroids:
-            self.model = torch.from_numpy(np.load(model_path).astype(np.float32)).to(device)
-        elif model_path is not None:
-            self.model = model_class.load_from_checkpoint(model_path).to(device)
-        else:
-            self.model = None
-            
-        dummy_input = torch.zeros(1,64, device=device)
-        if self.model is None: # vecobs
-            self.output_dim = self.forward(dummy_input).shape[1]
-        else: # action
-            centroids, idcs = self.forward(dummy_input)
-            self.output_dim = centroids.shape[1]
-            self.num_actions = self.model.shape[0]
-
-    def forward(self, x):
-        if self.centroids:
-            centroids, idcs = self._compute_closest_centroids(x)
-            return centroids, idcs
-        elif self.model is None:
-            return x
-        else:
-            return self.model.encode_only(x)[:2]
-
-    def _compute_closest_centroids(self, x):
-        idcs = torch.argmin((self.model[None,...] - x[:,None]).pow(2).sum(-1),-1)
-        centroids = self.model[idcs]
-
-        return centroids, idcs
-
-    @property
-    def trainable_params(self):
-        return []
-
-class ActionQuantizer(VectorQuantizer):
-    def __init__(self, model_path=None, action_centroids=True):
-        if not action_centroids:
-            raise NotImplementedError
-        super().__init__(ActionVQVAE, model_path, action_centroids)
-        
-class VecobsQuantizer(VectorQuantizer):
-    def __init__(self, model_path=None, vecobs_centroids=False):
-        super().__init__(VecObsVQVAE, model_path, vecobs_centroids)
-    
-
-class OfflineQLearner(pl.LightningModule):
-    def __init__(
-        self, 
-        model_cls_name, 
-        model_path, 
-        discount_factor, 
-        lr, 
-        action_quantizer_path=None, 
-        action_centroids=True,
-        vecobs_quantizer_path=None,
-        max_batch_size=1000,
-        margin=0.8
-    ):
-        super().__init__()
-        self.save_hyperparameters()
-
-        self.action_quantizer = ActionQuantizer(action_quantizer_path, action_centroids)
-        self.action_dim = self.action_quantizer.output_dim
-        
-        self.vecobs_quantizer = VecobsQuantizer(vecobs_quantizer_path)
-        self.vecobs_dim = self.vecobs_quantizer.output_dim
-
-        # init discount matrix
-        device = 'cuda' if torch.cuda.is_available() else 'cpu'
-        discount_array = torch.tensor([self.hparams.discount_factor ** i for i in range(30001)], device=device)
-        self.discount_matrix = torch.zeros((30000,30000), device=device)
-        for i in range(len(self.discount_matrix)):
-            self.discount_matrix[i,i:] = discount_array[:-i-1]
-
-        # init model
-        # TODO make this more customizable
-        if model_cls_name == 'vqvae':
-            self.pov_feature_extractor = VQVAEFeatureExtractor(model_path)
-        elif model_cls_name == 'conv':
-            self.pov_feature_extractor = ConvFeatureExtractor()
-        
-        # compute q_net input dim
-        dummy_pov = torch.zeros(1,3,64,64, dtype=torch.float32)
-        pov_features = self.pov_feature_extractor(dummy_pov)
-        assert len(pov_features.shape) == 2, f'{pov_features.shape = } but expected shape (T, D)'
-
-        q_net_input_dim = pov_features.shape[-1] + self.action_dim + self.vecobs_dim
-        self.q_net = nn.Sequential(
-            nn.Linear(q_net_input_dim, 512),
-            nn.GELU(),
-            nn.Linear(512, 128),
-            nn.GELU(),
-            nn.Linear(128, self.action_quantizer.num_actions),
-        )
-
-        #self._update_target()
-
-    def _update_target(self):
-        self.target_net = nn.ModuleDict({
-            'pov_feature_extractor':deepcopy(self.pov_feature_extractor),
-            'action_quantizer':deepcopy(self.action_quantizer),
-            'vecobs_quantizer':deepcopy(self.vecobs_quantizer),
-            'q_net':deepcopy(self.q_net)
-        })
-        self.target_net.eval()
-
-    def _sub_batch_processing(self, pov_obs, vec_obs, actions):
-        # preprocess
-        pov_features = self.pov_feature_extractor(pov_obs)
-        vec_obs = self.vecobs_quantizer(vec_obs)
-        action_quants, action_idcs = self.action_quantizer(actions)
-
-        # stack inputs
-        q_net_input = torch.cat([pov_features, vec_obs, action_quants], dim=1)
-
-        # predict q values
-        predicted_q_values = self.q_net(q_net_input)
-        
-        return predicted_q_values, action_idcs
-    
-    def forward(self, pov_obs, vec_obs, actions):
-        predicted_q_values = []
-        all_action_idcs = []
-        for i in range((len(pov_obs) - 1) // self.hparams.max_batch_size + 1):
-            start = i * self.hparams.max_batch_size
-            stop = (i + 1) * self.hparams.max_batch_size
-            q_values, action_idcs = self._sub_batch_processing(
-                pov_obs[start:stop],
-                vec_obs[start:stop],
-                actions[start:stop],
-            )
-            predicted_q_values.append(q_values)
-            all_action_idcs.append(action_idcs)
-        return torch.cat(predicted_q_values, dim=0), torch.cat(all_action_idcs, dim=0)
-            
-    def training_step(self, batch, batch_idx):
-        # unpack batch
-        pov_obs, vec_obs, actions, rew = batch
-        
-        rew = torch.log2(1 + rew) / 8
-
-        targets = self.compute_q_values(rew)
-        #print(f'{targets = }')
-        print(f'{rew = }')
-        predicted_q_values, action_idcs = self.forward(pov_obs, vec_obs, actions)
-        print(f'{predicted_q_values.shape = }')
-        print(f'{action_idcs.shape = }')
-        
-        # TODO:
-        # compute 1-step and n-step loss instead of episodic?
-
-        large_margin_loss = self._large_margin_classification_loss(predicted_q_values, action_idcs)
-        return_loss = nn.MSELoss(reduction='mean')(predicted_q_values[torch.arange(0,len(predicted_q_values),1,dtype=torch.long), action_idcs], targets)
-        loss = large_margin_loss + return_loss
-        self.log('Training/loss', loss, on_step=True)
-        self.log('Training/return_loss', return_loss, on_step=True)
-        self.log('Training/large_margin_loss', large_margin_loss, on_step=True)
-
-        #self.logger.experiment.add_histogram('Training/Predicted_Q', predicted_q_values, self.global_step)
-        #self.logger.experiment.add_histogram('Training/True_Q', targets, self.global_step)
-
-        """
-        figure = plt.figure()
-        plt.plot(np.arange(len(predicted_q_values)), predicted_q_values.detach().cpu().numpy())
-        plt.xlabel('Timestep t')
-        plt.ylabel('Q')
-        self.logger.experiment.add_figure('Training/Predicted_Q',figure, self.global_step)
-
-        figure = plt.figure()
-        plt.plot(np.arange(len(targets)), targets.detach().cpu().numpy())
-        plt.xlabel('Timestep t')
-        plt.ylabel('Q')
-        self.logger.experiment.add_figure('Training/True_Q',figure, self.global_step)
-        """
-
-        return loss
-    
-    def configure_optimizers(self):
-        params = list(self.q_net.parameters())
-        params += list(self.pov_feature_extractor.trainable_parameters)
-        self.optimizer = torch.optim.AdamW(params, lr=self.hparams.lr)
-        return self.optimizer
-    
-    def compute_q_values(self, rew):
-        assert len(rew.shape) == 1, f"{rew.shape = }, but expected (T,)"
-        # compute q values from rewards and discount matrix
-        print(f'self.discount_matrix[:len(rew), :len(rew)] = \n {self.discount_matrix[:len(rew), :len(rew)]}')
-        q_values = self.discount_matrix[:len(rew), :len(rew)] @ rew
-        return q_values
-
-    def _large_margin_classification_loss(self, q_values, expert_action):
-        '''
-        Computes the large margin classification loss J_E(Q) from the DQfD paper
-        '''
-        print(f'{q_values.shape = }')
-        print(f'{expert_action.shape = }')
-        idcs = torch.arange(0,len(q_values),dtype=torch.long)
-        q_values = q_values + self.hparams.margin
-        q_values[idcs, expert_action] = q_values[idcs,expert_action] - self.hparams.margin
-        return (torch.max(q_values, dim=1)[0] - q_values[idcs,expert_action]).mean()
-
-def main(
-    env_name,
-    log_dir,
-    data_dir,
-    model_class_name,
-    model_path,
-    finetune_vqvae,
-    num_workers,
-    save_freq,
-    lr,
-    discount_factor,
-    epochs,
-    action_quantizer_version,
-    vecobs_quantizer_version,
-    margin,
-    action_num_centroids
-):
-    pl.seed_everything(1337)
-
-    #
-    log_path = os.path.join(log_dir, 'EpisodicOfflineQLearner', env_name, model_class_name)
-    
-    # load data
-    data = TrajectoryData(env_name, data_dir)
-    data_loader = DataLoader(data, batch_size=None, num_workers=num_workers)
-
-    # set up model    
-    if action_quantizer_version is None:
-        action_quantizer_path = os.path.join(data_dir, env_name+'_'+str(action_num_centroids)+'_centroids.npy')
-        centroids = True
-    else:
-        action_quantizer_path = os.path.join(log_dir, 'ActionVQVAE', env_name, 'lightning_logs', 'version_'+str(action_quantizer_version), 'checkpoints', 'last.ckpt')
-        centroids = False
-    
-    if vecobs_quantizer_version is None:
-        vecobs_quantizer_path = None
-    else:
-        vecobs_quantizer_path = os.path.join(log_dir, 'VecObsVQVAE', env_name, 'lightning_logs', 'version_'+str(vecobs_quantizer_version), 'checkpoints', 'last.ckpt')
-
-    model = OfflineQLearner(
-        model_cls_name=model_class_name, 
-        model_path=model_path, 
-        discount_factor=discount_factor, 
-        lr=lr,
-        action_quantizer_path=action_quantizer_path,
-        action_centroids=centroids,
-        vecobs_quantizer_path=vecobs_quantizer_path,
-        margin=margin
-    )
-
-    # define callbacks
-    callbacks = [ModelCheckpoint(monitor='Training/loss', mode='min', every_n_train_steps=save_freq, save_last=True)]
-    
-    # set up trainer
-    trainer = pl.Trainer(
-        progress_bar_refresh_rate=1,
-        log_every_n_steps=1,
-        callbacks=callbacks,
-        gpus=torch.cuda.device_count(),
-        accelerator='dp',
-        default_root_dir=log_path,
-        max_epochs=epochs
-    )
-    
-    # train model
-    trainer.fit(model, data_loader)
-
-if __name__ == '__main__':
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--env_name', default='MineRLTreechopVectorObf-v0')
-    parser.add_argument('--data_dir', default='/home/lieberummaas/datadisk/minerl/data')
-    parser.add_argument('--log_dir', default='/home/lieberummaas/datadisk/minerl/run_logs')
-    parser.add_argument('--model_class_name', default='conv', choices=['conv', 'vqvae'])
-    parser.add_argument('--model_path')
-    parser.add_argument('--finetune_vqvae', action='store_true')
-    parser.add_argument('--num_workers', default=0, type=int)
-    parser.add_argument('--save_freq', default=10, type=int)
-    parser.add_argument('--lr', default=3e-4, type=float)
-    parser.add_argument('--discount_factor', default=0.99, type=float)
-    parser.add_argument('--margin', default=0.8, type=float)
-    parser.add_argument('--epochs', default=2, type=int)
-    parser.add_argument('--action_quantizer_version', default=None, type=int, help='Version of action quantizer')
-    parser.add_argument('--action_num_centroids', default=150, type=int, help='Number of clusters for actions, if using kmeans instead of vqvae')
-    parser.add_argument('--vecobs_quantizer_version', default=None, type=int, help='Version of vecobs quantizer')
-    
-    args = parser.parse_args()
-    
-    main(**vars(args))
\ No newline at end of file
diff --git a/research_code/eval_BC.py b/research_code/eval_BC.py
deleted file mode 100755
index 49a6572..0000000
--- a/research_code/eval_BC.py
+++ /dev/null
@@ -1,74 +0,0 @@
-import env_wrappers
-import models
-import datasets
-import train_BC
-
-import torch
-from torch.utils.data import DataLoader, random_split
-import pytorch_lightning as pl
-from pytorch_lightning.callbacks import ModelCheckpoint
-import numpy as np
-import argparse
-import os
-from pyvirtualdisplay import Display
-import gym
-
-MAX_TEST_EPISODE_LEN = 15000
-
-def load_model_and_eval(env_name, model_path, test_episodes):
-    model = models.BCLinear.load_from_checkpoint(model_path)
-    evaluate_BC(env_name, model, test_episodes)
-
-def evaluate_BC(env_name, model, test_episodes):
-    display = Display(visible=0, size=(400, 300))
-    display.start()
-
-    env = gym.make(env_name)
-
-    num_actions = model.centroids.shape[0]
-    action_list = np.arange(num_actions)
-
-    n_frames = 5
-
-    for episode in range(test_episodes):
-        obs = env.reset()
-        done = False
-        total_reward = 0
-        steps = 0
-
-        while not done:
-            # only sample new action every N frames
-            if steps % n_frames == 0:    
-                obs_pov = model.VAE.encode_only(torch.from_numpy(obs['pov'].transpose(2, 0, 1)[None].astype(np.float32) / 255).to(model.device))
-                obs_vec = torch.from_numpy(obs['vector'][None].astype(np.float32)).to(model.device)
-                obs = torch.cat([obs_pov, obs_vec], dim=1)
-                
-                # Turn logits into probabilities
-                probabilities = torch.softmax(model(obs), dim=1)[0]
-                # Into numpy
-                probabilities = probabilities.detach().cpu().numpy()
-                # Sample action according to the probabilities
-                discrete_action = np.random.choice(action_list, p=probabilities)
-
-                # Map the discrete action to the corresponding action centroid (vector)
-                action = model.centroids[discrete_action].numpy()
-                minerl_action = {"vector": action}
-            obs, reward, done, info = env.step(minerl_action)
-            total_reward += reward
-            steps += 1
-            if steps >= MAX_TEST_EPISODE_LEN:
-                break
-
-        #env.release()
-        #env.play()
-        print(f'Episode #{episode + 1} reward: {total_reward}\t\t episode length: {steps}\n')
-
-if __name__ == '__main__':
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--model_path', help='Path to encoding model')
-    parser.add_argument('--env_name')
-    parser.add_argument('--test_episodes', default=10, type=int, help='Number of episodes to evaluate the model on')
-
-    args = vars(parser.parse_args())
-
-    load_model_and_eval(**args)
\ No newline at end of file
diff --git a/research_code/get_unique_actions_and_quantize.py b/research_code/get_unique_actions_and_quantize.py
deleted file mode 100644
index d312377..0000000
--- a/research_code/get_unique_actions_and_quantize.py
+++ /dev/null
@@ -1,167 +0,0 @@
-from action_vqvae import ActionVQVAE
-import argparse
-import os
-import minerl
-import torch
-import numpy as np
-import pandas as pd
-from tqdm import tqdm
-
-equip_map = {
-    'air':0,
-    'iron_axe':1,
-    'iron_pickaxe':2,
-    'none':3,
-    'stone_axe':4,
-    'stone_pickaxe':5,
-    'wooden_axe':6,
-    'wooden_pickaxe':7
-}
-
-craft_map = {
-    'crafting_table':0,
-    'none':1,
-    'planks':2,
-    'stick':3,
-    'torch':4
-}
-
-nearbyCraft_map = {
-    'furnace':0,
-    'iron_axe':1,
-    'iron_pickaxe':2,
-    'none':3,
-    'stone_axe':4,
-    'stone_pickaxe':5,
-    'wooden_axe':6,
-    'wooden_pickaxe':7
-} 
-
-nearbySmelt_map = {
-    'coal':0,
-    'iron_ingot':1,
-    'none':2
-}
-
-place_map = {
-    'cobblestone':0,
-    'crafting_table':1,
-    'dirt':2,
-    'furnace':3,
-    'none':4,
-    'stone':5,
-    'torch':6
-}
-
-def unpack_action(action):
-    out = []
-    for key in action:
-        if key == 'camera':
-            out.extend(list(action[key]))
-        elif key == 'craft':
-            out.append(float(craft_map[action[key]]))
-        elif key == 'equip':
-            out.append(float(equip_map[action[key]]))
-        elif key == 'place':
-            out.append(float(place_map[action[key]]))
-        elif key == 'nearbyCraft':
-            out.append(float(nearbyCraft_map[action[key]]))
-        elif key == 'nearbySmelt':
-            out.append(float(nearbySmelt_map[action[key]]))
-        else:
-            out.append(float(action[key]))
-    return out
-
-
-
-def main(
-    env_name,
-    data_dir,
-    log_dir,
-    action_quantizer,
-    num_trajs,
-    save_dir
-):
-    
-    # set device
-    device = 'cuda' if torch.cuda.is_available() else 'cpu'    
-
-    # get env names
-    obf_env_name = env_name + 'VectorObf-v0'
-    deobf_env_name = env_name + '-v0'
-    
-    # load model
-    model_path = os.path.join(log_dir, 'ActionVQVAE', obf_env_name, 'lightning_logs', 'version_'+str(action_quantizer), 'checkpoints', 'last.ckpt')
-    model: ActionVQVAE = ActionVQVAE.load_from_checkpoint(model_path).to(device)  
-    model.eval()
-    
-    # set up data pipelines
-    obf_pipeline = minerl.data.make(obf_env_name, data_dir)
-    if not os.path.exists(os.path.join(data_dir, deobf_env_name)):
-        minerl.data.download(data_dir, environment=deobf_env_name)
-    deobf_pipeline = minerl.data.make(deobf_env_name, data_dir)
-    names = obf_pipeline.get_trajectory_names()
-
-    #
-    unique_deobf = None
-    all_obf = None
-
-
-    # iterate over all trajectories
-    for i, name in tqdm(enumerate(names)):
-        if i >= num_trajs and num_trajs > 0:
-            break
-        # load trajectory
-        obf_traj = obf_pipeline.load_data(name)
-        deobf_traj = deobf_pipeline.load_data(name)
-
-        # get actions
-        _, obf_actions, *_ = zip(*obf_traj)
-        _, deobf_actions, *_ = zip(*deobf_traj)
-
-        deobf_actions = np.array(list(map(unpack_action, deobf_actions)))
-        obf_actions = np.array([ac['vector'] for ac in obf_actions])
-        # drop duplicates
-        df = pd.DataFrame(deobf_actions)
-        df = df.drop_duplicates()
-        
-        deobf_actions = deobf_actions[df.index.to_list()]
-        obf_actions = obf_actions[df.index.to_list()] # only quantize actions which have not been dropped
-        obf_actions = torch.from_numpy(obf_actions.astype(np.float32)).to(device)
-        #print(f'{deobf_actions.shape}')
-        #print(f'{obf_actions.shape = }')
-
-        #print(df.index)
-
-        # compute quantized actions
-        quantized_actions, *_ = model.encode_only(obf_actions)
-        quantized_actions = quantized_actions.to('cpu').numpy()
-        #print(f'{quantized_actions.shape = }')
-
-        # merge with collection and drop duplicates
-        if unique_deobf is None:
-            unique_deobf = pd.DataFrame(deobf_actions)
-            all_obf = quantized_actions
-        else:
-            unique_deobf = pd.concat([unique_deobf, df], ignore_index=True)
-            all_obf = np.concatenate([all_obf, quantized_actions], axis=0)
-            unique_deobf = unique_deobf.drop_duplicates()
-            all_obf = all_obf[unique_deobf.index.to_list()]
-        print(f'Total unique actions: {all_obf.shape[0]}')
-    
-    os.makedirs(os.path.join(save_dir, deobf_env_name), exist_ok=True)
-    save_path = os.path.join(save_dir, deobf_env_name, f'actions_version_{action_quantizer}.npz')
-    np.savez_compressed(save_path, unique_deobf=unique_deobf.to_numpy(), all_obf=all_obf)
-
-if __name__ == '__main__':
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--env_name', type=str, default='MineRLTreechop')
-    parser.add_argument('--data_dir', type=str, default='/home/lieberummaas/datadisk/minerl/data')
-    parser.add_argument('--save_dir', type=str, default='/home/lieberummaas/datadisk/minerl/action_analysis')
-    parser.add_argument('--log_dir', default='/home/lieberummaas/datadisk/minerl/run_logs')
-    parser.add_argument('--action_quantizer', type=int, default=None, help='Version of the ActionVQVAE to use')
-    parser.add_argument('--num_trajs', type=int, default=0)
-    
-    args = parser.parse_args()
-    
-    main(**vars(args))
\ No newline at end of file
diff --git a/research_code/get_unique_vecobs_and_quantize.py b/research_code/get_unique_vecobs_and_quantize.py
deleted file mode 100644
index 0b2a006..0000000
--- a/research_code/get_unique_vecobs_and_quantize.py
+++ /dev/null
@@ -1,125 +0,0 @@
-from vecobs_vqvae import VecObsVQVAE
-import argparse
-import os
-import minerl
-import torch
-import numpy as np
-import pandas as pd
-from tqdm import tqdm
-
-type_to_num = {
-    'air':0,
-    'iron_axe':1,
-    'iron_pickaxe':2,
-    'none':3,
-    'other':4,
-    'stone_axe':5,
-    'stone_pickaxe':6,
-    'wooden_axe':7,
-    'wooden_pickaxe':8
-}
-
-def unpack_obs(obs):
-    out = []
-    for key in obs:
-        if key == 'pov':
-            continue
-        elif key == 'equipped_items.mainhand.type':
-            out.append(float(type_to_num[obs['equipped_items.mainhand.type']]))
-        elif key.startswith('equipped_items'):
-            out.append(float(obs[key]))
-        else:
-            for item in obs[key]:
-                out.append(float(obs[key][item]))
-    return out
-
-def main(
-    env_name,
-    data_dir,
-    log_dir,
-    vecobs_quantizer,
-    num_trajs,
-    save_dir
-):
-    
-    # set device
-    device = 'cuda' if torch.cuda.is_available() else 'cpu'    
-
-    # get env names
-    obf_env_name = env_name + 'VectorObf-v0'
-    deobf_env_name = env_name + '-v0'
-    
-    # load model
-    model_path = os.path.join(log_dir, 'VecObsVQVAE', obf_env_name, 'lightning_logs', 'version_'+str(vecobs_quantizer), 'checkpoints', 'last.ckpt')
-    model: VecObsVQVAE = VecObsVQVAE.load_from_checkpoint(model_path).to(device)  
-    model.eval()
-    
-    # set up data pipelines
-    obf_pipeline = minerl.data.make(obf_env_name, data_dir)
-    if not os.path.exists(os.path.join(data_dir, deobf_env_name)):
-        minerl.data.download(data_dir, environment=deobf_env_name)
-    deobf_pipeline = minerl.data.make(deobf_env_name, data_dir)
-    names = obf_pipeline.get_trajectory_names()
-
-    # init collection of obs
-    unique_deobf = None
-    all_obf = None
-
-    # iterate over all trajectories
-    for i, name in tqdm(enumerate(names)):
-        if i >= num_trajs and num_trajs > 0:
-            break
-        # load trajectory
-        obf_traj = obf_pipeline.load_data(name)
-        deobf_traj = deobf_pipeline.load_data(name)
-
-        # get obs
-        obf_obs, *_ = zip(*obf_traj)
-        deobf_obs, *_ = zip(*deobf_traj)
-
-        deobf_obs = np.array(list(map(unpack_obs, deobf_obs)))
-        obf_obs = np.array([obs['vector'] for obs in obf_obs])
-        # drop duplicates
-        df = pd.DataFrame(deobf_obs)
-        df = df.drop_duplicates()
-        
-        deobf_obs = deobf_obs[df.index.to_list()]
-        obf_obs = obf_obs[df.index.to_list()] # only quantize obs which have not been dropped
-        obf_obs = torch.from_numpy(obf_obs.astype(np.float32)).to(device)
-        #print(f'{deobf_obs.shape}')
-        #print(f'{obf_obs.shape = }')
-
-        #print(df.index)
-
-        # compute quantized obs
-        quantized_obs, *_ = model.encode_only(obf_obs)
-        quantized_obs = quantized_obs.to('cpu').numpy()
-        #print(f'{quantized_obs.shape = }')
-
-        # merge with collection and drop duplicates
-        if unique_deobf is None:
-            unique_deobf = pd.DataFrame(deobf_obs)
-            all_obf = quantized_obs
-        else:
-            unique_deobf = pd.concat([unique_deobf, df], ignore_index=True)
-            all_obf = np.concatenate([all_obf, quantized_obs], axis=0)
-            unique_deobf = unique_deobf.drop_duplicates()
-            all_obf = all_obf[unique_deobf.index.to_list()]
-        print(f'Total unique obs: {all_obf.shape[0]}')
-    
-    os.makedirs(os.path.join(save_dir, deobf_env_name), exist_ok=True)
-    save_path = os.path.join(save_dir, deobf_env_name, f'obs_version_{vecobs_quantizer}.npz')
-    np.savez_compressed(save_path, unique_deobf=unique_deobf.to_numpy(), all_obf=all_obf)
-
-if __name__ == '__main__':
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--env_name', type=str, default='MineRLObtainIronPickaxe')
-    parser.add_argument('--data_dir', type=str, default='/home/lieberummaas/datadisk/minerl/data')
-    parser.add_argument('--save_dir', type=str, default='/home/lieberummaas/datadisk/minerl/vecobs_analysis')
-    parser.add_argument('--log_dir', default='/home/lieberummaas/datadisk/minerl/run_logs')
-    parser.add_argument('--vecobs_quantizer', type=int, default=None, help='Version of the ActionVQVAE to use')
-    parser.add_argument('--num_trajs', type=int, default=0)
-    
-    args = parser.parse_args()
-    
-    main(**vars(args))
\ No newline at end of file
diff --git a/research_code/investigate_action_distribution.py b/research_code/investigate_action_distribution.py
deleted file mode 100644
index 0ac40d7..0000000
--- a/research_code/investigate_action_distribution.py
+++ /dev/null
@@ -1,45 +0,0 @@
-import argparse
-import numpy as np
-import matplotlib.pyplot as plt
-import os
-from tqdm import tqdm
-import minerl
-
-def main(env_name, data_dir, num_trajs, eps):
-    pipeline = minerl.data.make(env_name, data_dir)
-    all_names = pipeline.get_trajectory_names()
-
-    all_actions = []
-    num_actions = 0
-
-    for i, name in tqdm(enumerate(all_names)):
-        if num_trajs > 0:
-            if i >= num_trajs:
-                break 
-        traj_data = pipeline.load_data(name)
-
-        # unpack data
-        _, actions, _, _, _ = zip(*traj_data)
-        num_actions += len(actions)
-        for ac in actions:
-            all_actions.append(ac['vector'])
-    
-    all_actions = np.array(all_actions)
-    
-    mean = np.mean(all_actions, axis=0)
-    std = np.std(all_actions, axis=0)
-
-    print(f'Mean = {mean}')
-    print(f'Std = {std}')
-
-
-if __name__ == '__main__':
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--env_name', type=str, default='MineRLTreechopVectorObf-v0')
-    parser.add_argument('--data_dir', type=str, default='/home/lieberummaas/datadisk/minerl/data')
-    parser.add_argument('--num_trajs', type=int, default=0)
-    parser.add_argument('--eps', type=float, default=1e-8)
-
-    args = parser.parse_args()
-
-    main(**vars(args))
\ No newline at end of file
diff --git a/research_code/new_train_BC.py b/research_code/new_train_BC.py
deleted file mode 100644
index 8f8ea8f..0000000
--- a/research_code/new_train_BC.py
+++ /dev/null
@@ -1,114 +0,0 @@
-import argparse 
-from BCModels import BCModel
-from vqvae import VQVAE
-from visual_models import ResnetVAE
-from datasets import BufferedBatchDataset
-import os
-import pytorch_lightning as pl
-from pytorch_lightning.callbacks import ModelCheckpoint
-from torch.utils.data import DataLoader
-import torch
-
-def train(
-    env_name,
-    data_dir,
-    log_dir,
-    feature_extractor_class_name,
-    feature_extractor_version,
-    lr,
-    action_num_centroids, 
-    action_vqvae_version, 
-    vecobs_vqvae_version,
-    hidden_dim,
-    batch_size,
-    num_epochs,
-    save_freq
-):
-    # set seed
-    pl.seed_everything(1337)
-
-    # get feature extractor class
-    feature_extractor_class = {
-        'vqvae':VQVAE,
-        'vae':ResnetVAE,
-        'conv':None
-    }[feature_extractor_class_name]
-    
-    # get paths
-    if feature_extractor_class is not None:
-        feature_extractor_path = os.path.join(log_dir, feature_extractor_class.__name__, env_name, 'lightning_logs', 'version_'+feature_extractor_version, 'checkpoints', 'last.ckpt')
-    else:
-        feature_extractor_path = None
-    if action_num_centroids is not None:
-        action_centroids_path = os.path.join(data_dir, env_name+'_'+str(action_num_centroids)+'_centroids.npy')
-    if action_vqvae_version is not None:
-        action_vqvae_path = os.path.join(log_dir, 'ActionVQVAE', env_name, 'lightning_logs', 'version_'+action_vqvae_version, 'checkpoints', 'last.ckpt')
-    else:
-        action_vqvae_path = None
-    if vecobs_vqvae_version is not None:
-        vecobs_vqvae_path = os.path.join(log_dir, 'VecObsVQVAE', env_name, 'lightning_logs', 'version_'+vecobs_vqvae_version, 'checkpoints', 'last.ckpt')
-    else:
-        vecobs_vqvae_path = None
-    
-    # set up BC model    
-    model = BCModel(
-        feature_extractor_path,
-        feature_extractor_class,
-        lr,
-        action_centroids_path,
-        action_vqvae_path,
-        vecobs_vqvae_path,
-        hidden_dim
-    )
-    
-    # set up data
-    data = BufferedBatchDataset(env_name, data_dir, batch_size, num_epochs)
-    dataloader = DataLoader(data, num_workers=1)
-    
-    # set log dir for trainer
-    default_log_dir = os.path.join(log_dir, 'BC', env_name, feature_extractor_class_name)
-    if action_vqvae_path is None and vecobs_vqvae_path is None:
-        default_log_dir = os.path.join(default_log_dir, 'action_centroids_only')
-    if action_vqvae_path is not None and vecobs_vqvae_path is None:
-        default_log_dir = os.path.join(default_log_dir, 'action_vqvae_only')
-    if action_vqvae_path is None and vecobs_vqvae_path is not None:
-        default_log_dir = os.path.join(default_log_dir, 'action_centroids_vecobs_vqvae')
-    if action_vqvae_path is not None and vecobs_vqvae_path is not None:
-        default_log_dir = os.path.join(default_log_dir, 'action_vqvae_vecobs_vqvae')
-    
-    # set up trainer
-    callbacks = [ModelCheckpoint(monitor='Training/loss', mode='min', every_n_train_steps=save_freq, save_last=True)]
-    trainer = pl.Trainer(
-        progress_bar_refresh_rate=10,
-        log_every_n_steps=10,
-        callbacks=callbacks,
-        gpus=torch.cuda.device_count(),
-        accelerator='dp',
-        default_root_dir=default_log_dir,
-        max_epochs=num_epochs
-    )
-    
-    # fit model
-    trainer.fit(model, dataloader)
-    
-    
-
-if __name__ == '__main__':
-    parser = argparse.ArgumentParser()
-    parser.add_argument('--env_name', type=str, default='MineRLTreechopVectorObf-v0')
-    parser.add_argument('--data_dir', type=str, default='/home/lieberummaas/datadisk/minerl/data')
-    parser.add_argument('--log_dir', type=str, default='/home/lieberummaas/datadisk/minerl/run_logs')
-    parser.add_argument('--feature_extractor_class_name', type=str, default='vqvae', choices=['vqvae', 'vae', 'conv'])
-    parser.add_argument('--feature_extractor_version', type=str, default='0')
-    parser.add_argument('--lr', type=float, default=3e-4)
-    parser.add_argument('--action_num_centroids', type=int, default=None)
-    parser.add_argument('--action_vqvae_version', type=str, default=None)
-    parser.add_argument('--vecobs_vqvae_version', type=str, default=None)
-    parser.add_argument('--hidden_dim', type=int, default=1024)
-    parser.add_argument('--batch_size', type=int, default=500)
-    parser.add_argument('--num_epochs', type=int, default=1)
-    parser.add_argument('--save_freq', type=int, default=100)
-    
-    args = parser.parse_args()
-    
-    train(**vars(args))
\ No newline at end of file
diff --git a/research_code/overfit_dynamics.py b/research_code/overfit_dynamics.py
deleted file mode 100644
index caa70f4..0000000
--- a/research_code/overfit_dynamics.py
+++ /dev/null
@@ -1,128 +0,0 @@
-import dynamics_models
-import datasets
-
-import torch
-from torch.utils.data import DataLoader, random_split
-from torch.utils.tensorboard import SummaryWriter
-from torchvision.utils import make_grid
-
-import pytorch_lightning as pl
-from pytorch_lightning.callbacks import ModelCheckpoint
-
-import numpy as np
-from time import time
-import os
-import argparse
-
-# for debugging
-torch.autograd.set_detect_anomaly(True)
-
-STR_TO_MODEL = {
-    'rssm':dynamics_models.RSSM,
-    'node':dynamics_models.NODEDynamicsModel,
-    'mdn':dynamics_models.MDN_RNN
-}
-
-def train_DynamicsModel(env_name, data_dir, seq_len, lr, 
-                        val_perc, batch_size, num_data, epochs, 
-                        lr_gamma, lr_decrease_freq, log_dir, lr_step_mode, 
-                        model_path, VAE_class, num_components, skip_connection,
-                        val_check_interval, load_from_checkpoint, version_dir,
-                        latent_overshooting, soft_targets, profile, temp, regression):
-    
-    # make sure that relevant dirs exist
-    run_name = f'DynamicsModel/{dynamics_models.MDN_RNN.__name__}/{env_name}'
-    log_dir = os.path.join(log_dir, run_name)
-    os.makedirs(log_dir, exist_ok=True)
-    print(f'\nSaving logs and model to {log_dir}')
-
-    ## some model kwargs
-    optim_kwargs = {'lr':lr}
-    scheduler_kwargs = {'lr_gamma':lr_gamma, 'lr_decrease_freq':lr_decrease_freq, 'lr_step_mode':lr_step_mode}
-    
-    seq_len = seq_len        
-    gru_kwargs = {'num_layers':1, 'hidden_size':1024}
-    model_kwargs = {
-        'gru_kwargs':gru_kwargs, 
-        'seq_len':seq_len, 
-        'VAE_path':model_path,
-        'optim_kwargs':optim_kwargs,
-        'scheduler_kwargs':scheduler_kwargs,
-        'VAE_class':VAE_class,
-        'num_components':num_components,
-        'temp':temp,
-        'skip_connection':skip_connection,
-        'latent_overshooting':latent_overshooting,
-        'soft_targets':soft_targets,
-    }
-    monitor = 'Validation/loss'
-
-    # init model
-    if load_from_checkpoint:
-        checkpoint = os.path.join(version_dir, 'checkpoints', 'last.ckpt')
-        
-        print(f'\nLoading model from {checkpoint}')
-        model = STR_TO_MODEL['mdn'].load_from_checkpoint(checkpoint, lr=lr)
-    else:
-        model = STR_TO_MODEL['mdn'](**model_kwargs)
-
-    # load data
-    data = datasets.DynamicsData(env_name, data_dir, seq_len, num_data)
-    lengths = [len(data)-int(len(data)*val_perc), int(len(data)*val_perc)]
-    train_data, val_data = random_split(data, lengths)
-    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=6, pin_memory=True)
-    val_loader = DataLoader(val_data, shuffle=False, batch_size=batch_size, num_workers=6, pin_memory=True)
-
-    num_batches = len(train_data) // batch_size
-    if len(train_data) % batch_size != 0:
-        num_batches += 1
-
-    print(f'\nnum train samples = {len(train_data)} --> {num_batches} train batches')
-    print(f'num val samples = {len(val_data)}')
-
-    model_checkpoint = ModelCheckpoint(save_weights_only=True, mode="min", monitor=monitor, save_last=True)
-    trainer=pl.Trainer(
-                    precision=32, #32 is normal, 16 is mixed precision
-                    progress_bar_refresh_rate=1, #every N batches update progress bar
-                    log_every_n_steps=10,
-                    callbacks=[model_checkpoint],
-                    gpus=torch.cuda.device_count(),
-                    accelerator='dp', #anything else here seems to lead to crashes/errors
-                    default_root_dir=log_dir,
-                    val_check_interval=val_check_interval if val_check_interval > 1 else float(val_check_interval),
-                    max_epochs=epochs
-                )
-    trainer.logger._default_hp_metric = None # optional logging metric that we don't need right now
-    trainer.fit(model, train_loader, val_loader)
-
-if __name__=='__main__':
-    parser = argparse.ArgumentParser()
-    
-    parser.add_argument('--model_path', help='Path to encoding model')
-    parser.add_argument('--data_dir', default='/home/lieberummaas/datadisk/minerl/data/numpy_data')
-    parser.add_argument('--log_dir', default='/home/lieberummaas/datadisk/minerl/run_logs')
-    parser.add_argument('--env_name', default='MineRLTreechopVectorObf-v0')
-    parser.add_argument('--seq_len', default=4, type=int)
-    parser.add_argument('--batch_size', default=1, type=int)
-    parser.add_argument('--num_data', default=2, type=int, help='Number of datapoints to use')
-    parser.add_argument('--epochs', default=1, type=int)
-    parser.add_argument('--lr', default=3e-4, type=float, help='Learning rate')
-    parser.add_argument('--lr_gamma', default=0.5, type=float, help='Learning rate adjustment factor')
-    parser.add_argument('--lr_step_mode', default='epoch', choices=['epoch', 'step'], type=str, help='Learning rate adjustment interval')
-    parser.add_argument('--lr_decrease_freq', default=1, type=int, help='Learning rate adjustment frequency')
-    parser.add_argument('--val_perc', default=0.5, type=float, help='How much of the data should be used for validation')
-    parser.add_argument('--VAE_class', type=str, default='Conv', choices=['Conv', 'ResNet', 'vqvae'])
-    parser.add_argument('--num_components', type=int, default=5, help='Number of mixture components. Only used in MDN-RNN')
-    parser.add_argument('--skip_connection', action='store_true', help='Whether to use skip connection in MDN-RNN.')
-    parser.add_argument('--val_check_interval', default=1, type=int, help='How often to validate. N == 1 --> once per epoch; N > 1 --> every N steps')
-    parser.add_argument('--load_from_checkpoint', action='store_true')
-    parser.add_argument('--latent_overshooting', action='store_true')
-    parser.add_argument('--soft_targets', action='store_true')
-    parser.add_argument('--profile', action='store_true')
-    parser.add_argument('--regression', action='store_true', help='Whether to perform regression or KL minimization')
-    parser.add_argument('--temp', default=1, type=float)
-    parser.add_argument('--version_dir', default='', type=str, help='Version directory of model, if training is resumed from checkpoint')
-
-    args = vars(parser.parse_args())
-
-    train_DynamicsModel(**args)
diff --git a/research_code/predict_inventory_change.py b/research_code/predict_inventory_change.py
deleted file mode 100644
index 2cd5a1b..0000000
--- a/research_code/predict_inventory_change.py
+++ /dev/null
@@ -1,174 +0,0 @@
-import argparse
-import os
-import datasets
-import visual_models
-import vqvae
-
-
-from torch.utils.data import random_split, DataLoader
-import pytorch_lightning as pl
-from pytorch_lightning.callbacks import ModelCheckpoint
-import torch
-import torch.nn as nn
-from torch.optim import AdamW
-
-import einops
-import numpy as np
-
-vae_model_by_str = {
-    'Conv':visual_models.ConvVAE,
-    'ResNet':visual_models.ResnetVAE,
-    'vqvae':vqvae.VQVAE
-}
-
-class InventoryPredictor(pl.LightningModule):
-    def __init__(self, optim_kwargs, scheduler_kwargs, VAE_path, 
-                 VAE_class='vqvae'):
-        super().__init__()
-        self.save_hyperparameters()
-
-        self.loss_fn = nn.BCEWithLogitsLoss()
-
-        # load VAE
-        self.VAE = vae_model_by_str[VAE_class].load_from_checkpoint(VAE_path)
-        
-        dummy, dummy_idcs, _ = self.VAE.encode_only(torch.ones(2,3,64,64).float().to(self.VAE.device))
-            
-        num_channels = dummy.shape[1]
-
-        self.conv_net = nn.Sequential(
-            nn.Conv2d(in_channels=num_channels, out_channels=256, kernel_size=3, padding=1, stride=2), # 16 -> 8
-            nn.GELU(),
-            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1, stride=2), # 8 -> 4
-            nn.GELU(),
-            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1, stride=2), # 4 -> 2
-            nn.GELU(),
-            nn.Conv2d(in_channels=1024, out_channels=2048, kernel_size=3, padding=1, stride=2)#, # 2 -> 1
-            #nn.GELU()
-        )
-        
-        self.mlp = nn.Sequential(
-            nn.Linear(2048+128, 1024),
-            nn.GELU(),
-            nn.Linear(1024, 512),
-            nn.GELU(),
-            nn.Linear(512, 256),
-            nn.GELU(),
-            nn.Linear(256, 128),
-            nn.GELU(),
-            nn.Linear(128, 1)
-        )
-        
-    def forward(self, pov, vec_obs, act):
-        out, _, _ = self.VAE.encode_only(pov)        
-        out = einops.rearrange(self.conv_net(out), 'b c h w -> b (c h w)')
-        out = torch.cat([out, vec_obs, act], dim=1)
-        out = self.mlp(out).squeeze()
-        return out
-    
-    def configure_optimizers(self):
-        # set up optimizer
-        optimizer =  AdamW(self.parameters(), **self.hparams.optim_kwargs)
-        # set up scheduler
-        lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, self.hparams.scheduler_kwargs['lr_gamma'])
-        lr_dict = {
-            'scheduler': lr_scheduler,
-            'interval': self.hparams.scheduler_kwargs['lr_step_mode'],
-            'frequency': self.hparams.scheduler_kwargs['lr_decrease_freq'],
-        }
-        return {'optimizer':optimizer, 'lr_scheduler':lr_dict}
-    
-    def training_step(self, batch, batch_idx):
-        pov, vec_obs, act, targets = batch
-        pred = self(pov, vec_obs, act)
-        loss = self.loss_fn(pred, targets)
-        accuracy = (targets[torch.sigmoid(pred) >= 0.5].sum() + (1 - targets)[torch.sigmoid(pred) < 0.5].sum()) / len(targets) 
-        self.log('Training/Accuracy', accuracy, on_step=True)
-        self.log('Training/Loss', loss, on_step=True)
-        return loss
-    
-    def validation_step(self, batch, batch_idx):
-        pov, vec_obs, act, targets = batch
-        pred = self(pov, vec_obs, act)
-        loss = self.loss_fn(pred, targets)
-        accuracy = (targets[torch.sigmoid(pred) >= 0.5].sum() + (1 - targets)[torch.sigmoid(pred) < 0.5].sum()) / len(targets) 
-        self.log('Validation/Accuracy', accuracy, on_epoch=True)
-        self.log('Validation/Loss', loss, on_epoch=True)
-        return loss
-
-def main(env_name, batch_size, lr, load_from_checkpoint, version, vae_path, data_dir, log_dir,
-        num_data, epochs, lr_gamma, lr_step_mode, lr_decrease_freq, val_perc, val_check_interval, VAE_class):
-    # make sure that relevant dirs exist
-    run_name = f'PredictInventory/{env_name}'
-    log_dir = os.path.join(log_dir, run_name)
-    os.makedirs(log_dir, exist_ok=True)
-    print(f'\nSaving logs and model to {log_dir}')
-
-    ## some model kwargs
-    optim_kwargs = {'lr':lr}
-    scheduler_kwargs = {'lr_gamma':lr_gamma, 'lr_decrease_freq':lr_decrease_freq, 'lr_step_mode':lr_step_mode}
-    model_kwargs = {
-        'VAE_path':vae_path,
-        'optim_kwargs':optim_kwargs,
-        'scheduler_kwargs':scheduler_kwargs,
-        'VAE_class':VAE_class
-    }
-
-    if load_from_checkpoint:
-        checkpoint_file = os.path.join(log_dir, 'lightning_logs', f'version_{version}', 'checkpoints', 'last.ckpt')
-        print(f'\nLoading model from {checkpoint_file}')
-        model = InventoryPredictor.load_from_checkpoint(checkpoint_file, **model_kwargs)
-    else:
-        model = InventoryPredictor(**model_kwargs)
-        
-    
-    # load data
-    data = datasets.VectorObsData(env_name, data_dir, num_data)
-    lengths = [len(data)-int(len(data)*val_perc), int(len(data)*val_perc)]
-    train_data, val_data = random_split(data, lengths)
-    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=6, pin_memory=True)
-    val_loader = DataLoader(val_data, shuffle=False, batch_size=batch_size, num_workers=6, pin_memory=True)
-
-    num_batches = len(train_data) // batch_size
-    if len(train_data) % batch_size != 0:
-        num_batches += 1
-
-    print(f'\nnum train samples = {len(train_data)} --> {num_batches} train batches')
-    print(f'num val samples = {len(val_data)}')
-
-    model_checkpoint = ModelCheckpoint(save_weights_only=True, mode="min", monitor='Validation/Loss', save_last=True)
-    trainer=pl.Trainer(
-                    precision=32, #32 is normal, 16 is mixed precision
-                    progress_bar_refresh_rate=1, #every N batches update progress bar
-                    log_every_n_steps=10,
-                    callbacks=[model_checkpoint],
-                    gpus=torch.cuda.device_count(),
-                    accelerator='dp', #anything else here seems to lead to crashes/errors
-                    default_root_dir=log_dir,
-                    val_check_interval=val_check_interval if val_check_interval > 1 else float(val_check_interval),
-                    max_epochs=epochs
-                )
-    trainer.fit(model, train_loader, val_loader)
-
-if __name__ == '__main__':
-    p = argparse.ArgumentParser()
-    p.add_argument('--env_name', default='MineRLTreechopVectorObf-v0')
-    p.add_argument('--batch_size', default=100, type=int)
-    p.add_argument('--lr', default=3e-4, type=float)
-    p.add_argument('--load_from_checkpoint', action='store_true')
-    p.add_argument('--version', default=0, type=int, help='Version of model, if training is resumed from checkpoint')
-    p.add_argument('--vae_path', help='Path to encoding model')
-    p.add_argument('--data_dir', default='/home/lieberummaas/datadisk/minerl/data/numpy_data')
-    p.add_argument('--log_dir', default='/home/lieberummaas/datadisk/minerl/run_logs')
-    p.add_argument('--num_data', default=0, type=int, help='Number of datapoints to use')
-    p.add_argument('--epochs', default=10, type=int)
-    p.add_argument('--lr_gamma', default=1, type=float, help='Learning rate adjustment factor')
-    p.add_argument('--lr_step_mode', default='epoch', choices=['epoch', 'step'], type=str, help='Learning rate adjustment interval')
-    p.add_argument('--lr_decrease_freq', default=1, type=int, help='Learning rate adjustment frequency')
-    p.add_argument('--val_perc', default=0.1, type=float, help='How much of the data should be used for validation')
-    p.add_argument('--val_check_interval', default=1, type=int, help='How often to validate. N == 1 --> once per epoch; N > 1 --> every N steps')
-    p.add_argument('--VAE_class', default='vqvae')    
-    
-    args = p.parse_args()
-    
-    main(**vars(args))
\ No newline at end of file
diff --git a/research_code/pretraining.sh b/research_code/pretraining.sh
index c9fd1f8..82ad78c 100644
--- a/research_code/pretraining.sh
+++ b/research_code/pretraining.sh
@@ -17,6 +17,6 @@ for feature_extractor_cls in vqvae conv vae
             PATH=""
         fi
         echo "Now training ${feature_extractor_cls}"
-        python PretrainDQN.py --log_dir $LOGDIR --feature_extractor_cls $feature_extractor_cls --num_workers 6 --train_feature_extractor --feature_extractor_path $PATH
+        python DQfD_pretrain.py --log_dir $LOGDIR --feature_extractor_cls $feature_extractor_cls --num_workers 6 --feature_extractor_path $PATH
     done
 ~         
\ No newline at end of file
diff --git a/research_code/reward_model.py b/research_code/reward_model.py
index ceaad4a..7bc4f55 100644
--- a/research_code/reward_model.py
+++ b/research_code/reward_model.py
@@ -1,169 +1,180 @@
 
-import torch
-from torch.optim import AdamW
-import torch.nn as nn
-from torch.utils.data import DataLoader
-
-import pytorch_lightning as pl
-from pytorch_lightning.callbacks import ModelCheckpoint
-
-import numpy as np
-import os
-import argparse
-
-import datasets
-from vecobs_vqvae import VecObsVQVAE
-
-
-class RewardMLP(pl.LightningModule):
-    def __init__(self, hidden_dims, learning_rate, scheduler_kwargs, quantizer_path, seq_len):
-        super().__init__()
-        self.save_hyperparameters()
-
-        assert seq_len in [1,2]
-
-        # set up quantizer
-        if quantizer_path is not None:
-            self.use_quantizer = True
-            print(f'\nLoading quantizer from {quantizer_path}')
-            self.quantizer = VecObsVQVAE.load_from_checkpoint(quantizer_path)
-            self.quantizer.eval()
-            dummy_vec = torch.ones(1,64).to(self.quantizer.device)
-            dummy_quant = self.quantizer.encode_only(dummy_vec)[0]
-            self.input_dim = dummy_quant.shape[-1]
-        else:
-            self.use_quantizer = False
-            self.input_dim = 64
-
-        # create MLP
-        self.mlp = [nn.Sequential(nn.Linear(self.input_dim, hidden_dims[0]), nn.GELU())]
-        for i in range(len(hidden_dims)-1):
-            self.mlp.append(nn.Sequential(nn.Linear(hidden_dims[i], hidden_dims[i+1]), nn.GELU()))
-        self.mlp.append(nn.Linear(hidden_dims[-1], 1))
-        self.mlp = nn.Sequential(*self.mlp)
+"""
+Using a reward model doesn't make sense if we are already monitoring the 1-step TD error
+"""
+
+
+# import torch
+# from torch.optim import AdamW
+# import torch.nn as nn
+# from torch.utils.data import DataLoader
+
+# import pytorch_lightning as pl
+# from pytorch_lightning.callbacks import ModelCheckpoint
+
+# import numpy as np
+# import os
+# import argparse
+
+# import datasets
+# from vecobs_vqvae import VecObsVQVAE
+
+
+# class RewardMLP(pl.LightningModule):
+#     def __init__(
+#         self, 
+#         hidden_dims, 
+#         learning_rate, 
+#         visual_model_cls,
+#         visual_model_path,
+#         dynamics_model_cls,
+#         dynamics_model_path
+#     ):
+#         super().__init__()
+#         self.save_hyperparameters()
+
+#         # load VAE
+#         self.visual_model = visual_model_by_str[visual_model_cls].load_from_checkpoint(visual_model_path)
+#         self.visual_model.eval()
+
+#         if self.hparams.visual_model_cls == 'vqvae':
+#             if use_one_hot:
+#                 print('\nUsing one-hot representation')
+#                 self.latent_dim = self.visual_model.quantizer.num_variables * self.visual_model.quantizer.codebook_size
+#             else:
+#                 print('\nUsing learned embedding representation')
+#                 self.latent_dim = self.visual_model.quantizer.num_variables * self.visual_model.quantizer.embedding_dim
+
+#         elif self.hparams.visual_model_cls == 'vae':
+#             self.latent_dim = self.visual_model.hparams.encoder_kwargs['latent_dim']
+#         print(f'\nlatent_dim = {self.latent_dim}')
+
+#         self.input_dim = self.latent_dim + 64
+
+#         # create MLP
+#         self.mlp = [nn.Sequential(nn.Linear(self.input_dim, hidden_dims[0]), nn.GELU())]
+#         for i in range(len(hidden_dims)-1):
+#             self.mlp.append(nn.Sequential(nn.Linear(hidden_dims[i], hidden_dims[i+1]), nn.GELU()))
+#         self.mlp.append(nn.Linear(hidden_dims[-1], 1))
+#         self.mlp = nn.Sequential(*self.mlp)
         
-        # set up loss function
-        self.loss_fn = nn.MSELoss(reduction='none')
+#         # set up loss function
+#         self.loss_fn = nn.MSELoss(reduction='none')
         
 
-    def forward(self, vec_obs):
-        out = vec_obs
-        if self.use_quantizer:
-            out = self.quantizer.encode_only(out)[0]
-        return self.mlp(out)
+#     def forward(self, vec_obs):
+#         out = vec_obs
+#         if self.use_quantizer:
+#             out = self.quantizer.encode_only(out)[0]
+#         return self.mlp(out)
     
-    def training_step(self, batch, batch_idx):
-        vec_obs, reward = batch
-        vec_obs = vec_obs[0]
-        reward = reward[0]
-        print(f'{vec_obs.shape = }')
-        print(f'{reward.shape = }')
-        if self.hparams.seq_len == 2:
-            vec_obs = torch.diff(vec_obs, n=1, dim=0)
-            reward = reward[1:]
-        reward = torch.log2(1+reward)
-        loss_scaling_factor = 2 ** reward.detach()
-        predicted_reward = self.forward(vec_obs)[:,0]
+#     def training_step(self, batch, batch_idx):
+#         vec_obs, reward = batch
+#         vec_obs = vec_obs[0]
+#         reward = reward[0]
+#         print(f'{vec_obs.shape = }')
+#         print(f'{reward.shape = }')
+#         if self.hparams.seq_len == 2:
+#             vec_obs = torch.diff(vec_obs, n=1, dim=0)
+#             reward = reward[1:]
+#         reward = torch.log2(1+reward)
+#         loss_scaling_factor = 2 ** reward.detach()
+#         predicted_reward = self.forward(vec_obs)[:,0]
         
-        loss = torch.mean(self.loss_fn(predicted_reward, reward) * loss_scaling_factor)
-
-        self.log('Training/loss', loss, on_step=True)
-        return loss
-
-    def configure_optimizers(self):
-        # set up optimizer, only train mlp params
-        optimizer =  AdamW(self.mlp.parameters(), lr=self.hparams.learning_rate)
-        # set up 
-        lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, self.hparams.scheduler_kwargs['lr_gamma'])
-        lr_dict = {
-            'scheduler': lr_scheduler,
-            'interval': self.hparams.scheduler_kwargs['lr_step_mode'],
-            'frequency': self.hparams.scheduler_kwargs['lr_decrease_freq'],
-        }
-        return {'optimizer':optimizer, 'lr_scheduler':lr_dict}
-
-def train(
-    env_name, 
-    data_dir, 
-    log_dir, 
-    batch_size,
-    epochs, 
-    save_freq,
-    lr,
-    lr_gamma, 
-    lr_decrease_freq, 
-    lr_step_mode,
-    model_class, 
-    load_from_checkpoint, 
-    version,
-    quantizer_version,
-    seq_len
-):
-    pl.seed_everything(1337)
-
-    # set quantizer_path
-    if quantizer_version is None:
-        quantizer_path = None
-    else:
-        quantizer_path = os.path.join(log_dir, 'VecObsVQVAE', env_name, 'lightning_logs', 'version_'+str(quantizer_version), 'checkpoints', 'last.ckpt')
-
-    # make sure that relevant dirs exist
-    run_name = f'RewardModel_{model_class}/{env_name}'
-    log_dir = os.path.join(log_dir, run_name)
-    os.makedirs(log_dir, exist_ok=True)
-    print(f'Saving logs and model to {log_dir}')
-
-    # set hidden dims and input dim
-    # TODO parse these as args
-    hidden_dims = [100,100]
+#         loss = torch.mean(self.loss_fn(predicted_reward, reward) * loss_scaling_factor)
+
+#         self.log('Training/loss', loss, on_step=True)
+#         return loss
+
+#     def configure_optimizers(self):
+#         # set up optimizer, only train mlp params
+#         optimizer =  AdamW(self.mlp.parameters(), lr=self.hparams.learning_rate)
+#         return optimizer
+        
+# def train(
+#     env_name, 
+#     data_dir, 
+#     log_dir, 
+#     epochs, 
+#     save_freq,
+#     lr,
+#     model_class, 
+#     load_from_checkpoint, 
+#     version,
+#     visual_model_cls,
+#     visual_model_path,
+#     dynamics_model_cls,
+#     dynamics_model_path
+# ):
+#     pl.seed_everything(1337)
+
+#     # make sure that relevant dirs exist
+#     os.makedirs(log_dir, exist_ok=True)
+#     print(f'Saving logs and model to {log_dir}')
+
+#     # set hidden dims and input dim
+#     # TODO parse these as args
+#     hidden_dims = [1000,1000]
     
-    # instantiate model
-    if load_from_checkpoint:
-        checkpoint = os.path.join(log_dir, 'lightning_logs', 'version_'+str(version), 'checkpoints', 'last.ckpt')        
-        print(f'Loading model from {checkpoint}')
-        model = RewardMLP.load_from_checkpoint(checkpoint, lr=lr)
-    else:
-        scheduler_kwargs = {'lr_gamma':lr_gamma, 'lr_decrease_freq':lr_decrease_freq, 'lr_step_mode':lr_step_mode}
-        model = RewardMLP(hidden_dims, lr, scheduler_kwargs, quantizer_path, seq_len)
+#     # instantiate model
+#     model = RewardMLP(
+#         hidden_dims, 
+#         lr, 
+#         visual_model_cls, 
+#         visual_model_path, 
+#         dynamics_model_cls, 
+#         dynamics_model_path
+#     )
         
-    # load data
-    data = datasets.RewardData(env_name, data_dir, epochs)
-    dataloader = DataLoader(data, num_workers=1)
-
-    # create callbacks to sample reconstructed images and for model checkpointing
-    checkpoint_callback = ModelCheckpoint(mode="min", monitor="Training/loss", save_last=True, every_n_train_steps=save_freq)
-    trainer=pl.Trainer(
-        progress_bar_refresh_rate=1, #every N batches update progress bar
-        callbacks=[checkpoint_callback],
-        gpus=torch.cuda.device_count(),
-        default_root_dir=log_dir,
-        max_epochs=epochs,
-        log_every_n_steps=10,
-    )
+#     # load data
+#     data = datasets.TrajectoryData(env_name, data_dir)
+#     dataloader = DataLoader(data, num_workers=1, pin_memory=True, batch_size=1)
+
+#     # create callbacks to sample reconstructed images and for model checkpointing
+#     checkpoint_callback = ModelCheckpoint(mode="min", monitor="Training/loss", save_last=True, every_n_train_steps=save_freq)
+#     config = dict(
+#         env_name=env_name,
+#         visual_model_cls=visual_model_cls,
+#         visual_model_path=visual_model_path,
+#         dynamics_model_cls=dynamics_model_cls,
+#         dynamics_model_path=dynamics_model_path,
+#         use_one_hot=use_one_hot,
+#     )
+#     tags = [visual_model_cls, 'one_hot_'+str(use_one_hot), 'dyn_'+dynamics_model_cls]
+#     wandb_logger = WandbLogger(project='RewardModel', config=config, tags=tags)
+#     trainer=pl.Trainer(
+#         logger=wandb_logger,
+#         progress_bar_refresh_rate=1, #every N batches update progress bar
+#         callbacks=[checkpoint_callback],
+#         gpus=torch.cuda.device_count(),
+#         default_root_dir=log_dir,
+#         max_epochs=epochs,
+#         log_every_n_steps=10,
+#     )
                     
-    # fit model
-    trainer.fit(model, dataloader)
+#     # fit model
+#     trainer.fit(model, dataloader)
+    
+
+# if __name__=='__main__':
+#     parser = argparse.ArgumentParser()
+
+#     parser.add_argument('--data_dir', default='/home/lieberummaas/datadisk/minerl/data')
+#     parser.add_argument('--log_dir', default='/home/lieberummaas/datadisk/minerl/run_logs')
+#     parser.add_argument('--env_name', default='MineRLObtainIronPickaxeVectorObf-v0')
     
+#     parser.add_argument('--save_freq', default=100, type=int)
+#     parser.add_argument('--epochs', default=1, type=int)
+#     parser.add_argument('--lr', default=3e-4, type=float, help='Learning rate')
+#     parser.add_argument('--load_from_checkpoint', default=False, action='store_true')
+#     parser.add_argument('--version', default=0, type=int, help='Version of model, if training is resumed from checkpoint')
+
+#     parser.add_argument('--model_class', default='MLP', type=str)
+#     parser.add_argument('--visual_model_cls', type=str, default='vae', choices=['vae', 'vqvae'])
+#     parser.add_argument('--visual_model_path', type=str, default='none')
+#     parser.add_argument('--dynamics_model_cls', type=str, default='none', choices=['mdn', 'none'])
+#     parser.add_argument('--dynamics_model_path', type=str, default='none')
+
+#     args = vars(parser.parse_args())
 
-if __name__=='__main__':
-    parser = argparse.ArgumentParser()
-
-    parser.add_argument('--data_dir', default='/home/lieberummaas/datadisk/minerl/data')
-    parser.add_argument('--log_dir', default='/home/lieberummaas/datadisk/minerl/run_logs')
-    parser.add_argument('--env_name', default='MineRLObtainIronPickaxeVectorObf-v0')
-    parser.add_argument('--batch_size', default=128, type=int)
-    parser.add_argument('--save_freq', default=100, type=int)
-    parser.add_argument('--epochs', default=1, type=int)
-    parser.add_argument('--seq_len', default=1, type=int)
-    parser.add_argument('--lr', default=3e-4, type=float, help='Learning rate')
-    parser.add_argument('--lr_gamma', default=0.5, type=float, help='Learning rate adjustment factor')
-    parser.add_argument('--lr_step_mode', default='epoch', choices=['epoch', 'step'], type=str, help='Learning rate adjustment interval')
-    parser.add_argument('--lr_decrease_freq', default=1, type=int, help='Learning rate adjustment frequency')
-    parser.add_argument('--load_from_checkpoint', default=False, action='store_true')
-    parser.add_argument('--version', default=0, type=int, help='Version of model, if training is resumed from checkpoint')
-    parser.add_argument('--model_class', default='MLP', type=str)
-    parser.add_argument('--quantizer_version', default=None, type=int)
-
-    args = vars(parser.parse_args())
-
-    train(**args)
+#     train(**args)
diff --git a/research_code/testing_trafo.py b/research_code/testing_trafo.py
deleted file mode 100644
index c436a17..0000000
--- a/research_code/testing_trafo.py
+++ /dev/null
@@ -1,30 +0,0 @@
-from x_transformers import TransformerWrapper, Decoder, Encoder, XTransformer
-import torch
-
-L = 10
-D = 512
-B = 2
-
-model = XTransformer(
-    dim = 512,
-    enc_num_tokens = 3,
-    enc_depth = 6,
-    enc_heads = 8,
-    enc_max_seq_len = 10,
-    dec_num_tokens = 3,
-    dec_depth = 6,
-    dec_heads = 8,
-    dec_max_seq_len = 10,
-    tie_token_emb = True,      # tie embeddings of encoder and decoder
-    enc_rotary_pos_emb = True,
-    dec_rotary_pos_emb = True
-).to('cuda')
-
-
-src = torch.randint(0, 3, (B,L),device='cuda',dtype=torch.long)
-src_mask = torch.ones_like(src).bool()
-src_mask[:,-1] = False
-output = model(src, src, src_mask=src_mask, tgt_mask=~src_mask)
-
-print(output.shape)
-print(output)
\ No newline at end of file
diff --git a/research_code/train_DynamicsModel.py b/research_code/train_DynamicsModel.py
index d5e3c92..5d3569f 100755
--- a/research_code/train_DynamicsModel.py
+++ b/research_code/train_DynamicsModel.py
@@ -1,13 +1,14 @@
-import dynamics_models
+from dynamics_models import MDN_RNN
 import datasets
 
 import torch
 from torch.utils.data import DataLoader, random_split
-from torch.utils.tensorboard import SummaryWriter
 from torchvision.utils import make_grid
 
 import pytorch_lightning as pl
 from pytorch_lightning.callbacks import ModelCheckpoint
+from pytorch_lightning.loggers import WandbLogger
+import wandb
 
 import numpy as np
 from time import time
@@ -17,12 +18,8 @@ import einops
 
 
 # for debugging
-torch.autograd.set_detect_anomaly(True)
+#torch.autograd.set_detect_anomaly(True)
 
-STR_TO_MODEL = {
-    'rssm':dynamics_models.RSSM,
-    'mdn':dynamics_models.MDN_RNN
-}
 
 class PredictionCallback(pl.Callback):
 
@@ -39,8 +36,16 @@ class PredictionCallback(pl.Callback):
         self.every_n_epochs = every_n_epochs
         self.every_n_batches = every_n_batches
 
-        #x_samples, x_mean = pl_module.sample(self.batch_size)
-        pov, vec_obs, act = map(lambda x: x[None,:seq_len], next(iter(dataset))[:-1])
+        if isinstance(dataset, datasets.DynamicsData):
+            iterator = iter(dataset)
+            for _ in range(2000):
+                b = next(iterator)
+            pov, vec_obs, act = map(lambda x: x[None,:seq_len], next(iterator)[:-1])
+        elif isinstance(dataset, datasets.TrajectoryData):
+            pov, vec_obs, act = map(lambda x: x[None,:seq_len], dataset[3][:3])
+        else:
+            raise NotImplementedError
+
         #pov, vec_obs, act = map(lambda x: x[:,:seq_len], dataset[0][:-1])
         pov = torch.from_numpy(pov)
         vec = torch.from_numpy(vec_obs)
@@ -64,7 +69,7 @@ class PredictionCallback(pl.Callback):
         if (pl_module.global_step+1) % self.every_n_batches == 0:
             self.predict_sequence(trainer, pl_module, pl_module.global_step+1)
 
-    def predict_sequence(self, trainer, pl_module: dynamics_models.MDN_RNN, epoch):
+    def predict_sequence(self, trainer, pl_module: MDN_RNN, epoch):
         """
         Function that predicts sequence and generates images.
         Inputs:
@@ -77,104 +82,110 @@ class PredictionCallback(pl.Callback):
             self.sequence = list(map(lambda x: x.to(pl_module.device), self.sequence))
         
         # predict sequence
-        _, pov_samples, _ = pl_module.forward(*self.sequence)
-        if pl_module.hparams.VAE_class == 'vqvae':
-            pov_samples = einops.rearrange(pov_samples, 'b t c (h w) -> (b t) c h w', h=16, w=16)
-            
-            # reconstruct images
-            pov_reconstruction = pl_module.VAE.decode_only(pov_samples)
-            
-            # stack images
-            images = torch.stack([self.sequence[0][0,1:], pov_reconstruction], dim=1).reshape(((self.seq_len -1) * 2, 3, 64, 64))
-
-            # log images to tensorboard
-            trainer.logger.experiment.add_image('Prediction', make_grid(images, nrow=2), epoch)
+        if pl_module.hparams.visual_model_cls == 'vqvae':
+            # one-step predictions
+            logits, mixing_logits, *_ = pl_module.forward(*self.sequence)
+            logits = einops.rearrange(logits, 'b t K d -> (b t) K d')
+            mixing_logits = einops.rearrange(mixing_logits, 'b t K -> (b t) K')
+            sampled_mix = torch.nn.functional.gumbel_softmax(mixing_logits, tau=1, hard=True, dim=-1)
+            sampled_logits = torch.einsum('a b c, a b -> a c', logits, sampled_mix)
+            sampled_logits = einops.rearrange(sampled_logits, 'b (n d) -> b n d', n=32, d=32)
+            sampled_one_hot = torch.nn.functional.gumbel_softmax(sampled_logits, hard=True, dim=-1)
+
+            pov_samples = []
+            for i in range(sampled_one_hot.shape[1]):
+                pov_samples.append(sampled_one_hot[:,i] @ pl_module.visual_model.quantizer.embeds[i].weight)
+            pov_samples = torch.stack(pov_samples, dim=1)[:-1]
+
+            # n-step predictions
+            n_step_predictions = pl_module.imaginate(self.sequence[0][0][0], self.sequence[1][0][0], self.sequence[2][0])[:-1, :-64]
+            n_step_predictions = einops.rearrange(n_step_predictions, 'b (n d) -> b n d', n=32, d=32)
         
         else:
-            pov_samples = einops.rearrange(pov_samples, 'b t c h w -> (b t h w) c')
-            # reconstruct images
-            pov_reconstruction = pl_module.VAE.decode_only(pov_samples)
-
-            images = torch.stack([self.sequence[0][0,1:], pov_reconstruction], dim=1).reshape(((self.seq_len -1) * 2, 3, 64, 64))
-
-            # log images to tensorboard
-            trainer.logger.experiment.add_image('Prediction', make_grid(images, nrow=2), epoch)
-
-
-def train_DynamicsModel(env_name, data_dir, dynamics_model, seq_len, lr, 
-                        batch_size, num_data, epochs, 
-                        lr_gamma, lr_decrease_freq, log_dir, lr_step_mode, 
-                        VAE_class, vae_version, num_components,
-                        val_check_interval, load_from_checkpoint, version,
-                        profile, temp,
-                        conditioning_len, curriculum_threshold, curriculum_start,
-                        save_freq):
+            # one-step predictions
+            means, log_stds, mixing_logits, *_ = pl_module.forward(*self.sequence)
+            means = einops.rearrange(means, 'b t K d -> (b t) K d')
+            log_stds = einops.rearrange(log_stds, 'b t K d -> (b t) K d')
+            mixing_logits = einops.rearrange(mixing_logits, 'b t K -> (b t) K')
+            sampled_mix = torch.argmax(torch.nn.functional.gumbel_softmax(mixing_logits, tau=1, hard=True, dim=-1),dim=1)
+            sampled_means = means[torch.arange(len(means)), sampled_mix]
+            sampled_log_stds = log_stds[torch.arange(len(log_stds)), sampled_mix]
+            pov_samples = sampled_means + torch.exp(sampled_log_stds) * torch.normal(torch.zeros_like(sampled_means), torch.ones_like(sampled_log_stds))
+            pov_samples = pov_samples[:-1]
+
+            # n-step predictions
+            n_step_predictions = pl_module.imaginate(self.sequence[0][0][0], self.sequence[1][0][0], self.sequence[2][0])[:-1, :-64]
+            
+        # reconstruct images
+        base_reconstruction = pl_module.visual_model.reconstruct_only(self.sequence[0][0,1:])
+        one_step_pov_reconstruction = pl_module.visual_model.decode_only(pov_samples)
+        n_step_pov_reconstruction = pl_module.visual_model.decode_only(n_step_predictions)
+
+        # log images to tensorboard
+        images = torch.stack([self.sequence[0][0,1:], base_reconstruction, one_step_pov_reconstruction, n_step_pov_reconstruction], dim=1).reshape(((self.seq_len -1) * 4, 3, 64, 64))
+        pl_module.logger.experiment.log({'Predictions (raw | base reconstruction | one-step | n-step)': wandb.Image(make_grid(images, nrow=4))})
+
+
+def train_DynamicsModel(
+    env_name, 
+    data_dir, 
+    log_dir, 
+    seq_len, 
+    lr, 
+    batch_size, 
+    use_whole_trajectories,
+    num_epochs, 
+    visual_model_cls, 
+    visual_model_path, 
+    num_components,
+    gru_hidden_size,
+    load_from_checkpoint, 
+    checkpoint_path,
+    curriculum_threshold, 
+    curriculum_start,
+    save_freq,
+    use_one_hot
+):
     
     pl.seed_everything(1337)
-
-    if VAE_class == 'vae':
-        vae_path = os.path.join(log_dir, 'VAE', env_name, 'lightning_logs', 'version_'+str(vae_version), 'checkpoints/last.ckpt')
-    elif VAE_class == 'vqvae':
-        #vae_path = os.path.join(log_dir, 'VQVAE', env_name, 'lightning_logs', 'version_'+str(vae_version), 'checkpoints/last.ckpt')
-        vae_path = os.path.join(log_dir, 'VQVAE', env_name, 'lightning_logs', 'version_'+str(vae_version), 'checkpoints/last.ckpt')
-
+    
+    if use_whole_trajectories:
+        print('\nTraining on complete trajectories, batch size is forced to 1 and seq_len will vary!\n')
+        batch_size = 1
+        
     # make sure that relevant dirs exist
-    run_name = f'DynamicsModel/{STR_TO_MODEL[dynamics_model].__name__}/{env_name}'
-    log_dir = os.path.join(log_dir, run_name)
     os.makedirs(log_dir, exist_ok=True)
     print(f'\nSaving logs and model to {log_dir}')
 
     ## some model kwargs
     optim_kwargs = {'lr':lr}
-    scheduler_kwargs = {'lr_gamma':lr_gamma, 'lr_decrease_freq':lr_decrease_freq, 'lr_step_mode':lr_step_mode}
-    
-    if dynamics_model == 'rssm':
-        raise NotImplementedError
-        """
-        seq_len = seq_len        
-        lstm_kwargs = {'num_layers':1, 'hidden_size':2048}
-        model_kwargs = {
-            'lstm_kwargs':lstm_kwargs, 
-            'seq_len':seq_len, 
-            'VAE_path':model_path,
-            'optim_kwargs':optim_kwargs,
-            'scheduler_kwargs':scheduler_kwargs,
-            'VAE_class':VAE_class,
-            'latent_overshooting':latent_overshooting
-        }
-        monitor = 'Validation/loss'
-        """
-    elif dynamics_model == 'mdn':
-        gru_kwargs = {'num_layers':1, 'hidden_size':512}
-        model_kwargs = {
-            'gru_kwargs':gru_kwargs, 
-            'seq_len':seq_len, 
-            'visual_model_path':vae_path,
-            'optim_kwargs':optim_kwargs,
-            'scheduler_kwargs':scheduler_kwargs,
-            'visual_model_cls':VAE_class,
-            'num_components':num_components,
-            'temp':temp,
-            'conditioning_len':conditioning_len,
-            'curriculum_threshold':curriculum_threshold,
-            'curriculum_start':curriculum_start,
-        }
-        monitor = 'Training/loss'
-    else:
-        ValueError(f"Unrecognized model {dynamics_model}")
-    ##
+    gru_kwargs = {'num_layers':1, 'hidden_size':gru_hidden_size}
+    model_kwargs = {
+        'gru_kwargs':gru_kwargs, 
+        'visual_model_path':visual_model_path,
+        'optim_kwargs':optim_kwargs,
+        'visual_model_cls':visual_model_cls,
+        'num_components':num_components,
+        'curriculum_threshold':curriculum_threshold,
+        'curriculum_start':curriculum_start,
+        'use_one_hot':use_one_hot
+    }
+    monitor = 'Training/loss'
     
     # init model
     if load_from_checkpoint:
-        checkpoint = os.path.join(log_dir, 'lightning_logs', 'version_'+str(version), 'checkpoints', 'last.ckpt')
-        print(f'\nLoading model from {checkpoint}')
-        model = STR_TO_MODEL[dynamics_model].load_from_checkpoint(checkpoint)
+        print(f'\nLoading model from {checkpoint_path}')
+        model = MDN_RNN.load_from_checkpoint(checkpoint_path)
     else:
-        model = STR_TO_MODEL[dynamics_model](**model_kwargs)
+        model = MDN_RNN(**model_kwargs)
 
     # load data
-    #train_data = datasets.SingleSequenceDynamics(env_name, data_dir, seq_len + conditioning_len, batch_size)
-    train_data = datasets.DynamicsData(env_name, data_dir, seq_len + conditioning_len, batch_size)
+    if use_whole_trajectories:
+        train_data = datasets.TrajectoryData(env_name, data_dir)
+    else:
+        raise NotImplementedError("If you want to use this, make sure to only train on action centroids")
+        #train_data = datasets.DynamicsData(env_name, data_dir, seq_len, batch_size)
+    
     train_loader = DataLoader(train_data, batch_size=batch_size, num_workers=1, pin_memory=True)
 
     model_checkpoint = ModelCheckpoint(mode="min", monitor=monitor, save_last=True, every_n_train_steps=save_freq)
@@ -184,16 +195,23 @@ def train_DynamicsModel(env_name, data_dir, dynamics_model, seq_len, lr,
         seq_len=10
     )
     callbacks = [model_checkpoint, prediction_callback]
-    #callbacks = [model_checkpoint]
+    config = dict(
+        env_name=env_name,
+        visual_model_cls=visual_model_cls,
+        dynamics_model='MDN_RNN',
+        use_one_hot=use_one_hot,
+        use_whole_trajectories=use_whole_trajectories,
+        gru_hidden_size=gru_hidden_size
+    )
+    wandb_logger = WandbLogger(project='DynamicsModel', config=config, tags=['MDN_RNN', visual_model_cls, 'one_hot_'+str(use_one_hot)])
     trainer=pl.Trainer(
+        logger=wandb_logger,
         progress_bar_refresh_rate=1, #every N batches update progress bar
         log_every_n_steps=1,
         callbacks=callbacks,
         gpus=torch.cuda.device_count(),
-        accelerator='dp', #anything else here seems to lead to crashes/errors
         default_root_dir=log_dir,
-        max_epochs=epochs,
-        track_grad_norm=2,
+        max_epochs=num_epochs,
     )
     trainer.fit(model, train_loader)
 
@@ -202,29 +220,31 @@ if __name__=='__main__':
     
     parser.add_argument('--data_dir', default="/home/lieberummaas/datadisk/minerl/data")
     parser.add_argument('--log_dir', default="/home/lieberummaas/datadisk/minerl/run_logs")
-    parser.add_argument('--env_name', default='MineRLTreechopVectorObf-v0')
-    parser.add_argument('--dynamics_model', default='mdn', choices=['rssm', 'mdn'], help='Model used to predict the next latent state')
-    parser.add_argument('--seq_len', default=4, type=int)
-    parser.add_argument('--batch_size', default=10, type=int)
-    parser.add_argument('--num_data', default=0, type=int, help='Number of datapoints to use')
-    parser.add_argument('--epochs', default=1, type=int)
+    parser.add_argument('--env_name', default='MineRLNavigateDenseVectorObf-v0')
+    
+    # training args
+    parser.add_argument('--seq_len', default=10, type=int)
+    parser.add_argument('--batch_size', default=100, type=int)
+    parser.add_argument('--use_whole_trajectories', action='store_true', help='Train on complete trajectories instead of subsequences -> batch size is forced to 1!')
+    parser.add_argument('--num_epochs', default=10, type=int)
     parser.add_argument('--save_freq', default=100, type=int)
     parser.add_argument('--lr', default=3e-4, type=float, help='Learning rate')
-    parser.add_argument('--lr_gamma', default=1, type=float, help='Learning rate adjustment factor')
-    parser.add_argument('--lr_step_mode', default='epoch', choices=['epoch', 'step'], type=str, help='Learning rate adjustment interval')
-    parser.add_argument('--lr_decrease_freq', default=1, type=int, help='Learning rate adjustment frequency')
-    parser.add_argument('--VAE_class', type=str, default='vae', choices=['vae', 'vqvae'])
-    parser.add_argument('--vae_version', type=int, default=0)
-    parser.add_argument('--num_components', type=int, default=5, help='Number of mixture components. Only used in MDN-RNN')
-    parser.add_argument('--val_check_interval', default=1, type=int, help='How often to validate. N == 1 --> once per epoch; N > 1 --> every N steps')
     parser.add_argument('--load_from_checkpoint', action='store_true')
-    #parser.add_argument('--latent_overshooting', action='store_true')
-    parser.add_argument('--profile', action='store_true')
-    parser.add_argument('--temp', default=1, type=float)
+    parser.add_argument('--checkpoint_path', default=None, type=str)
+    
+    # sequence learning args
+    # parser.add_argument('--latent_overshooting', action='store_true')
     parser.add_argument('--curriculum_threshold', default=3, type=float)
     parser.add_argument('--curriculum_start', default=0, type=int)
-    parser.add_argument('--conditioning_len', default=0, type=int, help='Length of sequence to condition rnn on')
-    parser.add_argument('--version', default=0, type=int, help='Version directory of model, if training is resumed from checkpoint')
+
+    # visual model args
+    parser.add_argument('--visual_model_cls', type=str, default='vae', choices=['vae', 'vqvae'])
+    parser.add_argument('--visual_model_path', type=str, required=True)
+    
+    # MDN-RNN args
+    parser.add_argument('--num_components', type=int, default=5, help='Number of mixture components. Only used in MDN-RNN')
+    parser.add_argument('--gru_hidden_size', type=int, default=512, help='Hidden size of the gru')
+    parser.add_argument('--use_one_hot', action='store_true', help='whether to use one-hot representation')
 
     args = vars(parser.parse_args())
 
diff --git a/research_code/train_VAE.py b/research_code/train_VAE.py
index bb93466..a6b429e 100755
--- a/research_code/train_VAE.py
+++ b/research_code/train_VAE.py
@@ -8,6 +8,8 @@ from torch.utils.data import DataLoader, random_split
 from torchvision.utils import make_grid
 import pytorch_lightning as pl
 from pytorch_lightning.callbacks import ModelCheckpoint
+from pytorch_lightning.loggers import WandbLogger
+import wandb
 import numpy as np
 import einops
 
@@ -28,7 +30,7 @@ class RampBeta(pl.Callback):
         # "We divide the overall loss by 256 × 256 × 3, so that the weight of the KL term
         # becomes β/192, where β is the KL weight."
         # TODO: OpenAI uses 6.6/192 but kinda tricky to do the conversion here... about 5e-4 works for this repo so far... :\
-        t = cos_anneal(0, 10000, 0.0, 5e-4, trainer.global_step)
+        t = cos_anneal(0, 15000, 0.0, 5e-4, trainer.global_step)
         pl_module.beta = t
 
 class DecayLR(pl.Callback):
@@ -94,7 +96,7 @@ class GenerateCallback(pl.Callback):
         images = torch.stack([self.img_batch, reconstructed_img], dim=1).reshape((self.batch_size * 2, *self.img_batch.shape[1:]))
 
         # log images to tensorboard
-        trainer.logger.experiment.add_image('Reconstruction',make_grid(images, nrow=2), epoch)
+        pl_module.logger.experiment.log({'Reconstruction': wandb.Image(make_grid(images, nrow=2))})
 
 
 def train_VAE(
@@ -104,7 +106,7 @@ def train_VAE(
     eval_freq, 
     save_freq, 
     batch_size,
-    epochs, 
+    num_epochs, 
     log_dir, 
     latent_dim, 
     n_hid,
@@ -113,8 +115,6 @@ def train_VAE(
     pl.seed_everything(1337)
 
     # make sure that relevant dirs exist
-    run_name = f'VAE/{env_name}'
-    log_dir = os.path.join(log_dir, run_name)
     os.makedirs(log_dir, exist_ok=True)
     print(f'Saving logs and model to {log_dir}')
 
@@ -130,23 +130,33 @@ def train_VAE(
     
     # init model
     model = VAE(encoder_kwargs, decoder_kwargs, lr)
-    
+    print(model.summarize(mode='top'))
+
     # load data
-    train_data = datasets.BufferedBatchDataset(env_name, data_dir, batch_size, epochs)
-    train_loader = DataLoader(train_data, batch_size=None, num_workers=1)
+    train_data = datasets.BufferedBatchDataset(env_name, data_dir, batch_size, 1)
+    train_loader = DataLoader(train_data, batch_size=None, num_workers=3)
 
     # create callbacks to sample reconstructed images and for model checkpointing
     img_callback =  GenerateCallback(dataset=train_data, save_to_disk=False)
     checkpoint_callback = ModelCheckpoint(mode="min", monitor="Training/loss", save_last=True, every_n_train_steps=save_freq)
-    callbacks = [img_callback, checkpoint_callback, DecayLR(), RampBeta()]
+    callbacks = [img_callback, checkpoint_callback, RampBeta()]
+    # callbacks.append(DecayLR())
+    # init logger
+    config = dict(
+        env_name=env_name,
+        latent_dim=latent_dim,
+        architecture='VAE'
+    )
+    wandb_logger = WandbLogger(project="VisualModel", config=config, tags=['VAE'])
     trainer=pl.Trainer(
+        logger=wandb_logger,
         progress_bar_refresh_rate=10, #every N batches update progress bar
         log_every_n_steps=10,
         callbacks=callbacks,
         gpus=torch.cuda.device_count(),
         #accelerator='ddp', #anything else here seems to lead to crashes/errors
         default_root_dir=log_dir,
-        max_epochs=epochs,
+        max_epochs=num_epochs,
     )
                     
     # fit model
@@ -159,13 +169,13 @@ if __name__=='__main__':
     parser.add_argument('--data_dir', default='/home/lieberummaas/datadisk/minerl/data')
     parser.add_argument('--log_dir', default='/home/lieberummaas/datadisk/minerl/run_logs')
     parser.add_argument('--env_name', default='MineRLNavigateDenseVectorObf-v0')
-    parser.add_argument('--batch_size', default=20, type=int)
-    parser.add_argument('--latent_dim', default=1024, type=int)
+    parser.add_argument('--batch_size', default=100, type=int)
+    parser.add_argument('--latent_dim', default=512, type=int)
     parser.add_argument('--n_hid', default=64, type=int)
     parser.add_argument('--n_init', default=64, type=int)
-    parser.add_argument('--epochs', default=1, type=int)
+    parser.add_argument('--num_epochs', default=10, type=int)
     parser.add_argument('--lr', default=3e-4, type=float, help='Learning rate')
-    parser.add_argument('--eval_freq', default=1, type=int, help='How often to reconstruct images for tensorboard')
+    parser.add_argument('--eval_freq', default=100, type=int, help='How often to reconstruct images for tensorboard')
     parser.add_argument('--save_freq', default=100, type=int, help='How often to save model')
 
     args = vars(parser.parse_args())
diff --git a/research_code/util_models.py b/research_code/util_models.py
deleted file mode 100644
index 695101d..0000000
--- a/research_code/util_models.py
+++ /dev/null
@@ -1,50 +0,0 @@
-import torch
-import torch.nn as nn
-import pytorch_lightning as pl
-import numpy as np
-
-class MergeFramesWithBatch(nn.Module):
-    '''
-    Transforms tensor of shape (N, T, ...) to tensor of shape (N * T, ...)
-    '''
-    def __init__(self):
-        super().__init__()
-    
-    def forward(self, input):
-        return input.reshape((input.shape[0] * input.shape[1], *input.shape[2:]))
-
-class SplitFramesFromBatch(nn.Module):
-    '''
-    Transforms tensor of shape (N * T, ...) to tensor of shape (N, T, ...)
-    '''
-    def __init__(self, num_frames):
-        super().__init__()
-        self.num_frames = num_frames
-    
-    def forward(self, input):
-        return input.reshape((-1, self.num_frames, *input.shape[1:]))
-
-class SplitChannelsFromClasses(nn.Module):
-    '''
-    Reshapes a (N, num_classes * num_channels, H, W) tensor
-    into a     (N, num_classes, num_channels, H, W) tensor
-    '''
-
-    def __init__(self, num_channels):
-        super().__init__()
-        self.num_channels = num_channels
-
-    def forward(self, input):
-        return input.reshape((input.shape[0], -1, self.num_channels, *input.shape[2:]))
-
-class Transpose(nn.Module):
-    '''
-    Transposes two dimensions of a tensor
-    '''
-    def __init__(self, dim1, dim2):
-        super().__init__()
-        self.dim1 = dim1
-        self.dim2 = dim2
-    
-    def forward(self, input):
-        return input.transpose(dim1, dim2)
diff --git a/research_code/vae_model.py b/research_code/vae_model.py
index a8d5aeb..bdc785a 100644
--- a/research_code/vae_model.py
+++ b/research_code/vae_model.py
@@ -7,6 +7,7 @@ import torch.nn.functional as F
 import einops
 from einops.layers.torch import Rearrange
 
+
 class VAE(pl.LightningModule):
     '''
     A base class for VAEs
@@ -40,7 +41,9 @@ class VAE(pl.LightningModule):
 
         # sample latent vector
         z = self.sample(mean, log_std)
+        
         return torch.clamp(0.5 + self.decoder(z), 0, 1)
+        #return self.decoder(z)
 
     @torch.no_grad()
     def encode_only(self, x):
@@ -64,19 +67,16 @@ class VAE(pl.LightningModule):
     def encode_with_grad(self, x):
         b, *_ = x.shape
         mean, log_std = self.encoder(x-0.5)
-        h = int((mean.shape[0]//b) ** 0.5)
         
         # compute KL distance, i.e. regularization loss
         L_regul = (0.5 * (torch.exp(2 * log_std) + mean ** 2 - 1 - 2 * log_std)).sum(dim=-1).mean()
 
-        mean = einops.rearrange(mean, '(b h w) c -> b c h w', b=b, h=h, w=h)
-        log_std = einops.rearrange(log_std, '(b h w) c -> b c h w', b=b, h=h, w=h)
-
         sample = self.sample(mean, log_std)
         return sample, L_regul, None, None
 
     @torch.no_grad()
     def decode_only(self, z):
+        #return self.decoder(z)
         return torch.clamp(0.5 + self.decoder(z), 0, 1)
 
     def forward(self, x):
@@ -94,6 +94,7 @@ class VAE(pl.LightningModule):
         
         # decode
         x_hat = torch.clamp(self.decoder(z) + 0.5, 0, 1)
+        #x_hat = self.decoder(z)
         
         # compute reconstruction loss, sum over all dimension except batch
         L_reconstr = (x - x_hat).pow(2).mean() / (2* 0.06327039811675479) # cifar-10 data variance, from deepmind sonnet code)
@@ -109,13 +110,13 @@ class VAE(pl.LightningModule):
         obs, *_ = batch
         obs = obs['pov'].float() / 255
         obs = einops.rearrange(obs, 'b h w c -> b c h w')
-        L_rec, L_reg = self(obs)
+        recon_loss, latent_loss = self(obs)
 
-        loss = L_rec + self.beta * L_reg
+        loss = recon_loss + self.beta * latent_loss
 
         self.log('Training/loss', loss, on_step=True)
-        self.log('Training/recon_loss', L_rec, on_step=True)
-        self.log('Training/latent_loss', L_reg, on_step=True)
+        self.log('Training/recon_loss', recon_loss, on_step=True)
+        self.log('Training/latent_loss', latent_loss, on_step=True)
 
         return loss
 
@@ -142,6 +143,19 @@ class VAEEncoder(nn.Module):
     def __init__(self, input_channels=3, n_hid=64, latent_dim=64):
         super().__init__()
 
+        self.net = nn.Sequential(
+            nn.Conv2d(3, 32, 4, 2),
+            nn.ReLU(),
+            nn.Conv2d(32, 64, 4, 2),
+            nn.ReLU(),
+            nn.Conv2d(64, 128, 4, 2),
+            nn.ReLU(),
+            nn.Conv2d(128, 256, 4, 2),
+            nn.ReLU(),
+            Rearrange('b c h w -> b (c h w)'),
+            nn.Linear(1024, 2*latent_dim)
+        )
+        '''
         self.net = nn.Sequential(
             nn.Conv2d(input_channels, n_hid, 4, stride=2, padding=1),
             nn.ReLU(inplace=True),
@@ -158,7 +172,7 @@ class VAEEncoder(nn.Module):
             nn.ReLU(),
             Rearrange('b c h w -> b (c h w)'),
             nn.Linear(2*n_hid*16, 2*latent_dim)
-        )
+        )'''
 
     def forward(self, x):
         #out = self.net(x)
@@ -177,7 +191,7 @@ class VAEDecoder(nn.Module):
     def __init__(self, latent_dim=64, n_init=64, n_hid=64, output_channels=3):
         super().__init__()
 
-        
+        '''
         self.net = nn.Sequential(
             Rearrange('b (h w c) -> b c h w', w=4, h=4),
             nn.Conv2d(64, n_init, 3, padding=1),
@@ -196,20 +210,18 @@ class VAEDecoder(nn.Module):
         )
         '''
         self.net = nn.Sequential(
-            nn.Linear(latent_dim, 256*4),
+            nn.Linear(latent_dim, 1024),
+            Rearrange('b d -> b d 1 1'),
+            nn.ConvTranspose2d(1024, 128, 5, 2),
             nn.ReLU(),
-            Rearrange('b (c h w) -> b c h w', h=2, w=2),
-            nn.ConvTranspose2d(256, 128, 4, 2, 1),
+            nn.ConvTranspose2d(128, 64, 5, 2),
             nn.ReLU(),
-            nn.ConvTranspose2d(128, 64, 4, 2, 1),
+            nn.ConvTranspose2d(64, 32, 6, 2),
             nn.ReLU(),
-            nn.ConvTranspose2d(64, 64, 4, 2, 1),
-            nn.ReLU(),
-            nn.ConvTranspose2d(64, 64, 4, 2, 1),
-            nn.ReLU(),
-            nn.ConvTranspose2d(64, 3, 4, 2, 1),
+            nn.ConvTranspose2d(32, 3, 6, 2),
+            #nn.Sigmoid(),
         )
-        '''
+        
 
     def forward(self, x):
         #print('\nDecoder:')
diff --git a/research_code/vqvae.py b/research_code/vqvae.py
index 88e9db8..b419d77 100644
--- a/research_code/vqvae.py
+++ b/research_code/vqvae.py
@@ -9,6 +9,7 @@ from argparse import ArgumentParser, Namespace
 
 import numpy as np
 import einops
+from einops.layers.torch import Rearrange
 
 from torchvision.utils import make_grid
 
@@ -19,15 +20,106 @@ from torch.utils.data import DataLoader, random_split
 
 import pytorch_lightning as pl
 from pytorch_lightning.callbacks import ModelCheckpoint
-
-from vqvae_model.deepmind_enc_dec import DeepMindEncoder, DeepMindDecoder
-from vqvae_model.openai_enc_dec import OpenAIEncoder, OpenAIDecoder
-from vqvae_model.openai_enc_dec import Conv2d as PatchedConv2d
-from vqvae_model.quantize import VQVAEQuantize, GumbelQuantize
-from vqvae_model.loss import Normal, LogitLaplace
+from pytorch_lightning.loggers import WandbLogger
+import wandb
 
 import datasets
 
+class SeparateQuantizer(nn.Module):
+    """
+    Gumbel Softmax trick quantizer
+    Categorical Reparameterization with Gumbel-Softmax, Jang et al. 2016
+    https://arxiv.org/abs/1611.01144
+    """
+    def __init__(self, num_variables, codebook_size, embedding_dim, straight_through=False):
+        super().__init__()
+
+        self.embedding_dim = embedding_dim
+        self.codebook_size = codebook_size
+        self.num_variables = num_variables
+
+        self.straight_through = straight_through
+        self.temperature = 1.0
+        self.kld_scale = 5e-4
+
+        self.embeds = nn.ModuleList([nn.Embedding(codebook_size, embedding_dim) for _ in range(self.num_variables)])
+
+    def forward(self, logits):
+        # force hard = True when we are in eval mode, as we must quantize
+        hard = self.straight_through if self.training else True
+
+        logits = einops.rearrange(logits, 'b (num_variables codebook_size) -> b num_variables codebook_size', codebook_size=self.codebook_size, num_variables=self.num_variables)
+
+        soft_one_hot = F.gumbel_softmax(logits, tau=self.temperature, dim=2, hard=hard)
+        z_q = torch.stack([soft_one_hot[:,i,:] @ self.embeds[i].weight for i in range(self.num_variables)], dim=1) # (b num_vars embed_dim)
+
+        # + kl divergence to the prior loss
+        qy = F.softmax(logits, dim=2)
+        diff = self.kld_scale * torch.sum(qy * torch.log(qy * self.codebook_size + 1e-10), dim=2).mean()
+
+        ind = soft_one_hot.argmax(dim=1)
+        return z_q, diff, ind, logits
+
+    def embed_one_hot(self, embed_vec):
+        '''
+        embed vec is of shape (B * T * H * W, n_embed)
+        '''
+        raise NotImplementedError
+    
+    def embed_code(self, embed_id):
+        raise NotImplementedError
+    
+    def forward_one_hot(self, logits):
+        logits = einops.rearrange(logits, 'b (num_variables codebook_size) -> b num_variables codebook_size', codebook_size=self.codebook_size, num_variables=self.num_variables)
+
+        probs = torch.softmax(logits, dim=2)
+        one_hot = F.gumbel_softmax(logits, tau=self.temperature, dim=2, hard=True)
+        return one_hot, probs
+
+
+class SmallEncoder(nn.Module):
+
+    def __init__(self, input_channels=3, num_vars=32, latent_dim=32, codebook_size=32):
+        super().__init__()
+
+        self.net = nn.Sequential(
+            nn.Conv2d(3, 32, 4, 2),
+            nn.ReLU(),
+            nn.Conv2d(32, 64, 4, 2),
+            nn.ReLU(),
+            nn.Conv2d(64, 128, 4, 2),
+            nn.ReLU(),
+            nn.Conv2d(128, 256, 4, 2),
+            nn.ReLU(),
+            Rearrange('b c h w -> b (c h w)'),
+            nn.Linear(1024, num_vars*codebook_size)
+        )
+
+    def forward(self, x):
+        out = self.net(x)
+        return out
+
+class SmallDecoder(nn.Module):
+
+    def __init__(self, latent_dim=32, num_vars=32, n_init=64, n_hid=64, output_channels=3):
+        super().__init__()
+
+        self.net = nn.Sequential(
+            Rearrange('b n d -> b (n d)'),
+            nn.Linear(latent_dim*num_vars, 1024),
+            Rearrange('b d -> b d 1 1'),
+            nn.ConvTranspose2d(1024, 128, 5, 2),
+            nn.ReLU(),
+            nn.ConvTranspose2d(128, 64, 5, 2),
+            nn.ReLU(),
+            nn.ConvTranspose2d(64, 32, 6, 2),
+            nn.ReLU(),
+            nn.ConvTranspose2d(32, 3, 6, 2),
+        )
+        
+    def forward(self, x):
+        return self.net(x)
+
 # -----------------------------------------------------------------------------
 
 class VQVAE(pl.LightningModule):
@@ -37,62 +129,59 @@ class VQVAE(pl.LightningModule):
         self.save_hyperparameters()
 
         # encoder/decoder module pair
-        Encoder, Decoder = {
-            'deepmind': (DeepMindEncoder, DeepMindDecoder),
-            'openai': (OpenAIEncoder, OpenAIDecoder),
-        }[args.enc_dec_flavor]
-        self.encoder = Encoder(input_channels=input_channels, n_hid=args.n_hid)
-        self.decoder = Decoder(n_init=args.embedding_dim, n_hid=args.n_hid, output_channels=input_channels)
+        # self.encoder = DeepMindEncoder(input_channels=input_channels, n_hid=args.n_hid)
+        # self.decoder = DeepMindDecoder(n_init=args.embedding_dim, n_hid=args.n_hid, output_channels=input_channels)
 
         # the quantizer module sandwiched between them, +contributes a KL(posterior || prior) loss to ELBO
-        QuantizerModule = {
-            'vqvae': VQVAEQuantize,
-            'gumbel': GumbelQuantize,
-        }[args.vq_flavor]
-        self.quantizer = QuantizerModule(self.encoder.output_channels, args.num_embeddings, args.embedding_dim)
-
-        # the data reconstruction loss in the ELBO
-        ReconLoss = {
-            'l2': Normal,
-            'logit_laplace': LogitLaplace,
-            # todo: add vqgan
-        }[args.loss_flavor]
-        self.recon_loss = ReconLoss
+        # QuantizerModule = {
+        #     'vqvae': VQVAEQuantize,
+        #     'gumbel': GumbelQuantize,
+        # }[args.vq_flavor]
+        # self.quantizer = QuantizerModule(self.encoder.output_channels, args.num_embeddings, args.embedding_dim)
+        self.encoder = SmallEncoder(input_channels=3, latent_dim=args.embedding_dim, codebook_size=args.num_embeddings, num_vars=args.num_variables)
+        self.decoder = SmallDecoder(latent_dim=args.embedding_dim, num_vars=args.num_variables)
+        self.quantizer = SeparateQuantizer(num_variables=args.num_variables, codebook_size=args.num_embeddings, embedding_dim=args.embedding_dim)
+
 
     def forward(self, x):
-        z = self.encoder(self.recon_loss.inmap(x))
+        z = self.encoder(x-0.5)
         z_q, latent_loss, ind, _ = self.quantizer(z)
-        x_hat = self.recon_loss.unmap(self.decoder(z_q))
+        x_hat = torch.clamp(self.decoder(z_q)+0.5, 0, 1)
         return x_hat, latent_loss, ind
 
     
     @torch.no_grad()
     def reconstruct_only(self, x):
-        z = self.encoder(self.recon_loss.inmap(x))
+        z = self.encoder(x-0.5)
         z_q, *_ = self.quantizer(z)
-        x_hat = self.decoder(z_q)
-        x_hat = self.recon_loss.unmap(x_hat)
+        x_hat = torch.clamp(self.decoder(z_q)+0.5, 0, 1)
+
         return x_hat
     
     @torch.no_grad()
     def decode_only(self, z_q):
-        x_hat = self.decoder(z_q)
-        x_hat = self.recon_loss.unmap(x_hat)
+        x_hat = torch.clamp(self.decoder(z_q)+0.5, 0, 1)
         return x_hat
     
     def decode_with_grad(self, z_q):
-        x_hat = self.decoder(z_q)
-        x_hat = self.recon_loss.unmap(x_hat)
+        x_hat = torch.clamp(self.decoder(z_q)+0.5, 0, 1)
         return x_hat
 
     @torch.no_grad()
     def encode_only(self, x):
-        z = self.encoder(self.recon_loss.inmap(x))
+        z = self.encoder(x-0.5)
         z_q, _, ind, neg_dist = self.quantizer(z)
         return z_q, ind, neg_dist
     
+    @torch.no_grad()
+    def encode_only_one_hot(self, x):
+        z = self.encoder(x-0.5)
+        
+        one_hot, probs = self.quantizer.forward_one_hot(z)
+        return one_hot, probs    
+
     def encode_with_grad(self, x):
-        z = self.encoder(self.recon_loss.inmap(x))
+        z = self.encoder(x-0.5)
         z_q, diff, ind, neg_dist = self.quantizer(z)
         return z_q, diff, ind, neg_dist
     
@@ -107,7 +196,7 @@ class VQVAE(pl.LightningModule):
         img_hat, latent_loss, ind = self.forward(img)
         
         # compute reconstruction loss
-        recon_loss = self.recon_loss.nll(img, img_hat)
+        recon_loss = ((img - img_hat)**2).mean() / (2 * 0.06327039811675479)
         
         # loss = reconstruction_loss + codebook loss from quantizer
         loss = recon_loss + latent_loss
@@ -139,7 +228,7 @@ class VQVAE(pl.LightningModule):
         # separate out all parameters to those that will and won't experience regularizing weight decay
         decay = set()
         no_decay = set()
-        whitelist_weight_modules = (torch.nn.Linear, torch.nn.Conv2d, torch.nn.ConvTranspose2d, PatchedConv2d)
+        whitelist_weight_modules = (torch.nn.Linear, torch.nn.Conv2d, torch.nn.ConvTranspose2d)
         blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.BatchNorm2d, torch.nn.Embedding)
         for mn, m in self.named_modules():
             for pn, p in m.named_parameters():
@@ -253,7 +342,7 @@ class GenerateCallback(pl.Callback):
         images = torch.stack([self.img_batch, reconstructed_img], dim=1).reshape((self.batch_size * 2, *self.img_batch.shape[1:]))
 
         # log images to tensorboard
-        trainer.logger.experiment.add_image('Reconstruction',make_grid(images, nrow=2), epoch)
+        pl_module.logger.experiment.log({'Reconstruction': wandb.Image(make_grid(images, nrow=2))})
 
 
 class VisualizeLatents(pl.Callback):
@@ -290,7 +379,7 @@ class VisualizeLatents(pl.Callback):
         images = torch.stack(images, dim=0)
 
         # log images to tensorboard
-        trainer.logger.experiment.add_image('Latents',make_grid(images, nrow=4), epoch)
+        pl_module.logger.experiment.log({'Latents': wandb.Image(make_grid(images, nrow=2))})
 
 
 def cli_main():
@@ -301,23 +390,22 @@ def cli_main():
     parser = ArgumentParser()
     # training related
     parser = pl.Trainer.add_argparse_args(parser)
+    parser.add_argument('--num_epochs', type=int, default=10)
     # model type
-    parser.add_argument("--vq_flavor", type=str, default='gumbel', choices=['vqvae', 'gumbel'])
-    parser.add_argument("--enc_dec_flavor", type=str, default='deepmind', choices=['deepmind', 'openai'])
-    parser.add_argument("--loss_flavor", type=str, default='l2', choices=['l2', 'logit_laplace'])
     parser.add_argument('--callback_batch_size', type=int, default=6, help='How many images to reconstruct for callback (shown in tensorboard/images)')
     parser.add_argument('--callback_freq', type=int, default=100, help='How often to reconstruct for callback (shown in tensorboard/images)')
     parser.add_argument('--save_freq', type=int, default=500, help='Save the model every N training steps')
     parser.add_argument('--log_freq', type=int, default=10)
     parser.add_argument('--progbar_rate', type=int, default=10)
     # model size
-    parser.add_argument("--num_embeddings", type=int, default=256, help="vocabulary size; number of possible discrete states")
+    parser.add_argument("--num_embeddings", type=int, default=32, help="vocabulary size; number of possible discrete states")
     parser.add_argument("--embedding_dim", type=int, default=32, help="size of the vector of the embedding of each discrete token")
+    parser.add_argument("--num_variables", type=int, default=32, help="size of the vector of the embedding of each discrete token")
     parser.add_argument("--n_hid", type=int, default=64, help="number of channels controlling the size of the model")
     # dataloader related
     parser.add_argument("--data_dir", type=str, default='/home/lieberummaas/datadisk/minerl/data')
     parser.add_argument("--env_name", type=str, default='MineRLNavigateDenseVectorObf-v0')
-    parser.add_argument("--batch_size", type=int, default=20)
+    parser.add_argument("--batch_size", type=int, default=100)
     #other args
     parser.add_argument('--log_dir', type=str, default='/home/lieberummaas/datadisk/minerl/run_logs')
     parser.add_argument('--suffix', type=str, default='')
@@ -326,28 +414,23 @@ def cli_main():
     # -------------------------------------------------------------------------
 
     # make sure that relevant dirs exist
-    run_name = f'VQVAE/{args.env_name}'
-    if args.suffix != '':
-        run_name = run_name + '/' + args.suffix
-        
-    log_dir = os.path.join(args.log_dir, run_name)
     os.makedirs(args.log_dir, exist_ok=True)
-    print(f'\nSaving logs and model to {log_dir}')
+    print(f'\nSaving logs and model to {args.log_dir}')
 
     # init model
     vqvae_args = Namespace(**{
-            'vq_flavor':args.vq_flavor, 
-            'enc_dec_flavor':args.enc_dec_flavor, 
             'embedding_dim':args.embedding_dim, 
+            'num_variables':args.num_variables,
             'n_hid':args.n_hid, 
             'num_embeddings':args.num_embeddings,
             'loss_flavor':args.loss_flavor
         })
     model = VQVAE(args = vqvae_args)
+    print(model.summarize(mode='top'))
 
     # load data
     data = datasets.BufferedBatchDataset(args.env_name, args.data_dir, args.batch_size, num_epochs=1)
-    dataloader = DataLoader(data, batch_size=None, num_workers=1)
+    dataloader = DataLoader(data, batch_size=None, num_workers=3)
 
     # annealing schedules for lots of constants
     callbacks = []
@@ -361,20 +444,30 @@ def cli_main():
             every_n_batches=args.callback_freq
         )
     )
-    callbacks.append(
-        VisualizeLatents(every_n_batches=args.callback_freq)
-    )
-    callbacks.append(DecayLR())
+    # callbacks.append(
+    #     VisualizeLatents(every_n_batches=args.callback_freq)
+    # )
+    #callbacks.append(DecayLR())
     if args.vq_flavor == 'gumbel':
        callbacks.extend([DecayTemperature(), RampBeta()])
     
+    # init logger
+    config = dict(
+        env_name=args.env_name,
+        num_variables=args.num_variables,
+        codebook_size=args.num_embeddings,
+        embedding_dim=args.embedding_dim,
+        architecture='VQVAE'
+    )
+    wandb_logger = WandbLogger(project="VisualModel", config=config, tags=['VQVAE'])
+
     # create trainer instance
     trainer = pl.Trainer(
+        logger=wandb_logger,
         callbacks=callbacks, 
-        default_root_dir=log_dir, 
+        default_root_dir=args.log_dir, 
         gpus=torch.cuda.device_count(),
-        max_epochs=1,
-        accelerator='dp',
+        max_epochs=args.num_epochs,
         log_every_n_steps=args.log_freq,
         progress_bar_refresh_rate=args.progbar_rate
     )
diff --git a/research_code/vqvae_model/__init__.py b/research_code/vqvae_model/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/research_code/vqvae_model/deepmind_enc_dec.py b/research_code/vqvae_model/deepmind_enc_dec.py
deleted file mode 100644
index 60af3e1..0000000
--- a/research_code/vqvae_model/deepmind_enc_dec.py
+++ /dev/null
@@ -1,68 +0,0 @@
-"""
-Patch Encoders / Decoders as used by DeepMind in their sonnet repo example:
-https://github.com/deepmind/sonnet/blob/v2/examples/vqvae_example.ipynb
-"""
-
-import torch
-from torch import nn, einsum
-import torch.nn.functional as F
-
-# -----------------------------------------------------------------------------
-
-class ResBlock(nn.Module):
-    def __init__(self, input_channels, channel):
-        super().__init__()
-
-        self.conv = nn.Sequential(
-            nn.Conv2d(input_channels, channel, 3, padding=1),
-            nn.ReLU(inplace=True),
-            nn.Conv2d(channel, input_channels, 1),
-        )
-
-    def forward(self, x):
-        out = self.conv(x)
-        out += x
-        out = F.relu(out)
-        return out
-
-
-class DeepMindEncoder(nn.Module):
-
-    def __init__(self, input_channels=3, n_hid=64):
-        super().__init__()
-
-        self.net = nn.Sequential(
-            nn.Conv2d(input_channels, n_hid, 4, stride=2, padding=1),
-            nn.ReLU(inplace=True),
-            nn.Conv2d(n_hid, 2*n_hid, 4, stride=2, padding=1),
-            nn.ReLU(inplace=True),
-            nn.Conv2d(2*n_hid, 2*n_hid, 3, padding=1),
-            nn.ReLU(),
-            ResBlock(2*n_hid, 2*n_hid//4),
-            ResBlock(2*n_hid, 2*n_hid//4),
-        )
-
-        self.output_channels = 2 * n_hid
-        self.output_stide = 4
-
-    def forward(self, x):
-        return self.net(x)
-
-
-class DeepMindDecoder(nn.Module):
-
-    def __init__(self, n_init=32, n_hid=64, output_channels=3):
-        super().__init__()
-
-        self.net = nn.Sequential(
-            nn.Conv2d(n_init, 2*n_hid, 3, padding=1),
-            nn.ReLU(),
-            ResBlock(2*n_hid, 2*n_hid//4),
-            ResBlock(2*n_hid, 2*n_hid//4),
-            nn.ConvTranspose2d(2*n_hid, n_hid, 4, stride=2, padding=1),
-            nn.ReLU(inplace=True),
-            nn.ConvTranspose2d(n_hid, output_channels, 4, stride=2, padding=1),
-        )
-
-    def forward(self, x):
-        return self.net(x)
diff --git a/research_code/vqvae_model/loss.py b/research_code/vqvae_model/loss.py
deleted file mode 100644
index 2c63047..0000000
--- a/research_code/vqvae_model/loss.py
+++ /dev/null
@@ -1,48 +0,0 @@
-"""
-VQVAE losses, used for the reconstruction term in the ELBO
-"""
-
-import math
-import torch
-
-# -----------------------------------------------------------------------------
-
-class LogitLaplace:
-    """ the Logit Laplace distribution log likelihood from OpenAI's DALL-E paper """
-    logit_laplace_eps = 0.1
-
-    @classmethod
-    def inmap(cls, x):
-        # map [0,1] range to [eps, 1-eps]
-        return (1 - 2 * cls.logit_laplace_eps) * x + cls.logit_laplace_eps
-
-    @classmethod
-    def unmap(cls, x):
-        # inverse map, from [eps, 1-eps] to [0,1], with clamping
-        return torch.clamp((x - cls.logit_laplace_eps) / (1 - 2 * cls.logit_laplace_eps), 0, 1)
-
-    @classmethod
-    def nll(cls, x, mu_logb):
-        raise NotImplementedError # coming right up
-
-
-class Normal:
-    """
-    simple normal distribution with fixed variance, as used by DeepMind in their VQVAE
-    note that DeepMind's reconstruction loss (I think incorrectly?) misses a factor of 2,
-    which I have added to the normalizer of the reconstruction loss in nll(), we'll report
-    number that is half of what we expect in their jupyter notebook
-    """
-    data_variance = 0.06327039811675479 # cifar-10 data variance, from deepmind sonnet code
-
-    @classmethod
-    def inmap(cls, x):
-        return x - 0.5 # map [0,1] range to [-0.5, 0.5]
-
-    @classmethod
-    def unmap(cls, x):
-        return torch.clamp(x + 0.5, 0, 1)
-
-    @classmethod
-    def nll(cls, x, mu):
-        return ((x - mu)**2).mean() / (2 * cls.data_variance)
diff --git a/research_code/vqvae_model/openai_enc_dec.py b/research_code/vqvae_model/openai_enc_dec.py
deleted file mode 100644
index 6827b2f..0000000
--- a/research_code/vqvae_model/openai_enc_dec.py
+++ /dev/null
@@ -1,190 +0,0 @@
-"""
-OpenAI DALL-E Encoder/Decoder, taken and modified from their official repo @
-https://github.com/openai/DALL-E
-
-- Removed first/last 1x1 convs because in this repo those are part of the Quantize layers. This
-  is done so that VQVAE and GumbelSoftmax can be viewed side by side cleaner and more symmetrically.
-- Got rid of some of the fp16 / device / requires_grad settings, we're going to keep things simple
-"""
-
-import attr
-import math
-from collections import OrderedDict
-from functools import partial
-
-import numpy as np
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-# -----------------------------------------------------------------------------
-
-@attr.s(eq=False)
-class Conv2d(nn.Module): # TODO: simplify to standard PyTorch Conv2d
-    n_in:  int = attr.ib(validator=lambda i, a, x: x >= 1)
-    n_out: int = attr.ib(validator=lambda i, a, x: x >= 1)
-    kw:    int = attr.ib(validator=lambda i, a, x: x >= 1 and x % 2 == 1)
-
-    def __attrs_post_init__(self) -> None:
-        super().__init__()
-
-        w = torch.empty((self.n_out, self.n_in, self.kw, self.kw), dtype=torch.float32)
-        w.data.normal_(std=1/math.sqrt(self.n_in * self.kw ** 2))
-
-        b = torch.zeros((self.n_out,), dtype=torch.float32)
-
-        self.weight, self.bias = nn.Parameter(w), nn.Parameter(b)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        return F.conv2d(x, self.weight, self.bias, padding=(self.kw - 1) // 2)
-
-
-@attr.s(eq=False, repr=False)
-class EncoderBlock(nn.Module):
-    n_in:     int = attr.ib(validator=lambda i, a, x: x >= 1)
-    n_out:    int = attr.ib(validator=lambda i, a, x: x >= 1 and x % 4 ==0)
-    n_layers: int = attr.ib(validator=lambda i, a, x: x >= 1)
-
-    def __attrs_post_init__(self) -> None:
-        super().__init__()
-        self.n_hid = self.n_out // 4
-        self.post_gain = 1 / (self.n_layers ** 2)
-
-        make_conv     = partial(Conv2d)
-        self.id_path  = make_conv(self.n_in, self.n_out, 1) if self.n_in != self.n_out else nn.Identity()
-        self.res_path = nn.Sequential(OrderedDict([
-                ('relu_1', nn.ReLU()),
-                ('conv_1', make_conv(self.n_in,  self.n_hid, 3)),
-                ('relu_2', nn.ReLU()),
-                ('conv_2', make_conv(self.n_hid, self.n_hid, 3)),
-                ('relu_3', nn.ReLU()),
-                ('conv_3', make_conv(self.n_hid, self.n_hid, 3)),
-                ('relu_4', nn.ReLU()),
-                ('conv_4', make_conv(self.n_hid, self.n_out, 1)),]))
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        return self.id_path(x) + self.post_gain * self.res_path(x)
-
-@attr.s(eq=False, repr=False)
-class OpenAIEncoder(nn.Module):
-    input_channels:  int = attr.ib(default=3,    validator=lambda i, a, x: x >= 1)
-    n_hid:           int = attr.ib(default=256,  validator=lambda i, a, x: x >= 64)
-    n_blk_per_group: int = attr.ib(default=2,    validator=lambda i, a, x: x >= 1)
-
-    def __attrs_post_init__(self) -> None:
-        super().__init__()
-
-        group_count = 4
-        blk_range  = range(self.n_blk_per_group)
-        n_layers   = group_count * self.n_blk_per_group
-        make_conv  = partial(Conv2d)
-        make_blk   = partial(EncoderBlock, n_layers=n_layers)
-
-        self.blocks = nn.Sequential(OrderedDict([
-            ('input', make_conv(self.input_channels, 1 * self.n_hid, 7)),
-            ('group_1', nn.Sequential(OrderedDict([
-                *[(f'block_{i + 1}', make_blk(1 * self.n_hid, 1 * self.n_hid)) for i in blk_range],
-                ('pool', nn.MaxPool2d(kernel_size=2)),
-            ]))),
-            ('group_2', nn.Sequential(OrderedDict([
-                *[(f'block_{i + 1}', make_blk(1 * self.n_hid if i == 0 else 2 * self.n_hid, 2 * self.n_hid)) for i in blk_range],
-                ('pool', nn.MaxPool2d(kernel_size=2)),
-            ]))),
-            ('group_3', nn.Sequential(OrderedDict([
-                *[(f'block_{i + 1}', make_blk(2 * self.n_hid if i == 0 else 4 * self.n_hid, 4 * self.n_hid)) for i in blk_range],
-                ('pool', nn.MaxPool2d(kernel_size=2)),
-            ]))),
-            ('group_4', nn.Sequential(OrderedDict([
-                *[(f'block_{i + 1}', make_blk(4 * self.n_hid if i == 0 else 8 * self.n_hid, 8 * self.n_hid)) for i in blk_range],
-            ]))),
-            ('output', nn.Sequential(OrderedDict([
-                ('relu', nn.ReLU()),
-            ]))),
-        ]))
-
-        self.output_channels = 8 * self.n_hid
-        self.output_stide = 8
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        if len(x.shape) != 4:
-            raise ValueError(f'input shape {x.shape} is not 4d')
-        if x.shape[1] != self.input_channels:
-            raise ValueError(f'input has {x.shape[1]} channels but model built for {self.input_channels}')
-        if x.dtype != torch.float32:
-            raise ValueError('input must have dtype torch.float32')
-
-        return self.blocks(x)
-
-
-@attr.s(eq=False, repr=False)
-class DecoderBlock(nn.Module):
-    n_in:     int = attr.ib(validator=lambda i, a, x: x >= 1)
-    n_out:    int = attr.ib(validator=lambda i, a, x: x >= 1 and x % 4 ==0)
-    n_layers: int = attr.ib(validator=lambda i, a, x: x >= 1)
-
-    def __attrs_post_init__(self) -> None:
-        super().__init__()
-        self.n_hid = self.n_out // 4
-        self.post_gain = 1 / (self.n_layers ** 2)
-
-        make_conv     = partial(Conv2d)
-        self.id_path  = make_conv(self.n_in, self.n_out, 1) if self.n_in != self.n_out else nn.Identity()
-        self.res_path = nn.Sequential(OrderedDict([
-                ('relu_1', nn.ReLU()),
-                ('conv_1', make_conv(self.n_in,  self.n_hid, 1)),
-                ('relu_2', nn.ReLU()),
-                ('conv_2', make_conv(self.n_hid, self.n_hid, 3)),
-                ('relu_3', nn.ReLU()),
-                ('conv_3', make_conv(self.n_hid, self.n_hid, 3)),
-                ('relu_4', nn.ReLU()),
-                ('conv_4', make_conv(self.n_hid, self.n_out, 3)),]))
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        return self.id_path(x) + self.post_gain * self.res_path(x)
-
-@attr.s(eq=False, repr=False)
-class OpenAIDecoder(nn.Module):
-    n_init:          int = attr.ib(default=128,  validator=lambda i, a, x: x >= 8)
-    n_hid:           int = attr.ib(default=256,  validator=lambda i, a, x: x >= 64)
-    output_channels: int = attr.ib(default=3,    validator=lambda i, a, x: x >= 1)
-    n_blk_per_group: int = attr.ib(default=2,    validator=lambda i, a, x: x >= 1)
-
-    def __attrs_post_init__(self) -> None:
-        super().__init__()
-
-        group_count = 4
-        blk_range  = range(self.n_blk_per_group)
-        n_layers   = group_count * self.n_blk_per_group
-        make_conv  = partial(Conv2d)
-        make_blk   = partial(DecoderBlock, n_layers=n_layers)
-
-        self.blocks = nn.Sequential(OrderedDict([
-            ('group_1', nn.Sequential(OrderedDict([
-                *[(f'block_{i + 1}', make_blk(self.n_init if i == 0 else 8 * self.n_hid, 8 * self.n_hid)) for i in blk_range],
-                ('upsample', nn.Upsample(scale_factor=2, mode='nearest')),
-            ]))),
-            ('group_2', nn.Sequential(OrderedDict([
-                *[(f'block_{i + 1}', make_blk(8 * self.n_hid if i == 0 else 4 * self.n_hid, 4 * self.n_hid)) for i in blk_range],
-                ('upsample', nn.Upsample(scale_factor=2, mode='nearest')),
-            ]))),
-            ('group_3', nn.Sequential(OrderedDict([
-                *[(f'block_{i + 1}', make_blk(4 * self.n_hid if i == 0 else 2 * self.n_hid, 2 * self.n_hid)) for i in blk_range],
-                ('upsample', nn.Upsample(scale_factor=2, mode='nearest')),
-            ]))),
-            ('group_4', nn.Sequential(OrderedDict([
-                *[(f'block_{i + 1}', make_blk(2 * self.n_hid if i == 0 else 1 * self.n_hid, 1 * self.n_hid)) for i in blk_range],
-            ]))),
-            ('output', nn.Sequential(OrderedDict([
-                ('relu', nn.ReLU()),
-                ('conv', make_conv(1 * self.n_hid, self.output_channels, 1)),
-            ]))),
-        ]))
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        if len(x.shape) != 4:
-            raise ValueError(f'input shape {x.shape} is not 4d')
-        if x.dtype != torch.float32:
-            raise ValueError('input must have dtype torch.float32')
-
-        return self.blocks(x)
\ No newline at end of file
diff --git a/research_code/vqvae_model/quantize.py b/research_code/vqvae_model/quantize.py
deleted file mode 100644
index 09f8568..0000000
--- a/research_code/vqvae_model/quantize.py
+++ /dev/null
@@ -1,140 +0,0 @@
-"""
-The critical quantization layers that we sandwich in the middle of the autoencoder
-(between the encoder and decoder) that force the representation through a categorical
-variable bottleneck and use various tricks (softening / straight-through estimators)
-to backpropagate through the sampling process.
-"""
-
-import torch
-from torch import nn, einsum
-import torch.nn.functional as F
-import einops
-import numpy as np
-from scipy.cluster.vq import kmeans2
-
-# -----------------------------------------------------------------------------
-
-
-class VQVAEQuantize(nn.Module):
-    """
-    Neural Discrete Representation Learning, van den Oord et al. 2017
-    https://arxiv.org/abs/1711.00937
-
-    Follows the original DeepMind implementation
-    https://github.com/deepmind/sonnet/blob/v2/sonnet/src/nets/vqvae.py
-    https://github.com/deepmind/sonnet/blob/v2/examples/vqvae_example.ipynb
-    """
-    def __init__(self, num_hiddens, n_embed, embedding_dim):
-        super().__init__()
-
-        self.embedding_dim = embedding_dim
-        self.n_embed = n_embed
-
-        self.kld_scale = 10.0
-
-        self.proj = nn.Conv2d(num_hiddens, embedding_dim, 1)
-        self.embed = nn.Embedding(n_embed, embedding_dim)
-        #print(self.embed.weight.shape) # n_embed x embedding_dim
-
-        self.register_buffer('data_initialized', torch.zeros(1))
-
-    def forward(self, z, proj=True):
-        B, C, H, W = z.size()
-
-        # project and flatten out space, so (B, C, H, W) -> (B*H*W, C)
-        if proj:
-            z_e = self.proj(z)
-        else:
-            z_e = z
-        z_e = z_e.permute(0, 2, 3, 1) # make (B, H, W, C)
-        flatten = z_e.reshape(-1, self.embedding_dim)
-
-        # DeepMind def does not do this but I find I have to... ;\
-        if self.training and self.data_initialized.item() == 0:
-            print('running kmeans!!') # data driven initialization for the embeddings
-            rp = torch.randperm(flatten.size(0))
-            kd = kmeans2(flatten[rp[:20000]].data.cpu().numpy(), self.n_embed, minit='points')
-            self.embed.weight.data.copy_(torch.from_numpy(kd[0]))
-            self.data_initialized.fill_(1)
-            # TODO: this won't work in multi-GPU setups
-
-        dist = self.get_dist(flatten)
-        _, ind = (-dist).max(1)
-        #print(np.unique(ind.cpu().numpy()))
-        ind = einops.rearrange(ind, '(B H W) -> B H W', B=B, H=H, W=W)
-        neg_dist = einops.rearrange((-dist), '(B H W) D -> B D H W', B=B, H=H, W=W)
-
-        # vector quantization cost that trains the embedding vectors
-        z_q = self.embed_code(ind) # (B, H, W, C)
-        commitment_cost = 0.25
-        diff = commitment_cost * (z_q.detach() - z_e).pow(2).mean() + (z_q - z_e.detach()).pow(2).mean()
-        diff *= self.kld_scale
-
-        z_q = z_e + (z_q - z_e).detach() # noop in forward pass, straight-through gradient estimator in backward pass
-        z_q = z_q.permute(0, 3, 1, 2) # stack encodings into channels again: (B, C, H, W)
-        return z_q, diff, ind, neg_dist
-
-    def get_dist(self, flat_z):
-        '''
-        returns distance from z to each embedding vec
-        flat_z should be of shape (B*H*W, C), e.g. (10*16*16, 256)
-        '''
-        dist = (
-            flat_z.pow(2).sum(1, keepdim=True)
-            - 2 * flat_z @ self.embed.weight.t()
-            + self.embed.weight.pow(2).sum(1, keepdim=True).t()
-        )
-        return dist
-
-    def embed_code(self, embed_id):
-        return F.embedding(embed_id, self.embed.weight)
-    
-    def embed_one_hot(self, embed_vec):
-        '''
-        embed vec is of shape (B * T * H * W, n_embed)
-        '''
-        return embed_vec @ self.embed.weight
-
-class GumbelQuantize(nn.Module):
-    """
-    Gumbel Softmax trick quantizer
-    Categorical Reparameterization with Gumbel-Softmax, Jang et al. 2016
-    https://arxiv.org/abs/1611.01144
-    """
-    def __init__(self, num_hiddens, n_embed, embedding_dim, straight_through=False):
-        super().__init__()
-
-        self.embedding_dim = embedding_dim
-        self.n_embed = n_embed
-
-        self.straight_through = straight_through
-        self.temperature = 1.0
-        self.kld_scale = 5e-4
-
-        self.proj = nn.Conv2d(num_hiddens, n_embed, 1)
-        self.embed = nn.Embedding(n_embed, embedding_dim)
-
-    def forward(self, z):
-
-        # force hard = True when we are in eval mode, as we must quantize
-        hard = self.straight_through if self.training else True
-
-        logits = self.proj(z)
-        soft_one_hot = F.gumbel_softmax(logits, tau=self.temperature, dim=1, hard=hard)
-        z_q = einsum('b n h w, n d -> b d h w', soft_one_hot, self.embed.weight)
-
-        # + kl divergence to the prior loss
-        qy = F.softmax(logits, dim=1)
-        diff = self.kld_scale * torch.sum(qy * torch.log(qy * self.n_embed + 1e-10), dim=1).mean()
-
-        ind = soft_one_hot.argmax(dim=1)
-        return z_q, diff, ind, logits
-
-    def embed_one_hot(self, embed_vec):
-        '''
-        embed vec is of shape (B * T * H * W, n_embed)
-        '''
-        return embed_vec @ self.embed.weight
-    
-    def embed_code(self, embed_id):
-        return F.embedding(embed_id, self.embed.weight)
\ No newline at end of file
diff --git a/run.py b/run.py
deleted file mode 100755
index 7bde813..0000000
--- a/run.py
+++ /dev/null
@@ -1,42 +0,0 @@
-import aicrowd_helper
-import train_submission_code
-import test_framework
-
-import os
-EVALUATION_RUNNING_ON = os.getenv('EVALUATION_RUNNING_ON', None)
-EVALUATION_STAGE = os.getenv('EVALUATION_STAGE', 'all')
-EXITED_SIGNAL_PATH = os.getenv('EXITED_SIGNAL_PATH', 'shared/exited')
-
-# Training Phase
-if EVALUATION_STAGE in ['all', 'training']:
-    aicrowd_helper.training_start()
-    try:
-        train_submission_code.main()
-        aicrowd_helper.training_end()
-    except Exception as e:
-        aicrowd_helper.training_error()
-        print(e)
-
-
-# Testing Phase
-if EVALUATION_STAGE in ['all', 'testing']:
-    if EVALUATION_RUNNING_ON in ['local']:
-        try:
-            os.remove(EXITED_SIGNAL_PATH)
-        except FileNotFoundError:
-            pass
-    aicrowd_helper.inference_start()
-    try:
-        test_framework.main()
-        aicrowd_helper.inference_end()
-    except Exception as e:
-        aicrowd_helper.inference_error()
-        print(e)
-    if EVALUATION_RUNNING_ON in ['local']:
-        from pathlib import Path
-        Path(EXITED_SIGNAL_PATH).touch()
-
-# Launch instance manager
-if EVALUATION_STAGE in ['manager']:
-    from minerl.env.malmo import launch_instance_manager
-    launch_instance_manager()
diff --git a/shared/.gitignore b/shared/.gitignore
deleted file mode 100755
index e69de29..0000000
diff --git a/shell_scripts/train_vae_and_rssm.sh b/shell_scripts/train_vae_and_rssm.sh
deleted file mode 100644
index da5fcf0..0000000
--- a/shell_scripts/train_vae_and_rssm.sh
+++ /dev/null
@@ -1,7 +0,0 @@
-
-
-# train VAE
-python3 -u train_VAE.py --data_dir $MRL/data/numpy_data --env_name MineRLObtainIronPickaxeVectorObf-v0 --batch_size 150 --epochs 1 --log_dir $MRL/run_logs --model_class Conv
-
-# train Dynamics
-python3 -u train_Dynamics --data_dir $MRL/data/numpy_data --env_name MineRLObtainIronPickaxeDenseVectorObf-v0 --batch_size 100 --epochs 1 --log_dir $MRL/run_logs --model_class rssm --model_path
diff --git a/test_framework.py b/test_framework.py
deleted file mode 100755
index a63a157..0000000
--- a/test_framework.py
+++ /dev/null
@@ -1,72 +0,0 @@
-import json
-import logging
-import os
-import threading
-
-import aicrowd_helper
-import gym
-import minerl
-
-from test_submission_code import MineRLAgent, Episode, EpisodeDone
-
-import coloredlogs
-coloredlogs.install(logging.DEBUG)
-
-
-# Read aicrowd.json for the tag on which env to use for the evaluation
-aicrowd_json = None
-with open("aicrowd.json") as f:
-    aicrowd_json = json.load(f)
-assert aicrowd_json["tags"] in ["research", "intro"], "aicrowd.json 'tag' needs to be one of ['research', 'intro']"
-is_research_track = aicrowd_json["tags"] == "research"
-
-# All the evaluations will be evaluated on MineRLObtainDiamondVectorObf-v0 or MineRLObtainDiamond-v0 environment
-MINERL_GYM_ENV = "MineRLObtainDiamondVectorObf-v0" if is_research_track else "MineRLObtainDiamond-v0"
-MINERL_MAX_EVALUATION_EPISODES = int(os.getenv('MINERL_MAX_EVALUATION_EPISODES', 5))
-
-# Parallel testing/inference, **you can override** below value based on compute
-# requirements, etc to save OOM in this phase.
-EVALUATION_THREAD_COUNT = int(os.getenv('EPISODES_EVALUATION_THREAD_COUNT', 2))
-
-####################
-# EVALUATION CODE  #
-####################
-
-def main():
-    agent = MineRLAgent()
-    agent.load_agent()
-
-    assert MINERL_MAX_EVALUATION_EPISODES > 0
-    assert EVALUATION_THREAD_COUNT > 0
-
-    # First call to reset will build/create the environment,
-    # but since MineRL v0.4 this works on Linux
-    envs = []
-    for _ in range(EVALUATION_THREAD_COUNT):
-        env = gym.make(MINERL_GYM_ENV)
-        envs.append(env)
-
-    episodes_per_thread = [MINERL_MAX_EVALUATION_EPISODES // EVALUATION_THREAD_COUNT for _ in range(EVALUATION_THREAD_COUNT)]
-    episodes_per_thread[-1] += MINERL_MAX_EVALUATION_EPISODES - EVALUATION_THREAD_COUNT *(MINERL_MAX_EVALUATION_EPISODES // EVALUATION_THREAD_COUNT)
-
-    # A simple function to evaluate on episodes!
-    def evaluate(i, env):
-        print("[{}] Starting evaluator.".format(i))
-        for i in range(episodes_per_thread[i]):
-            try:
-                agent.run_agent_on_episode(Episode(env))
-            except EpisodeDone:
-                print("[{}] Episode complete".format(i))
-                pass
-
-    evaluator_threads = [threading.Thread(target=evaluate, args=(i, envs[i])) for i in range(EVALUATION_THREAD_COUNT)]
-    for thread in evaluator_threads:
-        thread.start()
-
-    # wait fo the evaluation to finish
-    for thread in evaluator_threads:
-        thread.join()
-
-
-if __name__ == "__main__":
-    main()
diff --git a/test_submission_code.py b/test_submission_code.py
deleted file mode 100755
index d284c64..0000000
--- a/test_submission_code.py
+++ /dev/null
@@ -1,80 +0,0 @@
-import gym
-
-
-class EpisodeDone(Exception):
-    pass
-
-
-class Episode(gym.Env):
-    """A class for a single episode."""
-    def __init__(self, env):
-        self.env = env
-        self.action_space = env.action_space
-        self.observation_space = env.observation_space
-        self._done = False
-
-    def reset(self):
-        if not self._done:
-            return self.env.reset()
-
-    def step(self, action):
-        s, r, d, i = self.env.step(action)
-        if d:
-            self._done = True
-            raise EpisodeDone()
-        else:
-            return s, r, d, i
-
-
-class MineRLAgent():
-    """
-    To compete in the competition, you are required to implement the two
-    functions in this class:
-        - load_agent: a function that loads e.g. network models
-        - run_agent_on_episode: a function that plays one game of MineRL
-
-    By default this agent behaves like a random agent: pick random action on
-    each step.
-
-    NOTE:
-        This class enables the evaluator to run your agent in parallel in Threads,
-        which means anything loaded in load_agent will be shared among parallel
-        agents. Take care when tracking e.g. hidden state (this should go to run_agent_on_episode).
-    """
-
-    def load_agent(self):
-        """
-        This method is called at the beginning of the evaluation.
-        You should load your model and do any preprocessing here.
-        THIS METHOD IS ONLY CALLED ONCE AT THE BEGINNING OF THE EVALUATION.
-        DO NOT LOAD YOUR MODEL ANYWHERE ELSE.
-        """
-        # This is a random agent so no need to do anything
-        # YOUR CODE GOES HERE
-        pass
-
-    def run_agent_on_episode(self, single_episode_env: Episode):
-        """This method runs your agent on a SINGLE episode.
-
-        You should just implement the standard environment interaction loop here:
-            obs  = env.reset()
-            while not done:
-                env.step(self.agent.act(obs))
-                ...
-
-        NOTE:
-            This method will be called in PARALLEL during evaluation.
-            So, only store state in LOCAL variables.
-            For example, if using an LSTM, don't store the hidden state in the class
-            but as a local variable to the method.
-
-        Args:
-            env (gym.Env): The env your agent should interact with.
-        """
-        # An implementation of a random agent
-        # YOUR CODE GOES HERE
-        _ = single_episode_env.reset()
-        done = False
-        while not done:
-            random_act = single_episode_env.action_space.sample()
-            single_episode_env.step(random_act)
diff --git a/tmp/tmp_1jel5r_/test.o b/tmp/tmp_1jel5r_/test.o
deleted file mode 100644
index c049aea..0000000
Binary files a/tmp/tmp_1jel5r_/test.o and /dev/null differ
diff --git a/tmp/tmp_2itg5ea/test.o b/tmp/tmp_2itg5ea/test.o
deleted file mode 100644
index 7f13a54..0000000
Binary files a/tmp/tmp_2itg5ea/test.o and /dev/null differ
diff --git a/tmp/tmpfb05p5xf/test.o b/tmp/tmpfb05p5xf/test.o
deleted file mode 100644
index 035d37e..0000000
Binary files a/tmp/tmpfb05p5xf/test.o and /dev/null differ
diff --git a/tmp/tmpg859g0y4/test.o b/tmp/tmpg859g0y4/test.o
deleted file mode 100644
index 4553d3a..0000000
Binary files a/tmp/tmpg859g0y4/test.o and /dev/null differ
diff --git a/tmp/tmpilhjnsoi/test.o b/tmp/tmpilhjnsoi/test.o
deleted file mode 100644
index 82f6923..0000000
Binary files a/tmp/tmpilhjnsoi/test.o and /dev/null differ
diff --git a/tmp/tmpkuo0ed9c/test.o b/tmp/tmpkuo0ed9c/test.o
deleted file mode 100644
index ce80aff..0000000
Binary files a/tmp/tmpkuo0ed9c/test.o and /dev/null differ
diff --git a/tmp/tmpsj3gr_9l/test.o b/tmp/tmpsj3gr_9l/test.o
deleted file mode 100644
index cefe112..0000000
Binary files a/tmp/tmpsj3gr_9l/test.o and /dev/null differ
diff --git a/tmp/tmpum_ahe3c/test.o b/tmp/tmpum_ahe3c/test.o
deleted file mode 100644
index 7f13a54..0000000
Binary files a/tmp/tmpum_ahe3c/test.o and /dev/null differ
diff --git a/tmp/tmpxm2cpw7q/test.o b/tmp/tmpxm2cpw7q/test.o
deleted file mode 100644
index 7f13a54..0000000
Binary files a/tmp/tmpxm2cpw7q/test.o and /dev/null differ
diff --git a/tmp/tmpxxxdunph/test.o b/tmp/tmpxxxdunph/test.o
deleted file mode 100644
index 0cc831a..0000000
Binary files a/tmp/tmpxxxdunph/test.o and /dev/null differ
diff --git a/train/.gitignore b/train/.gitignore
deleted file mode 100755
index e69de29..0000000
diff --git a/train_submission_code.py b/train_submission_code.py
deleted file mode 100755
index 189ab5f..0000000
--- a/train_submission_code.py
+++ /dev/null
@@ -1,90 +0,0 @@
-import json
-import select
-import time
-import logging
-import os
-
-import numpy as np
-import aicrowd_helper
-import gym
-import minerl
-from utility.parser import Parser
-
-import coloredlogs
-coloredlogs.install(logging.DEBUG)
-
-# --- NOTE ---
-# This code is only used for "Research" track submissions
-# ------------
-
-# All research-tracks evaluations will be ran on the MineRLObtainDiamondVectorObf-v0 environment
-MINERL_GYM_ENV = os.getenv('MINERL_GYM_ENV', 'MineRLObtainDiamondVectorObf-v0')
-# You need to ensure that your submission is trained in under MINERL_TRAINING_MAX_STEPS steps
-MINERL_TRAINING_MAX_STEPS = int(os.getenv('MINERL_TRAINING_MAX_STEPS', 8000000))
-# You need to ensure that your submission is trained by launching less than MINERL_TRAINING_MAX_INSTANCES instances
-MINERL_TRAINING_MAX_INSTANCES = int(os.getenv('MINERL_TRAINING_MAX_INSTANCES', 5))
-# You need to ensure that your submission is trained within allowed training time.
-# Round 1: Training timeout is 5 minutes
-# Round 2: Training timeout is 4 days
-MINERL_TRAINING_TIMEOUT = int(os.getenv('MINERL_TRAINING_TIMEOUT_MINUTES', 4 * 24 * 60))
-# The dataset is available in data/ directory from repository root.
-MINERL_DATA_ROOT = os.getenv('MINERL_DATA_ROOT', 'data/')
-
-# Optional: You can view best effort status of your instances with the help of parser.py
-# This will give you current state like number of steps completed, instances launched and so on. Make your you keep a tap on the numbers to avoid breaching any limits.
-parser = Parser(
-    'performance/',
-    allowed_environment=MINERL_GYM_ENV,
-    maximum_instances=MINERL_TRAINING_MAX_INSTANCES,
-    maximum_steps=MINERL_TRAINING_MAX_STEPS,
-    raise_on_error=False,
-    no_entry_poll_timeout=600,
-    submission_timeout=MINERL_TRAINING_TIMEOUT * 60,
-    initial_poll_timeout=600
-)
-
-
-def main():
-    """
-    This function will be called for training phase.
-    """
-    # How to sample minerl data is document here:
-    # http://minerl.io/docs/tutorials/data_sampling.html
-    data = minerl.data.make(MINERL_GYM_ENV, data_dir=MINERL_DATA_ROOT)
-
-    # Sample code for illustration, add your training code below
-    env = gym.make(MINERL_GYM_ENV)
-
-    # For an example, lets just run one episode of MineRL for training
-    obs = env.reset()
-    done = False
-    while not done:
-        obs, reward, done, info = env.step(env.action_space.sample())
-        # Do your training here
-
-        # To get better view in your training phase, it is suggested
-        # to register progress continuously, example when 54% completed
-        # aicrowd_helper.register_progress(0.54)
-
-        # To fetch latest information from instance manager, you can run below when you want to know the state
-        #>> parser.update_information()
-        #>> print(parser.payload)
-        # .payload: provide AIcrowd generated json
-        # Example: {'state': 'RUNNING', 'score': {'score': 0.0, 'score_secondary': 0.0}, 'instances': {'1': {'totalNumberSteps': 2001, 'totalNumberEpisodes': 0, 'currentEnvironment': 'MineRLObtainDiamond-v0', 'state': 'IN_PROGRESS', 'episodes': [{'numTicks': 2001, 'environment': 'MineRLObtainDiamond-v0', 'rewards': 0.0, 'state': 'IN_PROGRESS'}], 'score': {'score': 0.0, 'score_secondary': 0.0}}}}
-        # .current_state: provide indepth state information avaiable as dictionary (key: instance id)
-
-    # Save trained model to train/ directory
-    # For a demonstration, we save some dummy data.
-    # NOTE: During Round 1 submission you upload trained agents as part of the git repository.
-    #       The training code is only ran for 5 minutes (i.e. no proper training), so you might
-    #       want to avoid overwriting any existing files here!
-    #       Remember to enable it for Round 2 submission, though!
-    np.save("./train/parameters.npy", np.random.random((10,)))
-
-    # Training 100% Completed
-    aicrowd_helper.register_progress(1)
-    env.close()
-
-
-if __name__ == "__main__":
-    main()
diff --git a/utility/docker_build.sh b/utility/docker_build.sh
deleted file mode 100755
index f082b52..0000000
--- a/utility/docker_build.sh
+++ /dev/null
@@ -1,26 +0,0 @@
-#!/bin/bash
-
-if [ -e environ_secret.sh ]
-then
-    source utility/environ_secret.sh
-else
-    source utility/environ.sh
-fi
-
-if ! [ -x "$(command -v aicrowd-repo2docker)" ]; then
-  echo 'Error: aicrowd-repo2docker is not installed.' >&2
-  echo 'Please install it using requirements.txt or pip -U install aicrowd-repo2docker' >&2
-  exit 1
-fi
-
-# Expected Env variables : in environ.sh
-
-REPO2DOCKER="$(which aicrowd-repo2docker)"
-
-sudo ${REPO2DOCKER} --no-run \
-  --user-id 1001 \
-  --user-name aicrowd \
-  --image-name ${IMAGE_NAME}:${IMAGE_TAG} \
-  --debug .
-
-#sudo docker push "${IMAGE_NAME}:${IMAGE_TAG}"
diff --git a/utility/docker_evaluation_locally.sh b/utility/docker_evaluation_locally.sh
deleted file mode 100755
index 1faf82d..0000000
--- a/utility/docker_evaluation_locally.sh
+++ /dev/null
@@ -1,45 +0,0 @@
-#!/bin/bash
-# This script run your submission inside a docker image, this is identical in termrs of 
-# how your code will be executed on AIcrowd platform, with the exception of some
-# environment variables removed (which do not work outside AICrowd platform)
-
-
-if [ -e environ_secret.sh ]
-then
-    echo "Note: Gathering environment variables from environ_secret.sh"
-    source utility/environ_secret.sh
-else
-    echo "Note: Gathering environment variables from environ.sh"
-    source utility/environ.sh
-fi
-
-# Skip building docker image on run, by default each run means new docker image build
-if [[ " $@ " =~ " --no-build " ]]; then
-    echo "Skipping docker image build"
-else
-    echo "Building docker image, for skipping docker image build use \"--no-build\""
-    ./utility/docker_build.sh
-fi
-
-ARGS="${@}"
-
-# Expected Env variables : in environ.sh
-if [[ " $@ " =~ " --nvidia " ]]; then
-    sudo nvidia-docker run \
-    --net=host \
-    --user 0 \
-    -v $(PWD)/data:/home/aicrowd/data \
-    -v $(PWD)/performance:/home/aicrowd/performance \
-    -v $(PWD)/.gradle:/home/aicrowd/.gradle \
-    -it ${IMAGE_NAME}:${IMAGE_TAG} \
-    /bin/bash -c "echo \"Staring docker evaluation...\"; xvfb-run -a ./utility/evaluation_locally.sh ${ARGS}"
-else
-    echo "NOTE: To run your submission with nvidia drivers locally, use \"--nvidia\" with this script"
-    sudo docker run \
-    --net=host \
-    -v $(PWD)/data:/home/aicrowd/data \
-    -v $(PWD)/performance:/home/aicrowd/performance \
-    -v $(PWD)/.gradle:/home/aicrowd/.gradle \
-    -it ${IMAGE_NAME}:${IMAGE_TAG} \
-    /bin/bash -c "echo \"Staring docker evaluation...\"; xvfb-run -a ./utility/evaluation_locally.sh ${ARGS}"
-fi
diff --git a/utility/docker_run.sh b/utility/docker_run.sh
deleted file mode 100755
index bee083e..0000000
--- a/utility/docker_run.sh
+++ /dev/null
@@ -1,37 +0,0 @@
-#!/bin/bash
-# This script run your submission inside a docker image, this is identical in termrs of 
-# how your code will be executed on AIcrowd platform, with the exception of some
-# environment variables removed (which do not work outside AICrowd platform)
-
-if [ -e environ_secret.sh ]
-then
-    echo "Note: Gathering environment variables from environ_secret.sh"
-    source utility/environ_secret.sh
-else
-    echo "Note: Gathering environment variables from environ.sh"
-    source utility/environ.sh
-fi
-
-# Skip building docker image on run, by default each run means new docker image build
-if [[ " $@ " =~ " --no-build " ]]; then
-    echo "Skipping docker image build"
-else
-    echo "Building docker image, for skipping docker image build use \"--no-build\""
-    ./utility/docker_build.sh
-fi
-
-# Expected Env variables : in environ.sh
-if [[ " $@ " =~ " --nvidia " ]]; then
-    sudo nvidia-docker run \
-    --net=host \
-    --user 0 \
-    -it ${IMAGE_NAME}:${IMAGE_TAG} \
-    /bin/bash
-else
-    echo "To run your submission with nvidia drivers, use \"--nvidia\" with this script"
-    sudo docker run \
-    --net=host \
-    --user 0 \
-    -it ${IMAGE_NAME}:${IMAGE_TAG} \
-    /bin/bash
-fi
diff --git a/utility/docker_train_locally.sh b/utility/docker_train_locally.sh
deleted file mode 100755
index 156c5d1..0000000
--- a/utility/docker_train_locally.sh
+++ /dev/null
@@ -1,44 +0,0 @@
-#!/bin/bash
-# This script run your submission inside a docker image, this is identical in termrs of 
-# how your code will be executed on AIcrowd platform, with the exception of some
-# environment variables removed (which do not work outside AICrowd platform)
-
-
-if [ -e environ_secret.sh ]
-then
-    echo "Note: Gathering environment variables from environ_secret.sh"
-    source utility/environ_secret.sh
-else
-    echo "Note: Gathering environment variables from environ.sh"
-    source utility/environ.sh
-fi
-
-# Skip building docker image on run, by default each run means new docker image build
-if [[ " $@ " =~ " --no-build " ]]; then
-    echo "Skipping docker image build"
-else
-    echo "Building docker image, for skipping docker image build use \"--no-build\""
-    ./utility/docker_build.sh
-fi
-
-ARGS="${@}"
-
-# Expected Env variables : in environ.sh
-if [[ " $@ " =~ " --nvidia " ]]; then
-    sudo nvidia-docker run \
-    --net=host \
-    -v $(PWD)/data:/home/aicrowd/data \
-    -v $(PWD)/performance:/home/aicrowd/performance \
-    -v $(PWD)/.gradle:/home/aicrowd/.gradle \
-    -it ${IMAGE_NAME}:${IMAGE_TAG} \
-    /bin/bash -c "echo \"Staring docker training...\"; xvfb-run -a ./utility/train_locally.sh ${ARGS}"
-else
-    echo "To run your submission with nvidia drivers locally, use \"--nvidia\" with this script"
-    sudo docker run \
-    --net=host \
-    -v $(PWD)/data:/home/aicrowd/data \
-    -v $(PWD)/performance:/home/aicrowd/performance \
-    -v $(PWD)/.gradle:/home/aicrowd/.gradle \
-    -it ${IMAGE_NAME}:${IMAGE_TAG} \
-    /bin/bash -c "echo \"Staring docker training...\"; xvfb-run -a ./utility/train_locally.sh ${ARGS}"
-fi
diff --git a/utility/environ.sh b/utility/environ.sh
deleted file mode 100755
index 51734dc..0000000
--- a/utility/environ.sh
+++ /dev/null
@@ -1,4 +0,0 @@
-#!/bin/bash
-
-export IMAGE_NAME="aicrowd/neurips2021-minerl-challenge"
-export IMAGE_TAG="agent"
diff --git a/utility/evaluation_locally.sh b/utility/evaluation_locally.sh
deleted file mode 100755
index 171ed2d..0000000
--- a/utility/evaluation_locally.sh
+++ /dev/null
@@ -1,45 +0,0 @@
-#!/bin/bash
-set -e
-
-
-AICROWD_DATA_ENABLED="YES"
-if [[ " $@ " =~ " --no-data " ]]; then
-   AICROWD_DATA_ENABLED="NO"
-else
-    python3 ./utility/verify_or_download_data.py
-fi
-
-
-EXTRAOUTPUT=" > /dev/null 2>&1 "
-if [[ " $@ " =~ " --verbose " ]]; then
-   EXTRAOUTPUT=""
-fi
-
-# Fixes for the new MineRL code
-export PYRO_SERIALIZERS_ACCEPTED='pickle'
-export PYRO_SERIALIZER='pickle'
-
-# Run local name server
-eval "pyro4-ns $EXTRAOUTPUT &"
-trap "kill -11 $! > /dev/null 2>&1;" EXIT
-
-# Run instance manager to generate performance report
-export EVALUATION_STAGE='manager'
-eval "python3 run.py --seeds 1 $EXTRAOUTPUT &"
-trap "kill -11 $! > /dev/null 2>&1;" EXIT
-
-# Run the evaluation
-sleep 2
-export MINERL_INSTANCE_MANAGER_REMOTE="1"
-export EVALUATION_STAGE='testing'
-export EVALUATION_RUNNING_ON='local'
-export EXITED_SIGNAL_PATH='shared/exited'
-rm -f $EXITED_SIGNAL_PATH
-export ENABLE_AICROWD_JSON_OUTPUT='False'
-eval "python3 run.py $EXTRAOUTPUT && touch $EXITED_SIGNAL_PATH || touch $EXITED_SIGNAL_PATH &"
-trap "kill -11 $! > /dev/null 2>&1;" EXIT
-
-# View the evaluation state
-export ENABLE_AICROWD_JSON_OUTPUT='True'
-python3 utility/parser.py || true
-kill $(jobs -p)
diff --git a/utility/parser.py b/utility/parser.py
deleted file mode 100755
index 069a586..0000000
--- a/utility/parser.py
+++ /dev/null
@@ -1,296 +0,0 @@
-#!/usr/bin/env python3
-import copy
-import json
-import os
-import signal
-import sys
-import subprocess
-import time
-import glob
-
-import crowdai_api
-import uuid
-
-
-class AICrowdSubContractor:
-    def __init__(self):
-        self.debug = False
-        self.oracle_events = crowdai_api.events.CrowdAIEvents(with_oracle=True)
-
-    def handle_event(self, payload):
-        if self.debug:
-            print(payload)
-        if payload['state'] == 'FINISHED':
-            self.handle_success_event(payload)
-        elif payload['state'] == 'ERROR':
-            self.handle_error_event(payload)
-        else:
-            self.handle_info_event(payload)
-
-    def handle_info_event(self, payload):
-        self.oracle_events.register_event(
-            event_type=self.oracle_events.CROWDAI_EVENT_INFO,
-            payload=payload
-        )
-
-    def handle_success_event(self, payload):
-        self.oracle_events.register_event(
-            event_type=self.oracle_events.CROWDAI_EVENT_SUCCESS,
-            payload=payload
-        )
-
-    def handle_error_event(self, payload):
-        self.oracle_events.register_event(
-            event_type=self.oracle_events.CROWDAI_EVENT_ERROR,
-            payload=payload
-        )
-
-
-class Parser:
-    def __init__(self, directory, allowed_environment=None, maximum_instances=None, maximum_steps=None, raise_on_error=True, no_entry_poll_timeout=1800, submission_timeout=None, initial_poll_timeout=30*60, debug=False):
-        self.directory = directory
-        self.allowed_environment = allowed_environment
-        self.maximum_instances = maximum_instances
-        self.maximum_steps = maximum_steps
-        self.raise_on_error = raise_on_error
-        self.no_entry_poll_timeout = no_entry_poll_timeout
-        self.submission_timeout = submission_timeout
-        self.initial_poll_timeout = initial_poll_timeout
-
-        self.aicrowd_subcontractor = AICrowdSubContractor()
-        self.aicrowd_subcontractor.debug = debug
-        self.start_time = time.time()
-        self.current_state = {}
-        self.finished = {}
-        self.last_change_time = {}
-        self.totalInstances = 0
-        self.payload = {
-            'state': 'PENDING',
-            'score': {},
-            'instances': []
-        }
-        self.freeze = False
-
-    def add_instance(self, instance_id):
-        self.current_state[instance_id] = {
-            'state': 'PENDING',
-            'episodes': [],
-            'score': {},
-            'totalNumberSteps': 0
-        }
-        self.finished[instance_id] = False
-        self.last_change_time[instance_id] = time.time()
-        self.totalInstances += 1
-
-    def read_json_file(self, path):
-        try:
-            with open(path) as file:
-                return json.load(file), True
-        except:
-            return {}, False
-
-    def send_information_to_sourcerer(self):
-        if not self.freeze:
-            instance_started = False
-            instance_running = False
-            for instance_id in self.current_state:
-                instance_state = self.current_state[instance_id]['state']
-                if instance_state != 'PENDING':
-                    instance_started = True
-                if instance_state != 'FINISHED' and instance_state != 'ERROR':
-                    instance_running = True
-            if instance_started:
-                self.payload['state'] = 'RUNNING'
-            if self.totalInstances > 0 and not instance_running:
-                self.payload['state'] = 'FINISHED'
-            self.payload['instances'] = self.current_state
-
-            score = 0.0
-            instances = 0
-            for state in self.current_state:
-                episodes = self.current_state[state]['episodes']
-                for episode in episodes:
-                    score += episode['rewards']
-                    instances += 1
-            if instances > 0:
-                score = str(round(score/instances, 2))
-            self.payload['score'] = {
-                'score': score,
-                'score_secondary': sum(self.current_state[x]['score']['score_secondary'] for x in self.current_state)
-            }
-
-        self.aicrowd_subcontractor.handle_event(self.payload)
-
-
-    def update_instance_if_changed(self, instance_id, currentInformation):
-        updated = False
-        if self.finished[instance_id]:
-            return False
-
-        previousInformation = self.current_state[instance_id]
-        if previousInformation['totalNumberSteps'] != currentInformation['totalNumberSteps']:
-            updated = True
-        self.current_state[instance_id] = copy.deepcopy(currentInformation)
-        return updated
-
-    def check_for_condition_breach(self):
-        breached = False
-        if self.totalInstances > self.maximum_instances:
-            breached = True
-            self.payload['reason'] = 'You started more instances (%d) then allowed limit (%d).' % (self.totalInstances, self.maximum_instances)
-        totalSteps = sum(self.current_state[x]["totalNumberSteps"] for x in self.current_state)
-        if self.maximum_steps and totalSteps > self.maximum_steps:
-            breached = True
-            self.payload['reason'] = 'Steps (%d) are more then allowed limit (%d).' % (totalSteps, self.maximum_steps)
-        if (time.time() - self.start_time) > self.submission_timeout:
-            breached = True
-            self.payload['reason'] = 'Submission time increased the threshold (%d seconds).' % (self.submission_timeout)
-        if self.totalInstances == 0 and (time.time() - self.start_time) > self.initial_poll_timeout:
-            breached = True
-            self.payload['reason'] = 'No instance started in threshold (%d seconds).' % (self.initial_poll_timeout)
-
-        if breached:
-            self.payload['state'] = 'ERROR'
-        return breached
-
-    def update_information(self, finished=False):
-        if self.freeze:
-            return
-
-        any_instance_updated = False
-        instance_folders = list(filter(lambda x: os.path.isdir(os.path.join(self.directory, x)), os.listdir(self.directory)))
-        for instance_folder in instance_folders:
-            instance_id = instance_folder.split('mc_')[1]
-            if instance_id not in self.current_state:
-                self.add_instance(instance_id)
-
-            currentInformation = self.read_instance_information(instance_id, '/'.join([self.directory, instance_folder]))
-            updated = self.update_instance_if_changed(instance_id, currentInformation)
-
-            if updated:
-                self.last_change_time[instance_id] = time.time()
-
-            if (not updated and not self.finished[instance_id]) or finished:
-                currentTime = time.time()
-                if (currentTime - self.last_change_time[instance_id]) > self.no_entry_poll_timeout or finished:
-                    if 'totalNumberEpisodes' in currentInformation and len(currentInformation['episodes']) == currentInformation['totalNumberEpisodes']:
-                        currentInformation['state'] = 'FINISHED'
-                    else:
-                        currentInformation['state'] = 'ERROR'
-                    self.update_instance_if_changed(instance_id, currentInformation)
-                    self.finished[instance_id] = True
-                    updated = True
-
-            if updated:
-                any_instance_updated = True
-
-        if any_instance_updated:
-            self.send_information_to_sourcerer()
-        if self.check_for_condition_breach():
-            self.freeze = True
-            self.send_information_to_sourcerer()
-        if finished and not self.freeze:
-            return
-
-    def check_for_allowed_environment(self, environment, payload):
-        if self.allowed_environment is not None:
-            if not environment in self.allowed_environment:
-                payload['state'] = 'ERROR'
-                payload['reason'] = 'Wrong environment used, you should use "%s" instead of "%s"' \
-                                    % (MINERL_GYM_ENV, payload['currentEnvironment'])
-                if self.raise_on_error:
-                    raise Exception(payload['reason'])
-                return False
-        return True
-
-    def read_instance_information(self, instance_id, instance_directory):
-        status_file = instance_directory + '/status.json'
-        # {'totalNumberSteps': 18012, 'totalNumberEpisodes': 3, 'currentEnvironment': 'MineRLObtainDiamondVectorObf-v0'}
-        payload, found = self.read_json_file(status_file)
-        payload['state'] = 'PENDING'
-        payload['episodes'] = []
-        score = 0.00
-
-        if 'currentEnvironment' in payload:
-            self.check_for_allowed_environment(payload['currentEnvironment'], payload)
-
-        for episode in range(payload.get('totalNumberEpisodes', -1) + 1):
-            # 000000-MineRLObtainDiamondVectorObf-v0.json
-            episode_file = instance_directory + '/' + str(episode).zfill(6) + '-' + payload['currentEnvironment'] + '.json'
-            episode_info, found = self.read_json_file(episode_file)
-            if found:
-                # Atleast one file present, so submission has started for sure.
-                payload['state'] = 'IN_PROGRESS'
-                episode_info['state'] = 'IN_PROGRESS'
-                if episode < payload['totalNumberEpisodes']:
-                    episode_info['state'] = 'FINISHED'
-                episode_info['rewards'] = sum(episode_info['rewards'])
-                score += episode_info['rewards']
-                payload['episodes'].append(episode_info)
-            else:
-                break
-
-        if len(payload['episodes']) > 0:
-            score = str(round(score/len(payload['episodes']), 2))
-
-        payload['score'] = {
-            "score": score,
-            "score_secondary": 0.0
-        }
-
-        if 'totalNumberSteps' not in payload:
-            payload['totalNumberSteps'] = 0
-        return payload
-
-# Debug the aicrowd json
-ENABLE_AICROWD_JSON_OUTPUT = bool(os.getenv('ENABLE_AICROWD_JSON_OUTPUT', 'True'))
-# Where the output files will be located
-PERFORMANCE_DIRECTORY = os.getenv('PERFORMANCE_DIRECTORY', 'performance/')
-# Time (in seconds) to wait before checking performance directory updates
-POLL_INTERVAL=1
-# How many seconds to let the submission run
-SUBMISSION_TIMEOUT = int(os.getenv('SUBMISSION_TIMEOUT', 24*60*60))
-# How many seconds to wait before first instance start running
-INITIAL_POLL_TIMEOUT = int(os.getenv('INITIAL_POLL_TIMEOUT', 3*60))
-# How many seconds to wait before considering instance manager is dead
-NO_NEW_ENTRY_POLL_TIMEOUT = int(os.getenv('NO_NEW_ENTRY_POLL_TIMEOUT', 180))
-# Maximum number of instances to launch
-MAX_ALLOWED_INSTANCES = int(os.getenv('MAX_ALLOWED_INSTANCES', 2))
-# Maximum number of steps
-MAX_ALLOWED_STEPS = int(os.getenv('MAX_ALLOWED_STEPS', 0)) or None
-# All the evaluations will be allowed to run only below gym environment
-MINERL_GYM_ENV = os.getenv('MINERL_GYM_ENV', 'MineRLObtainDiamondVectorObf-v0,MineRLObtainDiamond-v0')
-
-# Where to look if submission has finished
-EXITED_SIGNAL_PATH = os.getenv('EXITED_SIGNAL_PATH', 'shared/exited')
-
-
-###############################################################
-# Helper Functions End
-###############################################################
-def sigusr1_handler(signum, stackframe):
-    print("The evaluator received SIGUSR1... shutting down our operation")
-    sys.exit(10)
-
-
-if __name__ == '__main__':
-    parser = Parser(PERFORMANCE_DIRECTORY,
-                    allowed_environment=MINERL_GYM_ENV,
-                    maximum_instances=MAX_ALLOWED_INSTANCES,
-                    maximum_steps=MAX_ALLOWED_STEPS,
-                    raise_on_error=True,
-                    no_entry_poll_timeout=NO_NEW_ENTRY_POLL_TIMEOUT,
-                    submission_timeout=SUBMISSION_TIMEOUT,
-                    initial_poll_timeout=INITIAL_POLL_TIMEOUT,
-                    debug=ENABLE_AICROWD_JSON_OUTPUT)
-
-    while True:
-        parser.update_information()
-
-        if os.path.exists(EXITED_SIGNAL_PATH):
-            # Sweet time to get performance written after agent exit
-            time.sleep(10)
-            parser.update_information(finished=True)
-            break
-
-        time.sleep(POLL_INTERVAL)
diff --git a/utility/train_locally.sh b/utility/train_locally.sh
deleted file mode 100755
index 1844612..0000000
--- a/utility/train_locally.sh
+++ /dev/null
@@ -1,46 +0,0 @@
-#!/bin/bash
-set -e
-
-
-AICROWD_DATA_ENABLED="YES"
-if [[ " $@ " =~ " --no-data " ]]; then
-   AICROWD_DATA_ENABLED="NO"
-else
-    python3 ./utility/verify_or_download_data.py
-fi
-
-
-
-EXTRAOUTPUT=" > /dev/null 2>&1 "
-if [[ " $@ " =~ " --verbose " ]]; then
-   EXTRAOUTPUT=""
-fi
-
-export PYRO_SERIALIZERS_ACCEPTED='pickle'
-export PYRO_SERIALIZER='pickle'
-
-# Run local name server
-eval "pyro4-ns $EXTRAOUTPUT &"
-trap "kill -11 $! > /dev/null 2>&1;" EXIT
-
-# Run instance manager to generate performance report
-export EVALUATION_STAGE='manager'
-eval "python3 run.py --seeds 1 $EXTRAOUTPUT &"
-trap "kill -11 $! > /dev/null 2>&1;" EXIT
-
-# Run the training phase
-sleep 2
-echo "RUNNING TRAINING!"
-export MINERL_INSTANCE_MANAGER_REMOTE="1"
-export EVALUATION_STAGE='training'
-export EVALUATION_RUNNING_ON='local'
-export EXITED_SIGNAL_PATH='shared/training_exited'
-rm -f $EXITED_SIGNAL_PATH
-export ENABLE_AICROWD_JSON_OUTPUT='False'
-eval "python3 run.py $EXTRAOUTPUT && touch $EXITED_SIGNAL_PATH || touch $EXITED_SIGNAL_PATH &"
-trap "kill -11 $! > /dev/null 2>&1;" EXIT
-
-# View the evaluation state
-export ENABLE_AICROWD_JSON_OUTPUT='True'
-python3 utility/parser.py || true
-kill $(jobs -p)
diff --git a/utility/verify_or_download_data.py b/utility/verify_or_download_data.py
deleted file mode 100755
index 1f9928f..0000000
--- a/utility/verify_or_download_data.py
+++ /dev/null
@@ -1,32 +0,0 @@
-import minerl
-import minerl.data
-import os
-
-if __name__ == "__main__":
-    data_dir = os.getenv('MINERL_DATA_ROOT', 'data')
-    data_dir = 'data' if not data_dir else data_dir
-
-    print("Verifying (and downloading) MineRL dataset..\n"
-          "\t**If you do not want to use the data**:\n\t\t run the local evaluation scripts with `--no-data`\n"
-          "\t**If you want to use your existing download of the data**:\n "
-          "\t\tmake sure your MINERL_DATA_ROOT is set.\n\n")
-
-    print("Data directory is {}".format(data_dir))
-    should_download = True
-    try:
-        data = minerl.data.make('MineRLObtainDiamondVectorObf-v0', data_dir=data_dir)
-        assert len(data._get_all_valid_recordings(data.data_dir)) > 0
-        should_download = False
-    except FileNotFoundError:
-        print("The data directory does not exist in your submission, are you running this script from"
-              " the root of the repository? data_dir={}".format(data_dir))
-    except RuntimeError:
-        print("The data contained in your data directory is out of date! data_dir={}".format(data_dir))
-    except AssertionError:
-        print("No MineRLObtainDiamond-v0 data found. Did the data really download correctly?" )
-
-    if should_download:
-        print("Attempting to download the dataset...")
-        minerl.data.download(data_dir)
-
-    print("Data verified! A+!")
diff --git a/videos/openaigym.episode_batch.0.22025.stats.json b/videos/openaigym.episode_batch.0.22025.stats.json
deleted file mode 100644
index ddc40b7..0000000
--- a/videos/openaigym.episode_batch.0.22025.stats.json
+++ /dev/null
@@ -1 +0,0 @@
-{"initial_reset_timestamp": 1629907333.7070508, "timestamps": [], "episode_lengths": [], "episode_rewards": [], "episode_types": ["t"]}
\ No newline at end of file
diff --git a/videos/openaigym.manifest.0.22025.manifest.json b/videos/openaigym.manifest.0.22025.manifest.json
deleted file mode 100644
index ef10c2f..0000000
--- a/videos/openaigym.manifest.0.22025.manifest.json
+++ /dev/null
@@ -1 +0,0 @@
-{"stats": "openaigym.episode_batch.0.22025.stats.json", "videos": [["openaigym.video.0.22025.video000000.mp4", "openaigym.video.0.22025.video000000.meta.json"]], "env_info": {"gym_version": "0.18.3", "env_id": "MineRLTreechopVectorObf-v0"}}
\ No newline at end of file
diff --git a/videos/openaigym.video.0.22025.video000000.meta.json b/videos/openaigym.video.0.22025.video000000.meta.json
deleted file mode 100644
index a427096..0000000
--- a/videos/openaigym.video.0.22025.video000000.meta.json
+++ /dev/null
@@ -1 +0,0 @@
-{"episode_id": 0, "content_type": "video/mp4", "encoder_version": {"backend": "ffmpeg", "version": "b'ffmpeg version 4.3.1 Copyright (c) 2000-2020 the FFmpeg developers\\nbuilt with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\\nconfiguration: --prefix=/home/lieberummaas/datadisk/anaconda3/envs/minerl --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1609680890771/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-libx264 --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1609680890771/_build_env/bin/pkg-config\\nlibavutil      56. 51.100 / 56. 51.100\\nlibavcodec     58. 91.100 / 58. 91.100\\nlibavformat    58. 45.100 / 58. 45.100\\nlibavdevice    58. 10.100 / 58. 10.100\\nlibavfilter     7. 85.100 /  7. 85.100\\nlibavresample   4.  0.  0 /  4.  0.  0\\nlibswscale      5.  7.100 /  5.  7.100\\nlibswresample   3.  7.100 /  3.  7.100\\nlibpostproc    55.  7.100 / 55.  7.100\\n'", "cmdline": ["ffmpeg", "-nostats", "-loglevel", "error", "-y", "-f", "rawvideo", "-s:v", "64x64", "-pix_fmt", "rgb24", "-framerate", "30", "-i", "-", "-vf", "scale=trunc(iw/2)*2:trunc(ih/2)*2", "-vcodec", "libx264", "-pix_fmt", "yuv420p", "-r", "30", "/home/lieberummaas/datadisk/minerl/videos/openaigym.video.0.22025.video000000.mp4"]}}
\ No newline at end of file
diff --git a/videos/openaigym.video.0.22025.video000000.mp4 b/videos/openaigym.video.0.22025.video000000.mp4
deleted file mode 100644
index bb674d0..0000000
Binary files a/videos/openaigym.video.0.22025.video000000.mp4 and /dev/null differ
